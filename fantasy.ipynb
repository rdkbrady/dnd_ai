{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from game.game_state import create_data_for_ai, batch_data_for_ai\n",
    "from resnet import resnet_v2, resnet_rl, lr_schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 10\n",
    "w = 10\n",
    "\n",
    "data_generator = batch_data_for_ai(h,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.001\n",
      "Epoch 1/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2.6831 - accuracy: 0.0200\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 392ms/step - loss: 2.5476 - accuracy: 0.0100\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 412ms/step - loss: 2.4334 - accuracy: 0.1900\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 427ms/step - loss: 2.3848 - accuracy: 0.2900\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 402ms/step - loss: 2.2972 - accuracy: 0.1900\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 391ms/step - loss: 2.2332 - accuracy: 0.2500\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 1s 518ms/step - loss: 2.1774 - accuracy: 0.2400\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 1s 577ms/step - loss: 2.1551 - accuracy: 0.2700\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 1s 501ms/step - loss: 2.1516 - accuracy: 0.2100\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 424ms/step - loss: 2.0714 - accuracy: 0.2100\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 2.0638 - accuracy: 0.2000\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 412ms/step - loss: 2.0752 - accuracy: 0.2400\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 446ms/step - loss: 1.9879 - accuracy: 0.2600\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 487ms/step - loss: 2.0677 - accuracy: 0.1900\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 419ms/step - loss: 2.0090 - accuracy: 0.2100\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 429ms/step - loss: 1.8707 - accuracy: 0.3700\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 1s 526ms/step - loss: 1.8651 - accuracy: 0.2800\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 453ms/step - loss: 1.9666 - accuracy: 0.2300\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 490ms/step - loss: 2.0506 - accuracy: 0.2600\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 1s 628ms/step - loss: 2.0288 - accuracy: 0.2100\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 466ms/step - loss: 2.0321 - accuracy: 0.2600\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 471ms/step - loss: 1.9104 - accuracy: 0.3300\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 457ms/step - loss: 1.8594 - accuracy: 0.2600\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 424ms/step - loss: 1.8043 - accuracy: 0.2800\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 489ms/step - loss: 1.9085 - accuracy: 0.2900\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 441ms/step - loss: 1.9512 - accuracy: 0.3000\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 439ms/step - loss: 2.0167 - accuracy: 0.2200\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 450ms/step - loss: 1.9893 - accuracy: 0.2700\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 491ms/step - loss: 1.9411 - accuracy: 0.3600\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 439ms/step - loss: 1.8794 - accuracy: 0.2700\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 461ms/step - loss: 1.9084 - accuracy: 0.2400\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 436ms/step - loss: 1.9274 - accuracy: 0.2900\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 458ms/step - loss: 1.9071 - accuracy: 0.2400\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 1s 513ms/step - loss: 1.7528 - accuracy: 0.3100\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 488ms/step - loss: 1.9620 - accuracy: 0.3200\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 453ms/step - loss: 1.8772 - accuracy: 0.3400\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 443ms/step - loss: 1.9267 - accuracy: 0.2800\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 393ms/step - loss: 1.9136 - accuracy: 0.2700\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 1.8323 - accuracy: 0.2500\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 491ms/step - loss: 1.9353 - accuracy: 0.2800\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 397ms/step - loss: 2.0907 - accuracy: 0.3300\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 401ms/step - loss: 1.8832 - accuracy: 0.2200\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 388ms/step - loss: 1.9027 - accuracy: 0.3000\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 386ms/step - loss: 1.8628 - accuracy: 0.3000\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 393ms/step - loss: 1.8316 - accuracy: 0.3400\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 377ms/step - loss: 1.8429 - accuracy: 0.3100\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 403ms/step - loss: 2.0002 - accuracy: 0.3300\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 430ms/step - loss: 1.8539 - accuracy: 0.2900\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 432ms/step - loss: 1.9058 - accuracy: 0.2900\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 399ms/step - loss: 1.8991 - accuracy: 0.2500\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 434ms/step - loss: 1.8432 - accuracy: 0.3600\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 391ms/step - loss: 1.9407 - accuracy: 0.2800\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 447ms/step - loss: 1.8111 - accuracy: 0.4400\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 444ms/step - loss: 1.8020 - accuracy: 0.2800\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 433ms/step - loss: 1.8149 - accuracy: 0.3400\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 410ms/step - loss: 1.9770 - accuracy: 0.3300\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 398ms/step - loss: 1.9023 - accuracy: 0.2300\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 493ms/step - loss: 1.7980 - accuracy: 0.3200\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 433ms/step - loss: 1.9353 - accuracy: 0.3400\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 457ms/step - loss: 1.8395 - accuracy: 0.4300\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 484ms/step - loss: 1.8661 - accuracy: 0.4500\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 464ms/step - loss: 2.0370 - accuracy: 0.3400\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 456ms/step - loss: 1.8360 - accuracy: 0.3900\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 412ms/step - loss: 1.8267 - accuracy: 0.4100\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 396ms/step - loss: 1.7523 - accuracy: 0.4700\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 1s 506ms/step - loss: 1.7115 - accuracy: 0.4100\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 491ms/step - loss: 1.7616 - accuracy: 0.4600\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 421ms/step - loss: 1.9266 - accuracy: 0.4000\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 402ms/step - loss: 1.8250 - accuracy: 0.4600\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 396ms/step - loss: 1.8511 - accuracy: 0.3200\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 420ms/step - loss: 1.7646 - accuracy: 0.4400\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 494ms/step - loss: 1.8062 - accuracy: 0.4700\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 399ms/step - loss: 1.8859 - accuracy: 0.3600\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 477ms/step - loss: 1.9253 - accuracy: 0.3700\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 1s 651ms/step - loss: 1.8952 - accuracy: 0.3500\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 453ms/step - loss: 1.7207 - accuracy: 0.4300\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 417ms/step - loss: 1.7300 - accuracy: 0.5000\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 396ms/step - loss: 1.9299 - accuracy: 0.4600\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 446ms/step - loss: 2.0838 - accuracy: 0.3100\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 416ms/step - loss: 1.7397 - accuracy: 0.4900\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 415ms/step - loss: 1.8535 - accuracy: 0.5000\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 419ms/step - loss: 1.8042 - accuracy: 0.5500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 407ms/step - loss: 1.7290 - accuracy: 0.4400\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 449ms/step - loss: 1.7907 - accuracy: 0.3800\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 411ms/step - loss: 1.6193 - accuracy: 0.5600\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 423ms/step - loss: 1.7619 - accuracy: 0.4600\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 1s 514ms/step - loss: 1.6771 - accuracy: 0.4900\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 1s 530ms/step - loss: 1.7498 - accuracy: 0.4300\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 1s 501ms/step - loss: 1.8388 - accuracy: 0.4200\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 437ms/step - loss: 1.6871 - accuracy: 0.5000\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 442ms/step - loss: 1.6545 - accuracy: 0.5000\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 481ms/step - loss: 1.7412 - accuracy: 0.5500\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 481ms/step - loss: 1.8973 - accuracy: 0.3900\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 1s 565ms/step - loss: 1.6854 - accuracy: 0.5000\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 458ms/step - loss: 1.7033 - accuracy: 0.5400\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 445ms/step - loss: 1.8092 - accuracy: 0.4200\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 429ms/step - loss: 1.7990 - accuracy: 0.5100\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 410ms/step - loss: 1.7344 - accuracy: 0.5500\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 1s 594ms/step - loss: 1.5728 - accuracy: 0.5900\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 1s 521ms/step - loss: 1.9038 - accuracy: 0.4800\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 478ms/step - loss: 1.7875 - accuracy: 0.5500\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 432ms/step - loss: 1.6816 - accuracy: 0.4500\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 394ms/step - loss: 1.6746 - accuracy: 0.5500\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 1s 517ms/step - loss: 1.7346 - accuracy: 0.5100\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 1s 521ms/step - loss: 1.7088 - accuracy: 0.5900\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 1s 501ms/step - loss: 1.5987 - accuracy: 0.6200\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 417ms/step - loss: 1.6203 - accuracy: 0.6000\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 424ms/step - loss: 1.6025 - accuracy: 0.5600\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 433ms/step - loss: 1.5914 - accuracy: 0.6100\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 1s 548ms/step - loss: 1.4533 - accuracy: 0.6400\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 0s 478ms/step - loss: 1.6238 - accuracy: 0.5600\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 0s 482ms/step - loss: 1.4944 - accuracy: 0.6800\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - 0s 480ms/step - loss: 1.5491 - accuracy: 0.6000\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 1s 516ms/step - loss: 1.4763 - accuracy: 0.6500\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 1s 568ms/step - loss: 1.6128 - accuracy: 0.6200\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 1s 502ms/step - loss: 1.6064 - accuracy: 0.5600\n",
      "Epoch 117/200\n",
      "1/1 [==============================] - 0s 445ms/step - loss: 1.4777 - accuracy: 0.7000\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 0s 485ms/step - loss: 1.6301 - accuracy: 0.7100\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 1s 520ms/step - loss: 1.5685 - accuracy: 0.6900\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 0s 497ms/step - loss: 1.7516 - accuracy: 0.5900\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 0s 408ms/step - loss: 1.5904 - accuracy: 0.6700\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 0s 427ms/step - loss: 1.4901 - accuracy: 0.7000\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 0s 426ms/step - loss: 1.4511 - accuracy: 0.7700\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 1s 501ms/step - loss: 1.5739 - accuracy: 0.7100\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 0s 394ms/step - loss: 1.4161 - accuracy: 0.7000\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 0s 433ms/step - loss: 1.4686 - accuracy: 0.7200\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 1s 513ms/step - loss: 1.2567 - accuracy: 0.7900\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 0s 482ms/step - loss: 1.3472 - accuracy: 0.7600\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - 1s 510ms/step - loss: 1.1856 - accuracy: 0.7900\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 0s 431ms/step - loss: 1.3428 - accuracy: 0.8000\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 0s 429ms/step - loss: 1.2795 - accuracy: 0.7400\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 0s 418ms/step - loss: 1.2676 - accuracy: 0.7700\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 0s 447ms/step - loss: 1.2603 - accuracy: 0.8100\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 0s 425ms/step - loss: 1.3487 - accuracy: 0.7900\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - 0s 405ms/step - loss: 1.3448 - accuracy: 0.7900\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - 0s 406ms/step - loss: 1.3227 - accuracy: 0.7600\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - 0s 400ms/step - loss: 1.2268 - accuracy: 0.7700\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - 0s 402ms/step - loss: 1.1761 - accuracy: 0.8200\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - 0s 492ms/step - loss: 1.0414 - accuracy: 0.8000\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - 1s 525ms/step - loss: 1.0484 - accuracy: 0.8600\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - 1s 602ms/step - loss: 1.3186 - accuracy: 0.7000\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - 1s 587ms/step - loss: 0.9328 - accuracy: 0.8400\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - 0s 422ms/step - loss: 1.2756 - accuracy: 0.7400\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - 0s 395ms/step - loss: 1.0403 - accuracy: 0.8500\n",
      "Epoch 145/200\n",
      "1/1 [==============================] - 0s 405ms/step - loss: 1.1295 - accuracy: 0.8400\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - 0s 390ms/step - loss: 1.0721 - accuracy: 0.8000\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - 0s 452ms/step - loss: 1.1229 - accuracy: 0.8000\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - 0s 448ms/step - loss: 1.1792 - accuracy: 0.8100\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - 0s 407ms/step - loss: 1.2494 - accuracy: 0.7500\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - 0s 398ms/step - loss: 1.0610 - accuracy: 0.8400\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - 0s 402ms/step - loss: 0.9842 - accuracy: 0.8800\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - 0s 404ms/step - loss: 1.2030 - accuracy: 0.8300\n",
      "Epoch 153/200\n",
      "1/1 [==============================] - 0s 469ms/step - loss: 1.2027 - accuracy: 0.8200\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - 0s 420ms/step - loss: 1.1153 - accuracy: 0.8200\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - 0s 414ms/step - loss: 1.1312 - accuracy: 0.8300\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - 0s 428ms/step - loss: 0.9552 - accuracy: 0.8700\n",
      "Epoch 157/200\n",
      "1/1 [==============================] - 1s 501ms/step - loss: 0.8304 - accuracy: 0.8900\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - 0s 451ms/step - loss: 0.9378 - accuracy: 0.8600\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - 0s 481ms/step - loss: 1.0017 - accuracy: 0.8600\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - 0s 429ms/step - loss: 1.0568 - accuracy: 0.8400\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - 0s 458ms/step - loss: 1.2993 - accuracy: 0.7500\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - 0s 440ms/step - loss: 0.8828 - accuracy: 0.8700\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - 0s 403ms/step - loss: 0.9532 - accuracy: 0.8300\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - 0s 408ms/step - loss: 0.9499 - accuracy: 0.8400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 165/200\n",
      "1/1 [==============================] - 0s 397ms/step - loss: 1.0153 - accuracy: 0.8400\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - 0s 496ms/step - loss: 0.9714 - accuracy: 0.8200\n",
      "Epoch 167/200\n",
      "1/1 [==============================] - 0s 429ms/step - loss: 0.9122 - accuracy: 0.8800\n",
      "Epoch 168/200\n",
      "1/1 [==============================] - 0s 422ms/step - loss: 0.7537 - accuracy: 0.8900\n",
      "Epoch 169/200\n",
      "1/1 [==============================] - 0s 421ms/step - loss: 1.0840 - accuracy: 0.8000\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - 0s 478ms/step - loss: 0.8857 - accuracy: 0.8600\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - 0s 397ms/step - loss: 0.8219 - accuracy: 0.9000\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - 0s 465ms/step - loss: 0.8821 - accuracy: 0.8800\n",
      "Epoch 173/200\n",
      "1/1 [==============================] - 0s 488ms/step - loss: 0.9074 - accuracy: 0.8400\n",
      "Epoch 174/200\n",
      "1/1 [==============================] - 0s 468ms/step - loss: 0.8401 - accuracy: 0.8100\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - 0s 406ms/step - loss: 0.7869 - accuracy: 0.8800\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - 1s 570ms/step - loss: 0.8933 - accuracy: 0.8200\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - 1s 502ms/step - loss: 0.8218 - accuracy: 0.8700\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - 0s 393ms/step - loss: 0.7713 - accuracy: 0.8800\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - 0s 394ms/step - loss: 0.7897 - accuracy: 0.8800\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - 0s 390ms/step - loss: 0.8196 - accuracy: 0.8600\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - 0s 450ms/step - loss: 0.8992 - accuracy: 0.8600\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - 1s 521ms/step - loss: 0.8338 - accuracy: 0.8700\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - 0s 415ms/step - loss: 0.6767 - accuracy: 0.9100\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - 1s 533ms/step - loss: 0.6151 - accuracy: 0.9200\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - 0s 462ms/step - loss: 0.7791 - accuracy: 0.9000\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - 0s 403ms/step - loss: 0.7945 - accuracy: 0.8600\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - 0s 463ms/step - loss: 0.5990 - accuracy: 0.9300\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - 0s 396ms/step - loss: 0.6582 - accuracy: 0.9100\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - 0s 397ms/step - loss: 0.7939 - accuracy: 0.8600\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - 1s 575ms/step - loss: 0.8113 - accuracy: 0.8200\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - 1s 503ms/step - loss: 0.7566 - accuracy: 0.8900\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - 0s 393ms/step - loss: 0.7840 - accuracy: 0.8900\n",
      "Epoch 193/200\n",
      "1/1 [==============================] - 1s 538ms/step - loss: 0.8485 - accuracy: 0.8600\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - 0s 478ms/step - loss: 0.7729 - accuracy: 0.8500\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - 1s 586ms/step - loss: 0.7796 - accuracy: 0.8800\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - 0s 424ms/step - loss: 0.7401 - accuracy: 0.8600\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - 1s 533ms/step - loss: 1.0405 - accuracy: 0.8000\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - 0s 448ms/step - loss: 0.9212 - accuracy: 0.8500\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - 0s 407ms/step - loss: 0.6944 - accuracy: 0.9100\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - 0s 463ms/step - loss: 0.7755 - accuracy: 0.8400\n",
      "Learning rate:  0.001\n",
      "Epoch 1/200\n",
      "1/1 [==============================] - 5s 5s/step - loss: 2.6075 - accuracy: 0.2800\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 1s 702ms/step - loss: 2.5126 - accuracy: 0.2100\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 1s 722ms/step - loss: 2.4222 - accuracy: 0.2300\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 1s 791ms/step - loss: 2.3174 - accuracy: 0.2400\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 1s 761ms/step - loss: 2.3210 - accuracy: 0.2100\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 1s 725ms/step - loss: 2.2394 - accuracy: 0.2600\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 1s 730ms/step - loss: 2.1692 - accuracy: 0.2900\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 1s 719ms/step - loss: 2.2015 - accuracy: 0.2000\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 1s 735ms/step - loss: 2.2055 - accuracy: 0.3300\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 1s 862ms/step - loss: 2.2272 - accuracy: 0.2500\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 1s 819ms/step - loss: 2.0313 - accuracy: 0.3400\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 1s 797ms/step - loss: 2.1635 - accuracy: 0.2400\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 1s 942ms/step - loss: 2.1453 - accuracy: 0.3100\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 1s 970ms/step - loss: 2.1750 - accuracy: 0.3300\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.0713 - accuracy: 0.3300\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 1s 799ms/step - loss: 1.9207 - accuracy: 0.2900\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 1s 866ms/step - loss: 2.0219 - accuracy: 0.2500\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.1012 - accuracy: 0.3100\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.1683 - accuracy: 0.2900\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 1s 881ms/step - loss: 2.1249 - accuracy: 0.3400\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 1s 788ms/step - loss: 2.1029 - accuracy: 0.2800\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 1s 867ms/step - loss: 2.0860 - accuracy: 0.3700\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 1s 747ms/step - loss: 1.9675 - accuracy: 0.4400\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 1s 772ms/step - loss: 2.1476 - accuracy: 0.2900\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.1654 - accuracy: 0.3000\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.0048 - accuracy: 0.2600\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 1s 849ms/step - loss: 2.2228 - accuracy: 0.2700\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 1s 745ms/step - loss: 2.0016 - accuracy: 0.3500\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 1s 876ms/step - loss: 2.0281 - accuracy: 0.2800\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 1s 910ms/step - loss: 2.0056 - accuracy: 0.3600\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.0598 - accuracy: 0.4200\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 1s 907ms/step - loss: 1.9694 - accuracy: 0.3900\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 1s 805ms/step - loss: 2.0391 - accuracy: 0.4400\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 1s 783ms/step - loss: 1.9832 - accuracy: 0.3900\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 1s 776ms/step - loss: 1.8384 - accuracy: 0.5100\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 1s 821ms/step - loss: 2.0884 - accuracy: 0.4100\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 1s 825ms/step - loss: 2.0832 - accuracy: 0.4400\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 1s 770ms/step - loss: 2.0643 - accuracy: 0.3800\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 1s 906ms/step - loss: 1.9497 - accuracy: 0.4200\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 1s 993ms/step - loss: 2.0138 - accuracy: 0.4600\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 1s 780ms/step - loss: 2.0990 - accuracy: 0.4600\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.9257 - accuracy: 0.4900\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 1s 765ms/step - loss: 1.9441 - accuracy: 0.4500\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 1s 949ms/step - loss: 1.8850 - accuracy: 0.4200\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 1s 827ms/step - loss: 2.0433 - accuracy: 0.4500\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 1s 815ms/step - loss: 1.8583 - accuracy: 0.4700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/200\n",
      "1/1 [==============================] - 1s 874ms/step - loss: 1.7326 - accuracy: 0.5400\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 1s 790ms/step - loss: 1.9526 - accuracy: 0.4800\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 1s 752ms/step - loss: 1.9863 - accuracy: 0.5300\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 1s 856ms/step - loss: 2.0536 - accuracy: 0.5000\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 1s 742ms/step - loss: 1.9716 - accuracy: 0.5300\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 1s 756ms/step - loss: 1.9040 - accuracy: 0.5300\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 1s 743ms/step - loss: 1.9234 - accuracy: 0.5000\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 1s 901ms/step - loss: 1.9825 - accuracy: 0.4600\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 1s 844ms/step - loss: 1.8491 - accuracy: 0.5100\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 1s 741ms/step - loss: 1.6626 - accuracy: 0.6600\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 1s 844ms/step - loss: 1.7159 - accuracy: 0.6300\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 1s 731ms/step - loss: 1.7079 - accuracy: 0.6000\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 1s 846ms/step - loss: 1.6531 - accuracy: 0.5500\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 1s 751ms/step - loss: 1.7150 - accuracy: 0.6400\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 1s 740ms/step - loss: 1.6668 - accuracy: 0.6200\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 1s 790ms/step - loss: 1.6438 - accuracy: 0.6600\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 1s 828ms/step - loss: 1.6111 - accuracy: 0.6200\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 1s 742ms/step - loss: 1.5549 - accuracy: 0.6600\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 1s 746ms/step - loss: 1.7137 - accuracy: 0.6000\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 1s 746ms/step - loss: 1.6927 - accuracy: 0.6300\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 1s 758ms/step - loss: 1.6390 - accuracy: 0.7400\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 1s 752ms/step - loss: 1.4705 - accuracy: 0.7200\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 1s 793ms/step - loss: 1.4862 - accuracy: 0.7800\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 1s 858ms/step - loss: 1.4679 - accuracy: 0.8000\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 1s 808ms/step - loss: 1.4031 - accuracy: 0.8000\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 1s 736ms/step - loss: 1.3317 - accuracy: 0.7700\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 1s 752ms/step - loss: 1.4700 - accuracy: 0.7800\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 1s 740ms/step - loss: 1.1880 - accuracy: 0.8400\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 1s 763ms/step - loss: 1.3304 - accuracy: 0.7600\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 1s 743ms/step - loss: 1.2702 - accuracy: 0.8000\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 1s 806ms/step - loss: 1.2862 - accuracy: 0.8100\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 1s 803ms/step - loss: 1.1391 - accuracy: 0.8900\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 1s 852ms/step - loss: 1.0979 - accuracy: 0.8700\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 1s 730ms/step - loss: 1.0520 - accuracy: 0.8900\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 1s 741ms/step - loss: 1.2191 - accuracy: 0.8400\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 1s 781ms/step - loss: 1.1419 - accuracy: 0.8300\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 1s 746ms/step - loss: 1.0920 - accuracy: 0.8600\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 1s 745ms/step - loss: 0.9158 - accuracy: 0.9000\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 1s 790ms/step - loss: 0.9514 - accuracy: 0.8900\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 1s 857ms/step - loss: 1.1220 - accuracy: 0.8300\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 1s 787ms/step - loss: 0.9720 - accuracy: 0.8800\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 1s 855ms/step - loss: 1.1172 - accuracy: 0.8400\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 1s 729ms/step - loss: 1.0269 - accuracy: 0.8700\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 1s 752ms/step - loss: 0.9496 - accuracy: 0.9000\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 1s 841ms/step - loss: 1.0496 - accuracy: 0.8300\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 1s 743ms/step - loss: 0.8202 - accuracy: 0.9100\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 1s 817ms/step - loss: 0.9621 - accuracy: 0.8600\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 1s 845ms/step - loss: 0.9730 - accuracy: 0.8400\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 1s 780ms/step - loss: 1.0666 - accuracy: 0.8400\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 1s 750ms/step - loss: 0.9548 - accuracy: 0.8600\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 1s 851ms/step - loss: 0.7102 - accuracy: 0.9500\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 1s 759ms/step - loss: 0.8927 - accuracy: 0.8600\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 1s 744ms/step - loss: 0.8035 - accuracy: 0.9200\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 1s 743ms/step - loss: 0.8740 - accuracy: 0.8600\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 1s 810ms/step - loss: 0.8884 - accuracy: 0.8700\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 1s 848ms/step - loss: 0.8527 - accuracy: 0.8900\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 1s 762ms/step - loss: 0.8012 - accuracy: 0.8700\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 1s 748ms/step - loss: 0.8943 - accuracy: 0.8700\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 1s 751ms/step - loss: 0.7566 - accuracy: 0.9400\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 1s 744ms/step - loss: 0.8459 - accuracy: 0.8800\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 1s 739ms/step - loss: 0.9040 - accuracy: 0.8300\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 1s 791ms/step - loss: 0.7539 - accuracy: 0.9100\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 1s 876ms/step - loss: 0.8947 - accuracy: 0.8700\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7371 - accuracy: 0.9000\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 1s 943ms/step - loss: 0.8513 - accuracy: 0.8700\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 1s 962ms/step - loss: 0.8068 - accuracy: 0.8700\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - 1s 770ms/step - loss: 0.7536 - accuracy: 0.9000\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 1s 948ms/step - loss: 0.6964 - accuracy: 0.9300\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7966 - accuracy: 0.8800\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 1s 910ms/step - loss: 0.7180 - accuracy: 0.9300\n",
      "Epoch 117/200\n",
      "1/1 [==============================] - 1s 977ms/step - loss: 0.6743 - accuracy: 0.9300\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 1s 926ms/step - loss: 0.7485 - accuracy: 0.9100\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 1s 862ms/step - loss: 0.9195 - accuracy: 0.8300\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 1s 826ms/step - loss: 0.6917 - accuracy: 0.9300\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 1s 756ms/step - loss: 0.7443 - accuracy: 0.8800\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 1s 812ms/step - loss: 0.8639 - accuracy: 0.8400\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 1s 764ms/step - loss: 0.7374 - accuracy: 0.9000\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 1s 851ms/step - loss: 0.6055 - accuracy: 0.9500\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 1s 895ms/step - loss: 0.7497 - accuracy: 0.9000\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 1s 834ms/step - loss: 0.7899 - accuracy: 0.8800\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 1s 760ms/step - loss: 0.6566 - accuracy: 0.9100\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 1s 767ms/step - loss: 0.5789 - accuracy: 0.9600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 129/200\n",
      "1/1 [==============================] - 1s 901ms/step - loss: 0.7515 - accuracy: 0.8700\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 1s 921ms/step - loss: 0.6790 - accuracy: 0.8900\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 1s 884ms/step - loss: 0.7440 - accuracy: 0.8900\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 1s 828ms/step - loss: 0.7462 - accuracy: 0.8700\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 1s 824ms/step - loss: 0.6557 - accuracy: 0.9400\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 1s 809ms/step - loss: 0.6348 - accuracy: 0.9300\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - 1s 794ms/step - loss: 0.6938 - accuracy: 0.9100\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - 1s 819ms/step - loss: 0.6993 - accuracy: 0.8900\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - 1s 974ms/step - loss: 0.6932 - accuracy: 0.9000\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - 1s 825ms/step - loss: 0.6540 - accuracy: 0.9200\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - 1s 840ms/step - loss: 0.6590 - accuracy: 0.9000\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - 1s 806ms/step - loss: 0.6533 - accuracy: 0.9100\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - 1s 736ms/step - loss: 0.7300 - accuracy: 0.8900\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - 1s 839ms/step - loss: 0.6683 - accuracy: 0.9400\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - 1s 712ms/step - loss: 0.7557 - accuracy: 0.8900\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - 1s 765ms/step - loss: 0.7008 - accuracy: 0.8900\n",
      "Epoch 145/200\n",
      "1/1 [==============================] - 1s 782ms/step - loss: 0.6475 - accuracy: 0.9300\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - 1s 722ms/step - loss: 0.6465 - accuracy: 0.9100\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - 1s 759ms/step - loss: 0.6510 - accuracy: 0.9100\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - 1s 981ms/step - loss: 0.6687 - accuracy: 0.9100\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - 1s 731ms/step - loss: 0.6543 - accuracy: 0.9100\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - 1s 737ms/step - loss: 0.5924 - accuracy: 0.9300\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - 1s 845ms/step - loss: 0.5977 - accuracy: 0.9500\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - 1s 795ms/step - loss: 0.7100 - accuracy: 0.8700\n",
      "Epoch 153/200\n",
      "1/1 [==============================] - 1s 744ms/step - loss: 0.6434 - accuracy: 0.8900\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - 1s 747ms/step - loss: 0.6363 - accuracy: 0.9100\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - 1s 734ms/step - loss: 0.7174 - accuracy: 0.8900\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - 1s 750ms/step - loss: 0.6524 - accuracy: 0.9100\n",
      "Epoch 157/200\n",
      "1/1 [==============================] - 1s 751ms/step - loss: 0.6470 - accuracy: 0.8900\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - 1s 839ms/step - loss: 0.6812 - accuracy: 0.8900\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - 1s 790ms/step - loss: 0.6310 - accuracy: 0.9000\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - 1s 795ms/step - loss: 0.5443 - accuracy: 0.9600\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - 1s 753ms/step - loss: 0.6807 - accuracy: 0.8800\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - 1s 744ms/step - loss: 0.7283 - accuracy: 0.8500\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - 1s 743ms/step - loss: 0.6624 - accuracy: 0.8800\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - 1s 738ms/step - loss: 0.6849 - accuracy: 0.8900\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - 1s 749ms/step - loss: 0.8246 - accuracy: 0.8100\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - 1s 749ms/step - loss: 0.6306 - accuracy: 0.9200\n",
      "Epoch 167/200\n",
      "1/1 [==============================] - 1s 761ms/step - loss: 0.6104 - accuracy: 0.9200\n",
      "Epoch 168/200\n",
      "1/1 [==============================] - 1s 790ms/step - loss: 0.6198 - accuracy: 0.9300\n",
      "Epoch 169/200\n",
      "1/1 [==============================] - 1s 859ms/step - loss: 0.6746 - accuracy: 0.9100\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - 1s 736ms/step - loss: 0.7166 - accuracy: 0.8400\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - 1s 743ms/step - loss: 0.6247 - accuracy: 0.9100\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - 1s 832ms/step - loss: 0.6642 - accuracy: 0.8800\n",
      "Epoch 173/200\n",
      "1/1 [==============================] - 1s 739ms/step - loss: 0.5827 - accuracy: 0.9700\n",
      "Epoch 174/200\n",
      "1/1 [==============================] - 1s 738ms/step - loss: 0.5968 - accuracy: 0.9300\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - 1s 748ms/step - loss: 0.6104 - accuracy: 0.9200\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - 1s 791ms/step - loss: 0.7152 - accuracy: 0.9000\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - 1s 742ms/step - loss: 0.7506 - accuracy: 0.8600\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - 1s 741ms/step - loss: 0.6133 - accuracy: 0.9400\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - 1s 748ms/step - loss: 0.7087 - accuracy: 0.8600\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - 1s 740ms/step - loss: 0.7195 - accuracy: 0.8900\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - 1s 764ms/step - loss: 0.5318 - accuracy: 0.9600\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - 1s 743ms/step - loss: 0.6043 - accuracy: 0.9200\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - 1s 763ms/step - loss: 0.7322 - accuracy: 0.8500\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - 1s 854ms/step - loss: 0.6689 - accuracy: 0.9000\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - 1s 752ms/step - loss: 0.5951 - accuracy: 0.9100\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - 1s 744ms/step - loss: 0.5802 - accuracy: 0.9400\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - 1s 747ms/step - loss: 0.6188 - accuracy: 0.9100\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - 1s 739ms/step - loss: 0.5635 - accuracy: 0.9300\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - 1s 756ms/step - loss: 0.6637 - accuracy: 0.8900\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - 1s 845ms/step - loss: 0.6590 - accuracy: 0.8700\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - 1s 799ms/step - loss: 0.6138 - accuracy: 0.9100\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - 1s 764ms/step - loss: 0.5675 - accuracy: 0.9300\n",
      "Epoch 193/200\n",
      "1/1 [==============================] - 1s 844ms/step - loss: 0.6369 - accuracy: 0.8900\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - 1s 732ms/step - loss: 0.6413 - accuracy: 0.8900\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - 1s 750ms/step - loss: 0.6848 - accuracy: 0.8800\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - 1s 740ms/step - loss: 0.6717 - accuracy: 0.8900\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - 1s 743ms/step - loss: 0.6441 - accuracy: 0.8900\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - 1s 838ms/step - loss: 0.5563 - accuracy: 0.9400\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - 1s 760ms/step - loss: 0.8003 - accuracy: 0.8300\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - 1s 832ms/step - loss: 0.7697 - accuracy: 0.8200\n",
      "Learning rate:  0.001\n",
      "Epoch 1/200\n",
      "1/1 [==============================] - 8s 8s/step - loss: 2.9034 - accuracy: 0.0200\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7374 - accuracy: 0.0500\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6449 - accuracy: 0.2000\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5264 - accuracy: 0.2800\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.4255 - accuracy: 0.2300\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.3669 - accuracy: 0.3100\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.3863 - accuracy: 0.3200\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.3847 - accuracy: 0.1800\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.3642 - accuracy: 0.2400\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.2204 - accuracy: 0.3600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.3130 - accuracy: 0.3400\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.3135 - accuracy: 0.2800\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.3580 - accuracy: 0.2500\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.2895 - accuracy: 0.2600\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.1915 - accuracy: 0.4200\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.2361 - accuracy: 0.3300\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.1403 - accuracy: 0.4100\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.2552 - accuracy: 0.2500\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.2616 - accuracy: 0.3500\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.1651 - accuracy: 0.3600\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.2049 - accuracy: 0.4100\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.3123 - accuracy: 0.3600\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.1259 - accuracy: 0.4700\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.1356 - accuracy: 0.4000\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.2321 - accuracy: 0.4100\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.3657 - accuracy: 0.3100\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.1915 - accuracy: 0.4600\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.0865 - accuracy: 0.4100\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.1642 - accuracy: 0.4300\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.0743 - accuracy: 0.4000\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.0013 - accuracy: 0.5300\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.1300 - accuracy: 0.4200\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.0434 - accuracy: 0.4000\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.2427 - accuracy: 0.4700\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.1523 - accuracy: 0.4900\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.1755 - accuracy: 0.4600\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.0845 - accuracy: 0.4500\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.0261 - accuracy: 0.5500\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.9498 - accuracy: 0.6400\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.1709 - accuracy: 0.5200\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.9707 - accuracy: 0.5700\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.9651 - accuracy: 0.5200\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.8404 - accuracy: 0.6100\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.0979 - accuracy: 0.4900\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.0913 - accuracy: 0.5500\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.8721 - accuracy: 0.6400\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.7579 - accuracy: 0.5300\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.8443 - accuracy: 0.5100\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.7899 - accuracy: 0.6200\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.7241 - accuracy: 0.5900\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.8352 - accuracy: 0.6000\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.8043 - accuracy: 0.5800\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.6768 - accuracy: 0.6100\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.7351 - accuracy: 0.6300\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.4960 - accuracy: 0.7100\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.4550 - accuracy: 0.8400\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.5739 - accuracy: 0.7700\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.5367 - accuracy: 0.7900\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.4083 - accuracy: 0.8000\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.3988 - accuracy: 0.8200\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.4892 - accuracy: 0.8000\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.3585 - accuracy: 0.8500\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.3368 - accuracy: 0.8600\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.4627 - accuracy: 0.7900\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.2269 - accuracy: 0.9000\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.1911 - accuracy: 0.9000\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.2048 - accuracy: 0.8700\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.2811 - accuracy: 0.8700\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.0791 - accuracy: 0.9200\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.1494 - accuracy: 0.8900\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.1190 - accuracy: 0.8600\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.1131 - accuracy: 0.8500\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.1110 - accuracy: 0.8300\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.9065 - accuracy: 0.9200\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.9779 - accuracy: 0.9100\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.9320 - accuracy: 0.9200\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.9786 - accuracy: 0.9100\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.1992 - accuracy: 0.8500\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.9215 - accuracy: 0.9300\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.0413 - accuracy: 0.8900\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.0571 - accuracy: 0.8800\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.9660 - accuracy: 0.8900\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.9828 - accuracy: 0.9000\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.9174 - accuracy: 0.9000\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.9302 - accuracy: 0.9000\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.9774 - accuracy: 0.8900\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.9274 - accuracy: 0.9000\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.0449 - accuracy: 0.8800\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.9028 - accuracy: 0.9000\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.9950 - accuracy: 0.8700\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.8833 - accuracy: 0.9100\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.9435 - accuracy: 0.8700\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7780 - accuracy: 0.9500\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.8718 - accuracy: 0.9200\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.8680 - accuracy: 0.9200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.0748 - accuracy: 0.8200\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.9355 - accuracy: 0.8700\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7974 - accuracy: 0.9300\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.0267 - accuracy: 0.8400\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.8761 - accuracy: 0.9200\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7849 - accuracy: 0.9400\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.9943 - accuracy: 0.8700\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.8488 - accuracy: 0.9400\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.8386 - accuracy: 0.9300\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.8370 - accuracy: 0.9200\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.9684 - accuracy: 0.8600\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.8286 - accuracy: 0.8900\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.8279 - accuracy: 0.8900\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7203 - accuracy: 0.9500\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.8195 - accuracy: 0.9100\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7408 - accuracy: 0.9400\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7672 - accuracy: 0.9300\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7361 - accuracy: 0.9500\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.8767 - accuracy: 0.8800\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.8772 - accuracy: 0.8800\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7880 - accuracy: 0.9100\n",
      "Epoch 117/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.8032 - accuracy: 0.9000\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.9544 - accuracy: 0.8600\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.8526 - accuracy: 0.8800\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.8687 - accuracy: 0.8800\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.8052 - accuracy: 0.9100\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.8975 - accuracy: 0.8700\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6791 - accuracy: 0.9600\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7928 - accuracy: 0.9100\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.9124 - accuracy: 0.8400\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7177 - accuracy: 0.9300\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.8342 - accuracy: 0.9000\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6900 - accuracy: 0.9700\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7007 - accuracy: 0.9600\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7303 - accuracy: 0.9300\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.8112 - accuracy: 0.8800\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7679 - accuracy: 0.9000\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7494 - accuracy: 0.9100\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7231 - accuracy: 0.9600\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6994 - accuracy: 0.9200\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7621 - accuracy: 0.9100\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7748 - accuracy: 0.9100\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7259 - accuracy: 0.9200\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7731 - accuracy: 0.9000\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.8371 - accuracy: 0.8800\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6885 - accuracy: 0.9300\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.8932 - accuracy: 0.8200\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7953 - accuracy: 0.8700\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7758 - accuracy: 0.9000\n",
      "Epoch 145/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7549 - accuracy: 0.9200\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7335 - accuracy: 0.9300\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6837 - accuracy: 0.9500\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7393 - accuracy: 0.9200\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.8226 - accuracy: 0.8700\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.8565 - accuracy: 0.8500\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7359 - accuracy: 0.9000\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7653 - accuracy: 0.9000\n",
      "Epoch 153/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7087 - accuracy: 0.9200\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7148 - accuracy: 0.9300\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.8460 - accuracy: 0.8900\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7816 - accuracy: 0.9200\n",
      "Epoch 157/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7422 - accuracy: 0.9300\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7990 - accuracy: 0.8900\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7341 - accuracy: 0.9200\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6983 - accuracy: 0.9700\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6807 - accuracy: 0.9400\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6978 - accuracy: 0.9400\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.8004 - accuracy: 0.8900\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7062 - accuracy: 0.9200\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7341 - accuracy: 0.9100\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7093 - accuracy: 0.9400\n",
      "Epoch 167/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7361 - accuracy: 0.9000\n",
      "Epoch 168/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7879 - accuracy: 0.9100\n",
      "Epoch 169/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7441 - accuracy: 0.9200\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7949 - accuracy: 0.8500\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.8719 - accuracy: 0.8600\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7269 - accuracy: 0.9200\n",
      "Epoch 173/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6779 - accuracy: 0.9400\n",
      "Epoch 174/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6855 - accuracy: 0.9300\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7524 - accuracy: 0.9200\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7707 - accuracy: 0.8900\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7747 - accuracy: 0.9000\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6873 - accuracy: 0.9200\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7169 - accuracy: 0.9300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 180/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6742 - accuracy: 0.9300\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7140 - accuracy: 0.9300\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7267 - accuracy: 0.9100\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.8105 - accuracy: 0.8800\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.7883 - accuracy: 0.8900\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7208 - accuracy: 0.9200\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6627 - accuracy: 0.9400\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.7153 - accuracy: 0.9000\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.7281 - accuracy: 0.9100\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6476 - accuracy: 0.9600\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7443 - accuracy: 0.9000\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7032 - accuracy: 0.9300\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7112 - accuracy: 0.9200\n",
      "Epoch 193/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6907 - accuracy: 0.9200\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6817 - accuracy: 0.9300\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7913 - accuracy: 0.8900\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6838 - accuracy: 0.9300\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7372 - accuracy: 0.8900\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7133 - accuracy: 0.8900\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7244 - accuracy: 0.9000\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6198 - accuracy: 0.9700\n",
      "Learning rate:  0.001\n",
      "Epoch 1/200\n",
      "1/1 [==============================] - 10s 10s/step - loss: 3.0321 - accuracy: 0.1000\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.8358 - accuracy: 0.2200\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7327 - accuracy: 0.2500\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7194 - accuracy: 0.2900\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6442 - accuracy: 0.2300\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5707 - accuracy: 0.3200\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5612 - accuracy: 0.2500\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.7080 - accuracy: 0.2300\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2.4862 - accuracy: 0.3200\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2.4761 - accuracy: 0.2900\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2.5060 - accuracy: 0.2500\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5586 - accuracy: 0.3500\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2.4338 - accuracy: 0.3500\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.4007 - accuracy: 0.3500\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.4395 - accuracy: 0.3300\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2.4265 - accuracy: 0.4300\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.4264 - accuracy: 0.3600\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2.4650 - accuracy: 0.2700\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2.4660 - accuracy: 0.2800\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.4494 - accuracy: 0.4000\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5608 - accuracy: 0.3600\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.2929 - accuracy: 0.3400\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.4876 - accuracy: 0.3500\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.4479 - accuracy: 0.3800\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.3895 - accuracy: 0.4700\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2.3403 - accuracy: 0.5200\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.4181 - accuracy: 0.5000\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.3887 - accuracy: 0.4200\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.2545 - accuracy: 0.5100\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.3335 - accuracy: 0.4200\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2.3985 - accuracy: 0.4700\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2.2091 - accuracy: 0.5700\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.4444 - accuracy: 0.4500\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2.3376 - accuracy: 0.5600\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2.2942 - accuracy: 0.5500\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2.0790 - accuracy: 0.6000\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2.2616 - accuracy: 0.4900\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2.2794 - accuracy: 0.5500\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2.1393 - accuracy: 0.6200\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2.0497 - accuracy: 0.5800\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2.1788 - accuracy: 0.5900\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2.1406 - accuracy: 0.6300\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2.0913 - accuracy: 0.6100\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2.0553 - accuracy: 0.6300\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.9003 - accuracy: 0.6900\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.8555 - accuracy: 0.6500\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.7451 - accuracy: 0.7300\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.7156 - accuracy: 0.7400\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.9613 - accuracy: 0.6900\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.8312 - accuracy: 0.6600\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.7445 - accuracy: 0.7000\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.6403 - accuracy: 0.7700\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.7605 - accuracy: 0.7200\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.6428 - accuracy: 0.8100\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.6972 - accuracy: 0.7700\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.4075 - accuracy: 0.9200\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.6585 - accuracy: 0.7900\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.4932 - accuracy: 0.7900\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.6002 - accuracy: 0.8100\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.5284 - accuracy: 0.8100\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.3125 - accuracy: 0.8800\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.2064 - accuracy: 0.9300\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.3284 - accuracy: 0.8600\n",
      "Epoch 64/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step - loss: 1.3831 - accuracy: 0.8100\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.1828 - accuracy: 0.9000\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.2649 - accuracy: 0.8500\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.2891 - accuracy: 0.8500\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.0800 - accuracy: 0.9300\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.2622 - accuracy: 0.8500\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.0967 - accuracy: 0.8900\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.2820 - accuracy: 0.8500\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.1214 - accuracy: 0.8900\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.9876 - accuracy: 0.9200\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.0915 - accuracy: 0.9100\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.1207 - accuracy: 0.9000\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.0571 - accuracy: 0.8900\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.1301 - accuracy: 0.8700\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.1824 - accuracy: 0.8700\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.0269 - accuracy: 0.9200\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.9663 - accuracy: 0.9300\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.0247 - accuracy: 0.9200\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.0047 - accuracy: 0.9300\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.1629 - accuracy: 0.8400\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.0856 - accuracy: 0.8800\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.1058 - accuracy: 0.8600\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.0764 - accuracy: 0.8600\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.9630 - accuracy: 0.9200\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.0558 - accuracy: 0.9100\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.9819 - accuracy: 0.9100\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.1073 - accuracy: 0.8500\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.9641 - accuracy: 0.9200\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.9229 - accuracy: 0.9100\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.1303 - accuracy: 0.8400\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.1034 - accuracy: 0.8500\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.9999 - accuracy: 0.9100\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.9733 - accuracy: 0.9100\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.0208 - accuracy: 0.8700\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.0476 - accuracy: 0.8700\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.9913 - accuracy: 0.8800\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.9568 - accuracy: 0.9200\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.9676 - accuracy: 0.9000\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.9071 - accuracy: 0.9200\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.0608 - accuracy: 0.8600\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.9706 - accuracy: 0.9200\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.9423 - accuracy: 0.9300\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.9816 - accuracy: 0.9000\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.0364 - accuracy: 0.8900\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.9352 - accuracy: 0.9200\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.0383 - accuracy: 0.8700\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.9014 - accuracy: 0.9300\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.9797 - accuracy: 0.8900\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.0138 - accuracy: 0.8900\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.9416 - accuracy: 0.9100\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.8841 - accuracy: 0.9300\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.9483 - accuracy: 0.9200\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.9023 - accuracy: 0.9000\n",
      "Epoch 117/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.9483 - accuracy: 0.8900\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.9596 - accuracy: 0.9100\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.9983 - accuracy: 0.8600\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.8597 - accuracy: 0.9400\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.8311 - accuracy: 0.9500\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.9541 - accuracy: 0.8800\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.9996 - accuracy: 0.9100\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.0565 - accuracy: 0.8700\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.9429 - accuracy: 0.8800\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.9292 - accuracy: 0.9400\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.9452 - accuracy: 0.8900\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.9780 - accuracy: 0.9000\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.9963 - accuracy: 0.9100\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.8431 - accuracy: 0.9400\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.8650 - accuracy: 0.9400\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.9327 - accuracy: 0.9000\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.8881 - accuracy: 0.9100\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.9733 - accuracy: 0.8600\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.9348 - accuracy: 0.9000\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.8960 - accuracy: 0.9200\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.8747 - accuracy: 0.9100\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.9202 - accuracy: 0.8900\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.9206 - accuracy: 0.9100\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.8895 - accuracy: 0.9200\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.8213 - accuracy: 0.9600\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.8784 - accuracy: 0.9300\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.8459 - accuracy: 0.9300\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.9205 - accuracy: 0.9300\n",
      "Epoch 145/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.9266 - accuracy: 0.9000\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.8966 - accuracy: 0.9000\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.8670 - accuracy: 0.9300\n",
      "Epoch 148/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step - loss: 0.7790 - accuracy: 0.9800\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7905 - accuracy: 0.9600\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7988 - accuracy: 0.9600\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.9499 - accuracy: 0.9100\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.8093 - accuracy: 0.9500\n",
      "Epoch 153/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.9545 - accuracy: 0.8800\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.8259 - accuracy: 0.9400\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.8873 - accuracy: 0.9100\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.8509 - accuracy: 0.9400\n",
      "Epoch 157/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.7979 - accuracy: 0.9300\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.9446 - accuracy: 0.9100\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.8143 - accuracy: 0.9400\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.9122 - accuracy: 0.8800\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.9008 - accuracy: 0.8900\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.9994 - accuracy: 0.8700\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.9314 - accuracy: 0.9000\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.9389 - accuracy: 0.8800\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.9346 - accuracy: 0.8900\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.8292 - accuracy: 0.9400\n",
      "Epoch 167/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.9172 - accuracy: 0.9100\n",
      "Epoch 168/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.8829 - accuracy: 0.9200\n",
      "Epoch 169/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.8982 - accuracy: 0.9000\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.8245 - accuracy: 0.9500\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.7706 - accuracy: 0.9500\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.9640 - accuracy: 0.8800\n",
      "Epoch 173/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.0215 - accuracy: 0.8300\n",
      "Epoch 174/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.9653 - accuracy: 0.8600\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.8371 - accuracy: 0.9500\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.9059 - accuracy: 0.8900\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.8308 - accuracy: 0.9400\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.8650 - accuracy: 0.9300\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.8830 - accuracy: 0.9100\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.8440 - accuracy: 0.9100\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.8696 - accuracy: 0.9000\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.8057 - accuracy: 0.9300\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.8317 - accuracy: 0.9300\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.8450 - accuracy: 0.9300\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.7955 - accuracy: 0.9400\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.7998 - accuracy: 0.9400\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.8501 - accuracy: 0.9000\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.8004 - accuracy: 0.9600\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7628 - accuracy: 0.9300\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.8264 - accuracy: 0.9100\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.7321 - accuracy: 0.9600\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.7687 - accuracy: 0.9500\n",
      "Epoch 193/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.8546 - accuracy: 0.9100\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.8430 - accuracy: 0.9100\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.8770 - accuracy: 0.9200\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7963 - accuracy: 0.9500\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7509 - accuracy: 0.9500\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.7815 - accuracy: 0.9300\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.8355 - accuracy: 0.9000\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.9766 - accuracy: 0.8400\n"
     ]
    }
   ],
   "source": [
    "histories = {}\n",
    "for n in range(1, 5):\n",
    "\n",
    "    depth = n * 9 + 2\n",
    "    model = resnet_v2(input_shape=next(data_generator)[0].shape[1:], depth=depth, num_classes = 9)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=Adam(learning_rate=lr_schedule(0)),\n",
    "                  metrics=['accuracy'])\n",
    "    model.fit_generator(data_generator, steps_per_epoch = 1, epochs = 200)\n",
    "    histories[\"N = {}\".format(n)] = model.history.history['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x14f39e940>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nOydd3hcxdn2f2d7065WK2nVm5tsueGCDRiwcQKmh9BCKIGEAG8ghryUNNJISEgjCXm/VAikUmNCTAfjGIN7L7Jsy6qrslppey/nfH/MamW5g21Iwt7X5cv2njlz5pw9e8899zzzjKQoCnnkkUceefznQ/VhNyCPPPLII48Tgzyh55FHHnn8lyBP6HnkkUce/yXIE3oeeeSRx38J8oSeRx555PFfAs2HdeHi4mKlrq7uw7p8Hnnkkcd/JDZu3DioKErJoY59aIReV1fHhg0bPqzL55FHHnn8R0KSpM7DHctbLnnkkUce/yU4KqFLkvQHSZIGJEnacZjjkiRJj0iS1CpJ0jZJkmac+GbmkUceeeRxNByLQn8CWHSE4+cD47J/bgF+ffzNyiOPPPLI473iqB66oihvS5JUd4QilwJ/UkQOgTWSJBVKklSuKErfe21MKpXC5XIRj8ff66n/FTAYDFRVVaHVaj/spuSRRx7/gTgRk6KVQPd+/3dlPzuI0CVJugWh4qmpqTmoIpfLRUFBAXV1dUiSdAKa9p8DRVEYGhrC5XJRX1//YTcnjzzy+A/EBzopqijK7xRFmaUoyqySkoOjbuLxOA6H4yNH5gCSJOFwOD6yo5M88sjj+HEiCL0HqN7v/1XZz94XPopkPoyP8r3nkUcex48TQej/BG7IRrvMBQLvxz/PI488/jOQaG0lvPKdD7sZeRwCxxK2+CSwGpggSZJLkqTPSZJ0myRJt2WLvAy0Aa3A74EvnLTWfgCQJIm777479/+f/OQnfPvb3z7uer/+9a9TXV2NxWI57rryyOPDhOeRX+K64w4yodCH3ZQ8DsBRCV1RlGsURSlXFEWrKEqVoiiPKYryG0VRfpM9riiKcruiKGMURZmiKMp/9PJPvV7PkiVLGBwcPKH1Xnzxxaxbt+6E1pnHRxcZOcM/9/2TaCr6gV872d6OkkgQfOWV46onLad5se1FMnLmxDQsEYLmF05MXScJqUyKX2/9NbuGdp2U+vMrRQ+ARqPhlltu4Wc/+9kJrXfu3LmUl5ef0Drz+Ohiq2crX3/n63x3zXf5IHcdUzIZkp1i5Xng+X8cV10rXSv56sqv8rbr7RPRNNj+HDxzA3h2n5j6TgI8MQ+/2vIrmoeaT0r9H1oul6PhO0t30twbPKF1Tqqw8q2Lm45a7vbbb2fq1Kncd999hy2zfPlyvvSlLx30uclkYtWqVcfVzjzyANg5uJNCQyGVlsqDjvWFewF4se1FTi07lcvGXXbIOlb3rqapuAmrznrEaynJJOGVK7Gcc84RJ+dTff0oySS6hgZimzcz+NvfYZwyGfPpp7+HOxNo8+8DYH3/ehbULHjP5x+EsFv83bcNSiYc2zm7X4Ex54BGf8RimWCQ2LbttI43U24ux2l2Et28GY3dju49JBl0R0UbnWbnMZ/zXpBX6IeA1Wrlhhtu4JFHHjlsmQULFrBly5aD/uTJPI8ThXtW3MPPNh56pOjuEfZdk7maX2391SFVuj/u59Y3buUvzX856rUCL76E6/Y7iG3efMRyyY4OAEq+eAcqkwnPz35G9+13oMjyUa9xIDpdqwHY0P2v93zuIRHJ2qT9246t/EALPPkp2Pb0UYv6nn6a7ptv5v7nbuU3234DQM+X/pfOG28i7fMdcxOHCb3UVHrM57wX/Nsq9GNR0icTd911FzNmzOCmm2465PG8Qv/PQWdQWAS11trRB4J9EPOC8/jeNVmRWd27mtMrTh+lbhVFYXXfamY7Z6NVH9vq39iWLWgqKtCUlOCJeWjzt8G+5TBmtIIdCLRjlmU+YWvkwd43cIVdVBdUjyrTEexAQTkmvzbeLCyA6Nq1mGYcOh2Toii0bFmGGTDNmsW4Ve/if/pp3D94iPTGl1HVTiPRM4jplFOO6V47wmI9Yku4m0AigE1vO6bzhhFZu45kZwfa8nIsZ54JEY840L8dJZ0mum7dkUcOgex6yL6tR71WqkuUrdsXpntsN5lwhHR/PwC9X/4y1b/5DZJqRB8rskzwlVeQIxEMTU0Ym8Q75o5kFbopr9A/UBQVFXHVVVfx2GOPHfJ4XqH/5+CB1Q9w74p7Dz7w1nfhqU8fd/2reldx25u3scE9Oh5gbf9abn3jVt7sevOY6olu2EDHtdcx9JvfEE1HSWQSdAXakf/8CejfPqqsO9KPM51htkpETW3oPzgWoT3QDkCLr+Wo1463CNKPHGHifl3/Ot5e/RQZkx51cTEqgwF940QAEo99lsEH76Xrhs+gpFLHdL+diSHqkikUYJN70zGdMwwlmaT75pvp/+a36L7lVjKBAESHxMH+7YSWLaPrs58jvnvP4SsJ9uTKHw2pHlF2UpdCb7g3N1IxnXoqkbdXEt+5c1T56Jo19N59j2jfrbflnslAdACD2nBUC+z9Ik/oR8Ddd999wqJd7rvvPqqqqohGo1RVVZ2QUMg8jg19kT5avC0EEoHRB4I9EOyFA+yKcDKcm7SKpqJsGdgiDqST4DqYOIdHAM1DzSiKwtq+taTlNC+0ioiLnvDR19mlfT567r4HMhlSPb14414AEkqafo0aPLtRFIXoxo0oisJAwk9pJk1DKk2RoYieN5YSWLqUeEuWvCNDdPatB6A/0o8/7j/stRVZJrFLnBfbtBnZ037Icv9o/QcVQ+AtMeRGIrr6OgCSITXxNhdKKkXa40GORoltFwla5USC6Pr1o+oaat0J/gQXhyPoUfF86/Ms61xGWk4fdN1ty58lHhsdIpns7kZJpTCfdSYoCql+N+mIhx06HUQHSXfuFeW6OlFkmciatSjeTvB3jVQSzC6X6d8BB1hG0U2bcT/zZ3b+4xfACKE3dSn0RfqIt7cBUHTjZwCIN48eBUXWrgONhrIHvkNmcJDw83+ATBp31I3T7DxpiwjzhH4AwuFw7t9Op5NoNHpCyPdHP/oRLpcLWZZxuVx5Qv8AMRgbREFho3vj6AORIcgkRbjbfvjpxp9ywys3kJbTPLvnWa5/5XqWdS6D7c/Aox+DwGiC7s1OULZ4W1jfv56bX7+ZB9c+yJudQpkPD7OPhMDz/yDtdqOrryflduOLj/iyHVotDLUSeXcVnddeR2zzFtxyHGc6gxQd4rz4OBY+spree++j+9bs8pC3f0znjmdydez2HT7yI9XTgxyJYD77LJREgvijdxxUJpwM82bnm5T7FNptiZxnr3EUodJCMqgh4RKdRqrfje/pZ+i4+mpS7gH8Tz9D5/U34F/yfK4+9+2Lufk1mXGpFHM0NpZ3L+euf93Fqt7RI9yOvRtR/883eeeX94/6fFghW+bNAyDt7ucJxc81lWX8w2Im7dqbu7fIu6vouvFGBr98Lfxjv2Uywwo9FQHfSCcWWraMzk9/Gu83v4/8td8QGWgn1dtL0qSjNAB2bwp/azNIEubTT0dlseRGOMOIrluHcfJkCi+7DHWhjcDvfgAtLzIQHThp/jnkCT2PY0S8ufmoE1+JtnbkSOS4rtM81EzK7yPZJZRUJhQi0dr6vuuLpqLE0jFARFOMPihGX7v7N5CSxZA4lo7xavurJDIJBmODOXX9jXe/QXfvVmJeDXjbRlUzTOg9HdvZvFOQ+HN7niOeiWNQG/APdJDauUYUjvlg14vQ8jIkR55VurcHldGA6dRTSbvdOYUO0KHRwOBekh2CdGItOxmUFEozGYh4mNNrBEB79aWk3W7SHg/4O+nQqpmWEsTb4j287RLfJcgofPGZgIJ/dTvBN94g+MYbQpn2bOT1jtfIxGOUBKHdlqQj2AGA1PE2OkuS6ICBTEzEk6cH3MTaWkGWia5fT2SNuPe+73yHgR0bSLndSJ29lPkU6lIpHs4U8ezFzwKwLxv5Moxdm14XJLUhO9Hp2Q2ZFMl28SxMc+YCkOrr43mDUL0POuz4eoTVkurpJbFXkPvgyiHCOztY37+eZZ3LWBbcw9sWG/G4iuALTxF84w0CL71E71e/hm7SRP50ngqNDFue/CVKKkXLdOHxT+pSCLfuRiorxZcJYWioIrGrBVmR2erZSiYSIbZ9O9Ep9aDRYJs3mVCvgXR/B+6I+6T555An9DyOAYm2Nto/eTmhZcsOW0bJZOi44gqGnnjifV+nK9jFDUuuovmqy2i79BPEd++m67Ofo/PGQ09MHwsGYyOW2SiPW1EgMohHreKqFXfxctvLALzV9RbhlBiluaNu3BE3pcZSUBTeffw1Ol4vIdmyZdQ1hkn/or+2UfPTvzOxaCKTHZOZ5JjEDOcMTv/9GvZdfROJfftg+Q/g6WvhqWtgxQ9zdaT3bkCtDqN1WMn4fPiDA7ljnVmF3r1H+Mx7Nr5KRpIoS2cgOkjlXj8uB7SfUgZAvGU3maCLLq2OGdEwpSoDu72HV+iJlhYUlcR1/Q+hKk4T2KPQ88XF9HxxMa7bbyH42EJ+t/HnnBavQlKg17Ff57jlb+gKVSQCI/EVqb5+du78FwC+d1cQ3bAB1ZlzCKqTbP3W3UTXiXNLA1CZTKOPDNJY1EiRoSjXUQyjf5e4Z8eeAZSWV+H/nQpvfY9ERwfq4mL0DfUgSbj2rqdLq+Wu4tMwo2L7gLDBUj09JDs6UNls6KwZdr+T4rOv3sRd/7qLu+Q+bi+xsWm7g54f/YWeLy6m9+57ANi3+FzWNogOwvOmiJNf7/QQNquZsU8h3dlFs9nPTf+8AnV8I/GWXfy/Tb/kupevY9vyZyGd5sHkC6zsWYltejHIEsHV2xmI5RV6HicRiiyTaGs7YpnEnqza6eo6bJmMz4ccjZLo7sp5yqOuk8kQ370Hd8Sd83N9cd8oa6En3MNnX5fRdruRNBo6rrqa+PbtZAYHUZLJQ1432dV1yFFByu0m7fPlCH1G6Qx2e3ezvGs5/ZF+iAdATtGp1SKj0BvoAH83K979G0ZFRKQM7WvG6+9nnH0cPw1fwLQdQsVH1o9M4KUGBgh4XDgMDsqHFMpcUU4vm8ufL/gzj5/3OLWJAsa3plHS0HPnnci9O8E5RcQ+b30aMsIzTnu9aAwZNEahqMN9IqqiIZmiQ2+AoX0kXOIz7z5BzqWmMpSQB7buYleNitZiUVe8ZRd9YTdJCWqLJjAhFqXF20Kqtzf3HPt7N5LIjlDiu1qIVNhJaSVC54aoP2+A+sd+iv3aa4nvbufBAgfupJ/blDMBGBxbIiZh4wHYtRTduMbhbxlJq2XbruWo+8UEZfSV15CDQZ6q7OaN6VDRPEDgxaUA6NKgwpaLH6+z1tER6ABgt3c3GTlDIqvEDXGZ8GNfIBlWE9/8JKG9u9DV1SJptaiLHXS2bcEoy1xTOZ+Hpt6BOiaeY6rHRbK9HX19LZGmKCafik9lTuG5i5/jOU+YCSoLul4tBROs1P/jeSqeew7Lcy/wnO91tKYMUT3Udon3q9mhwjOrkll7FTRd/XTYkrQnvbxUrUOJJ3hxxaPi+1m1EkWtYneVxI7BHehNPtT6DP5dHaTldF6h53HyEFm5krYLLyLpOvzE3bBfmeo/vBeczk4euzp3cNkLl42yDACCr7xK+6WX8scff4brXrkOV8jFlUuv5MqlV+ZIPbCnmQXbFdYtrKLy4YdRUik0FWJ1bdp36Em9jmuvpevmzx8UWeFavJiOT32KoUEXAJeMuQQFhcXLF/O///rfXEREr0YoS1/bMsL/byHXPbSJL++sxxJVcN7yfaa+0YbT7MTxx1fxVEn4zODdsTd3nc7P38w1L/hZWHEWRSEwpODUTA0alQaT1sSUNS5UChTP8JNo3Yd3RRuUTYZZn4VwP7QtF/fnj6Axymj0wh5K9PViVOloTCbpNJohGULVJ56/YUCMIJzOqcRdfpRIhP4JxbSm+9BWVpJobqYzLSaAa8ddyORoiP6+vey98AK6b/sf9vRv5eLXP8MD/7iaZFcX0bVr6asyie/AIGGwpzHYUxScey4oCv1DBhZ7fRRubUVXW8vYcaey2bMZdiyBdBzd7HMB0BVkUJWV0t66kdKQipgOpLjoQN5yuPGeMw2VApEVb5PQCfWbMjYJ60vOUGeroyPYwV7fXq5YegV3r7gb20CUYIEgZ9cWNftecvLUDvC1NqPNLujRljqJud2cE41hslYwd+atVMfFQqFEdyeJjnZ0lU5+PMNMUgPX77EzwVLNhLCXaxJ1mGMQrgihGTeem98JsOiJVawJ7uWiWAKPU4shIb5rd6GEZmYZ2gxokhl6iyQuDkV4scYAwJxwKTWyHdvyzXga7CR0Ei3eFqShVkylSeJ7PaAoeULP4+Qh5XaDopAeGDhsmWG/Mu0+AqF7BKFnBodIyamDJiAj2XDO8/7eSWZfB5/85yfxxr14416+/s7XkRU555W/3hDCPO8Mxv5rOc4vf0XU6x066JqZYJCMZ5DY5s24f/ADouvXkwkLNZXo7CTV2YX5x78HRWF+9XyWXLKEc4vOwLKzi9iGNSgK9GjUAHhjg3j6gmgzMG51DwuaVaiSaQrdESoUGxmvF3NlkuYaiXibD0VRSA8Oktq9l5oBhbmqcbkf01iPqFNRFKreaaWlEjITExjHV+HfnabfWgbjzkPRFZFa8Qfx/EIJNIYMoWyi0ox7gOq4ljHRFL1KkrgkoR0QnVqpH9QZhVJHI9G+LDFOHUdnsBP9xEbizTvplDQUBRXqJl3BdUk15zdLSLEEkVWreOmBzxGXJN6K9NO1+A5Qq1m6QPjwQa0gJ4ZaMU6fRloNp3QpfNoXIrphA6ZJtYxBS3+kn+imJ6CkEd0pIkZeX5giYFGo68ugTclsaRJ1BkpMGJxOLj37NnZVierfzYr6lLoaFBkig9RZ6/DGvbzR+QYAy7qWUeFV0Jak6CkCujWgQOMONbYIpCpLIJMCu5mCYJoJySSYxT4LloSWjAREE2Q8gwTtsMWqI1ybIrbsXSIrXiOdUDHH4xDXqozz8Ou7WN/hI6beiwLMs40hWCXqSxsUklqJMlscd7l4RtFiHQ8O+fmxUYOigpsjM7njxQz6YJwlF9gBMdJgaC/m0gRSMIXTf/IWFUGe0D/ykKMiuZMcPnzmvMSwQnf3H7bMsELX+UV9B05ARtetwzhzJlEd3PuChByNcu/se7l71t2s7FnJ2r61yJ1CTe8pCOOOutGWlqIpFj+4tHe04gdI9YrJSF19Pb6/PUnn9Tcw8MOHkONxFH+AcKEO+5pWxrolCvWFjLOP48LnuvnSH4bouPN7hFyGEYWeihAM6ADQDga4YqWwL0oCChVhYcEU6qLsrJVQhWWSHR25ULxSP5R79wt93CM6wPiOHRj6I6yYqsKtUWOboCIV1HDX+n/QFnYx0DWJtl9sQg6FkOMZVAaZzybeFXUMevnS4yGmvWNAAboVLbpICpcDNDJU+MGmK8XfbkJXX0NJ9Xi6Ql3oGxtJdrkofq2AHz4hU2QuwzrlKi7fFKOzVOKdSRIfXx7hvp4oZ22BVMteymcPshVhkwWtTigoh8FWwpkweyvgVJcO2TIXOZ7G5H+BurVibUbX4E445Tr09fVIei3G4iRt+iDlWRfNWx0mZFazuSbFLE87ExQdb00TlLMs+3cyYRaFw+7cwq8le5dQaalkmnUixQGoMsXZ2aAhpVXx3JlqbNl8ZKFSM2z6E/HBlRSFoCKVBlMxSiqFEgrTXzpCb+u0PWgVhbG1IeRwlK4vfo22V0rIbOoiVKhhSZmeZ9/exFWzqhhr34JBlplct5BAzSQA+uwaVApUe9ppP2sMGQlKC1JIYxfSOGYhRnuGyAtLqdvp5S8L1ay09GDUGOmN9BJIBjGVipHKpC7lpC37hzyhH4STkT43Go1y4YUX0tjYSFNTE1/5yleOs5UnDsP+s7xfuOb+UBSFZHsHAGn3iIpPxiLsbF/DrqFdpOU06UGxSs8UTiHJyihCT/X0kHK50C48i0cuUVE2mObJlnO4pvEaPl77cUBMiGp63Hgt5IaqAGp7EQC+vg52DmxlZ/dKdg3tQlbkXGyw8wcPUvu3v6FtHE9s3z6irWLl39pGoV4bgjrUKqGarf0hWqoAjZrYkI5erSBxr5Ik5dMQMAEFFoxxhYQWSgJQ4hdkXWJMsrNG1Bldsya3CEetQMF28YwkmzUXCx5YsgTUCqsaJdwFTqym7SQ1CqdtTXLfsv/Fv6EXOaUitk7kFh+0SLj0GRJ6FaV7PDi8MhaPaN9QUlgibRPEgpSJfh0Df/kXyaCGsts+RZ21jlg6Rry+DBSF6h4JW0RB9nqJ285EGtQw/pxTmXbXfagVOD8wh/O2yfSWKcTK/cQQEUwBox0cY2GolVd3/pkdtRKFfUkC0VPF9/uF31K36KcAdHzsGzDnNlRmM5bffpOtU2XaDSPzGUWGBPfeCI/Nl5kdi+NseYWts+zceYuanio1aoNEyp/1M8ID1Nnq0CcVQt5+5pTP4ZFxX0MF6AsybLmkgcWfl1j/8RoUc/aZWJPInj2EzDIFcaiMZ8BUlFuKH28YIc6XdR3Mj8Zwlsap++anqPzfq5BTEtHte0lPrGRAq8Gh7eTTk00YdduYEAf3+Jvwlk0DoLtQZrq3nMKhVgIXzOFLt6iZrHih5jQom0rVPA81j3yfvv+7h5dnQkpOcW6tsKL26HToig3EjdDUCYW6Ik4W8oR+AE5W+tx77rmHlpYWNm/ezLvvvssrx5l69ERBjgi5kwkdmtAzPh9yMIjKYiHt8aBkRGja6w/cytDVN3HV0iv5666/ksk+L5UCtUoRrf7WnI8eySrZxNRx7KhT4bvmY6RfegP/kucpNhajVWnpifRg6vXjc5qQGCF0TZEYuv56xQ/51CvX8am3vsBVL17FvSvuZWCfWLjya88S9NOn8q6xB9++nXT9XiSq2lYmfPWq7OBDURQMniDtTgl1uZWET0OPTnitXpWE5NXQViZhvHARaY2KZdMkisJQ2C9IymTOkHZoiZoV/M8+TXT1GuImQfaZVWtBgoJqhXjLLuREgsCLL2KoihMzSAwUFOPVxFjdqGL+Tonxy/Yih8V2g5E1QpXvsYlOZ8iiMLZFNFrlk9GlFIKyGKl4J4jnceFqCCxbR3FTGPMEJ3XWOgA2FgWQAb9dEHSyo4PQhnaQoKawnVN9GzCVJPAub6XcAy9NU/P29JHEXkGDGRxjYKiVVzrfwFeZQVIUfE89h27MGLSzL6Vm0icB6NDrQa0lmopyWetD3FlRgr9ghFJqzBJeq0RCJzE7rULa/gwTihrpc0jMSKTQFhlIDWYT8IXdVFuq+cLLCg/8JcOs0ploeoSA0BWkmVDWRMiu54cLf4p5/gziWug1+Oju3MdQgaiiNFkAai2Z7GjONusM8b0De6wJPhGKgKTG6JSwTiqkbKaYZyidK9ID6M17qdj3e1q1Kjoic3hxTxi3ZTIxHaSrKjAMzkVSMjSo9AwUqZgTS0D5VCibgtYoY65SM+bUj+Xu/9KxlwLQotORqZxOaxVMcIE/evDiqROFPKEfgJORPtdkMrFggfAZdTodM2bMwOVynbD6jwcjCv3QlktuifPs2ZDJkB4UXrbK1U9JECaqqljduzrnoQNcVrQAfVJhYzaBVHTdetSFhQSqssudb7wK05w59D/wAKnWfVRYKugNurANRElUFFFbUMPuQbGUOqRXyKigLGnkEVUl/9c/wE0JFa93vs7Lq/5IXAt/H3id5d3L6TRHMQSS9MeERdJVIpHQQGlAdEJyIIA6mmCgUCJTpice0OGWZCRFIaRIGPwqupwSlffcx6rvX86+8uxqyO2tSHotap1MhbGEt86SiTfvJtnRwbZJgjgTrW1oTRmMuj4ynkG8jz+OHApTUh9Fp9LiNhawwWDgqbNUaPUmrn9LJmESBB7bLEYUWx1ChQ8WgHo45F+Bag/Es4SeKIwi6WXKu2OYZkyluCkE0UFqJdEx/V/3X/nKZ9XUnp2dS2hvJ75zJ/oKBxr/Ftj8Z2znnUUmGAKtljUT1fwmLr5joywT1OjAMY5Y3MfWSDfVjjQ1f3ycql//iurfiqRURo2RMnNZLpqpM9hJPJPgFn+QG6uEolUbZCaccTsAFekMled+H0J9jE+JG5sdjaIdO4WUewhFBtnXgwYV09uhxgOnuGSS2dWYuoI0d0y/nb9f8ncqTGOp+MJt3H+jiv5YL0qghz6r+J5MafF+DRN6w9QzierBY4MzjMWcqSoAS6mIqgn2UjhRR/2Sv9Nw1ecoSaeRrH1sHxIhqWnV6ezoCdAXhV98+ktc9cCT7FbqALhAKuDvdVdTnU7Tpm6gz1AHkhra36ZaVmHUGDGoDcwwllEi6dhkMPBTs4aWUonioEK/79Di6UTg3zY5F6985ZhyLLwnlE2B8x86arGTmT7X7/ezdOlS7rzzzmNr80nGsId+uN1nhidEzXPnEF6+nPSAG62zFE1AEMYCaQKPD6wmNdiIolEjpTPM0zcx5rFn8e55DH68SEymzZ5Fd0JM6hWZi6n8yY9pu+yT9H3r21TcWIF37yrM0QzpyhImxvtZFVyBJ+rhgdUPcI0RFlpn0zTwJhQ0cGbvLtomnYbV20nKaSeSDvHdNd9lik1ChcKgV085MFQA3gJw+IVST2YtGo8NojoJU1xFQRgc+hQpnxaVLDFYqkZrKcBcV8PAtuyE4+at6BxmJAkqC8fwVlMXCyJjML+9j79P1jF5ZxpTTEJrSmOwxwELnp//Ao1Nh3lcKU5zGW60RAx6ElYVFT98iL4v3MHqmWYWrvMRaxHPeK1dy+REHG+B+FmGSzJYPGqavCbkmJakBkzxToylhSSiBVT8+EdIv38V2ldSuvROjDUVDMQ8nFFupVynJagTXn+8pQXTjJmg3gOVMym44he4l5yDZf7ZTBsb452edzBIGhpSEYJqNZQ2sk2vI4XCLEsl5uzinf1RZ60bRegA58kGqvUpOgBtSSGVs27BtvcJ5uqdyFOuJrj060zd9QqUFjPnlM+jTeoIr/0r7W84UTmmI7EAACAASURBVG9eSqnzQoxxQfiqH91JWN+EptCEWgsWSzkdfSHO+/WbfP/cMjI2GXekH1NigE6ritOAdErktUl7heVSXDGGnQ4NfmOa70VVSBYnqFQQHgAksFVimDQJZJmZ8SSrTIMsSaQw6lRMdkymuS8ICkysmYXdXgL2WmJxM0bXBsai4JXsnPPbXUgStFRMQL/+96jWP0rTlHlkFBn1z6bQVOrgTbMJom3cq0ugVnS421xQW/xef6rHhH9fQv8QsX/6XKPReMgyw8m53gvS6TTXXHMNixcvpqGh4UQ09bgxotAPvcIz2dEBWi3GbAa+VH8/xilT0AeFXTA16iBmiBF19xCqtGPtHMTWMUjGrxDZsY9MKESqq4vCyy/HmxDKyW6wozGX4LjpRgZ+/BMmXH4+WzxZ1VKk5fNdu1juLOLqF6/CExvkpuJSrMEEiqkPZeZtqBKP8RN1Be2KhLmhkiqLC1fYRWndeKCFwn41CZ1EoU6DtyBDRTANcibnuXtsEgFTEhNQ51aodSbxucVPISKCGigd6sBjG3lG2poSMBZRWTSe13tX8tJCNXvqDHxB3Y/VYiId06G1yBhLMlR/YT7ymIvRr/oiUsO5lBpjuCIegkYDM7TFFJ6zkBd+dA2/G3yW83ZlSAxmUFDoNcnc4g2zq8AOKETGJbFGixg/qEWKBhiwQZEsU/H5c1FOvxNtZRVozbDj70iSmjq9nV2pAJd6epCKpqCrlYht2kza7cYwZTpcfDvYKlHrzNQ98zSa4mIu9a/hnZ53qC1soFDSEVAy0LCA9bOuRdWzjBkX/eaQ70WttZaX215GURTag6JDqjYWo+lcARShrZ+AymDjT2c/gqNoLGs6Q3wrfj+zVf38ZVodTdNuwOv6G0oiQSKhBt8g/qdEGlt1hUygzQS0U371VDAOEUxmuP1vm0ikZXaHdDgzMu7YEEWylx2OaiBFOKKlkJGIqLS1kAnfWIz2ja9i6+lHblhAIiOh8feiDblg4sUAhJIZxsXUvGpJ8bYO7iuczpC5iGUte9GqVSxoFFEp9SUFvNs3h4/t+ieKxcmOTA2nNThY3TbES40P8cnyIVj7W76/dwtIElTN4v4Z13GhBvxt2zlf+RO9FBPs6AKmH9sP9D3i35fQj0FJn0ycjPS5t9xyC+PGjeOuu+46oW09HuQI/XAKvaMTXXU12uxuS8MTo8Zw1p/2qaBchCv2TS3G2gmxd8T9O3tiDGxZC4BhYiO+uPDFiwxiUsh68cUMPPwzpmwYoj0pvGhDcB3j0lG+NiTxTZWKhTULcZT7yQz04g2YGXrtb4y9qQpDoAeV24fO0M2l6in8P1xcXj8LaKF6EILFGhrTCt4CibFdgL+LVI+IihmwgScdpxyoc8NkW5LEbiNxLajMIhrB2bEanwUyKgW1LKG1yGCtpMJaTVqSeD3exdTGGhZta6O3pISAJ4W2rASpcgwWVRvMqoP1g1A3j8pIM/90bwStliuLRLxe1fQzSC9/lpBdQjcIQZOErFZxWlpNs90ARNE6ExjGNdDQ1YkqFKWjRMKeyaCdeT5UZeP/zA6RcKrxAsaUVeBqe5kFoQDUVaCr0xN6483c86dkfO571Y8dC8CCggVYdVYaCseAAt1DO0GlZn3KyyRHE5bSSYd8L+pt9YRSIbxxL22+Dkjb8WcKKNPGkTSgmyhS6DY0LATgude2sFepYm+kiluq5oMkoasRUS2F0wvwbw3hf/ZZdA4DZZOH6Oo1Y5tTR+EUC/QU8rsVbXR7oxi1aoYiaZwqHduSATSSTE+BlqQuyZBXogoREaWo1Mz4+RpW3HE+ZSvuAQWWu8ATSXOleidICtSJhVIt/SEqYgVAknMiUa475SLeUFtRFEimZcptIkyxvtjCn9vO4GPqt5ASQXbIl7BwYild3ihvuC188uNnQ/UcLL84DZDhisdxFlazCFjV8ShGs7D+Yl0nz27Ne+iHwYlOn3v//fcTCAT4+c9/fjKb/Z4xTOiZw0S5pPr70VZUoC4qAq2WtLsfRZYxRcXLqe5x02huQBNJ0FGQIKlXE8uOXExJ6H75OQD0jY14414sWgs6dTY8sLQUy7x5lK/cTb1bIa0Cq8YHOgufCEd4YuLn+cGZP0BT5CA9NEjUoyPjDxF0Gci4O5GDQbTqIW7y+fnDeX9gUrofWRIdg6pAxYR4FG8B6CISimcvqZ4eVBYLEQN4VVFidg1j+xSatqiY1arw3DwVjkwCejfjdG1GUUlECoTtotVHwVpOhaUCgCBpZmtFx6QfI4hS2zAR6uZBz0bY9U/xAOvmsfiUxXzvjO/x0KSbufLsBwFozBL7u+XC+9YYMvxu+t04SyYSa1TzjevV2ArS6JumUtwTpjAk88JcFUVnfQUa9suNbsoO3adfx//Ovpc/2k/HoChQUD5qJx19YyOHgl6t54lFT3DPrHuw6q0EkgFi6RjbB7czu2z2Yd+b4RDDjmAHu71tpOPF9KQLkFRQ++VLcXzuc7myoXiKl3f0MadePK81bUJBm08/jepHH6XsMx/DUi46UpMjjPmci6n7BJRdXAtxPxjt7HGHGFtqYUypGW8kQanGzKCSIiRJREjiKbTS4x/20H2kLAUkMrArqAOr2PFpT8TE2IYxqLPvCHVi0rS5NwhJB3/oc/MDzxBS8VgmVYykt3VaBaE3lJh5OzWBtFXknW+Waym3GZnb4GBtuxdZVkiYy7gs9QAXxb5DR3okmqU3oUdryqAAcl/vYZ/r8SJP6EfAiUqf63K5ePDBB2lubmbGjBlMnz6dRx999AS08PhxNIWe7u9H4yxFUqnQlpSQcrvJ+AO5SbtEezunG0Xy/jaNl1ShGTIZyMZ3695cg9rhQFNSgjfuxW6wj6rfdtllaAYDXLBBod8OxXIGTrsdCZiZzGDUGFEXFZEJRogHsgtVtvpJ9YswSa05g75/B7Ods5C6V5G2iuuaDWkmRUMMFUhIskSmcwepnh60lZVYdAUEMnEGKvTM3qug2mBh4xiJpXMkijMZePWrFGdkdCotqexEpVbpAVv1qO3gTsUAOgu6aXPEvU45A+rPBDkF//oB2GrAXovT7OTSsZdy4ew7MRnF/VeYKyjQFrAxy8cl2hRzq+ZB2RQao0PsrpIoNhRhmDwVgKfOUrGrRsJed5YYzg+joBzMpTB2ISWmEsbNulV8XliDrq4eAE1ZGRr76Oe+P8bZx1FqKsWqsxJKhtgxuIOUnGJW2azDnlNvE3Xv9u6mN9KFnCymM1UISBgvuQO1dYQQl+0aIJ6SuW/RBIot+hyhS2o1lnlnIDWcha1eCAqTIwR18zA2OFHFPSKZmbEQly9Gld2Ew6xnKJLEqS8ioZLYpRfiIOCsxDzkQZYV0t4hYiZxfZcvRsAmOrNxDWOY1ST+3ak48WtLURSFN3e58WtKmB1PYFIUKBpDZaERq0G8S8MKvaHYjIKK3loRFbRDqaPMpmduQxHeSJK9A2G2dgdoTZXQqZTx3MYRJd4Z1SOpIW42o/Ecfj3H8eLf13L5kHCo9LnHi6qqqg90I9/3gpGFRQcrdCWVIj04iNYpkj5pyspI9fcT9giFkSiyIHV2cU3RPfh4nvNnXkNh33ZS7m1YzjiD4MoV6EMJDGfMQpIkfHFfzm4ZRsG5H8fy5c/zsy2P0Voh8dvzfwGTLoeNT8CgWDmqLrIjxzPIgLqkmOiefiKmrMo3ZyARgO61MNSKrWwWsUAvxZKfxmgMU8NcYBXp9mZSPR60VVXYdFH8KjcvfbyAseOsXNfbzC/mFaNIEo5MBrpWozntDh475UoKtz1OrO11tOcuhrNuotwsGNgsyzR6e8BSiuVTd1ElqTBe8nlxU5f9DpIhqJx52OcuSRITiibQVyRCOrVGGYx2KJvC5Rseo15tosxag3zB+ayPNvNC6m8ABz0/zvseJKMwvCNS1Uy4/nmonoNOLXK+GA6jzg+ETW/LZQwEGG8ff9iyFeYKxtjG8GTLkyTkKHKymF+nZnH5TTeArWpU2a0uP0atmunVduY2FLGmzUs4kcasU4u84DVzsVQnscxLYK2ICeXc/A+R3jYZBXsdLl+UWXV2wvE0rQNhSqud4O9ms0GMcLS19RRv3013vx/F6yNsEBOkLl+ULt0YprCMmZMbwSxEwerMRCKbesjIMiv3DvLFKeNhL2ApA4MVCbEH8Zo2b06h15eIRVDvlF1HhWEiHSttOK0GSgvE8TVtQwRiKSQJplcX8vdNLr708fGoVRJt2cVpaZsJ49DhV2UfL/IK/SOOI1ku6cFBUBQ0TieKotCs6qenbRuhAaE84k31KKkU2t0dAMyfdhmGUuG1G0+ZTqxShNpF6sSkki/uO0ihSyoVlfNn8a9TJNrLJBxNV4gNex3jYEgQusZmyZUvWbwYJImBLWLGUpv1JVnzawB0dcIb1poyqIBp47LpVTv35BS6XWPCr1Kx1RLFt2gWjoYYOqP4URanM1A1Gz72baaXTscytgkkCe25XwBrBTq1jjJdITPjCTSu9WBxImm0FFx7t9iCTKWCaVfD7Juh4shbsU10TMRtByQFjVEGQyGUTcGgKJw+2AWFNaj0eswL5qNkVflBhF7UIHLD7I8x54DOjK5eZCI0TJp4xHZc/9hafv7mntwuOts82zBqjEdcoi5JEpeOvTSXHVFOltAa1hNyHmzTNPcGaSwvQK2SOG2Mg/5gnMnfeo27n8lu/WYsJFg4keqqIYb05VBYkw0vHICYj4TWSjCepspupMisYyiSwJkdKb1pEiMo58QpqFBo3bKLtMeDVyfemR5fjGZFjCYKS2vAKiyzLutMvvtiM99/uYXzmpzMnpp9ho6xuXZPrrChUUk5Qi+zGjDp1OwZSrNZNwtJgtICA9VFJioLjSzd2su7rYNMLLNy0xn19AXibOwUETd7g0I7qwqNFAYHT5rAyxP6RxiKoowo9ENYLsO5WzTOUp5seZIt2j6s3kQuE6AyReysHl0v0tJqSorRFAsFq29sxDFFKNS/pVcRSUXwxr0HExKgCg9QkU5j1Zpz/vrw4hYAdWbE9ipYsICqh75J6fQAlWf40JSUg6QSnrXeinas+GFqjILoNdVjAEjs3SeiVSorsCUidGm1+DJR6oonwTVPUZRdjl08/xtwzVM5xWv/1NVU/+63oyyLn8z+Kl8Z8kIqKojnfeKzkz/LLys/Ts38IYqmaURnUDpJ3A8IYgPKTGKEpFVpMWvNx1y/xm6n+ne/peiGGw5bRlEU1rV7aekL5fb03ObZRq21FpWkOiLxXNRwEapsW0sNQpW3D46OllIUhea+IJPKRWdx2SmVfOeSJuZPKGHptl68EeGd99mFvbOB7CSsxSn2CI0HCCLIucpuwmHRE0/JaHXCx27R67io4SIaZ4p3zbd6LanubvbaBHG7fDFeTZ/C981fQao5TXTWVz7BJdfdwdcvmMh3PzGZh6+ajpQleopHCP22+WN44qZT0WnEPUqSxHhnAS39QdzBOA6zPnds8cKxbOj0sbbdy9wGBzNqCgFo84RJZWT2BUFGhd6mwxELEMguKjvRyBP6RxhKLCa23lKryYTDB/14U9mIlnChnp9s+AmeEj0aGUJbRPpY3SliEUl42TJQqdAUFaEpFQRnmDgR62QRmrXB5uUXm36BL3Gw5SIqGKAqlRZ5x4fhGCuy8MV8aGIiLG64wyi46AocE+NYq2PgnCzUvCJD7enoagQJak1ZQq8ZD2oVoR6hsrQVFRQG+ujWCsVUZ6uDCedjN4rRRHHjRWAeiRFW22xiA+L9MK3+XKrl7E/H8v7zchQbizm9ZDpmZxKNI/tctEYozloddjHxOKyU7Qb7e966zHLmmagLCw973BNOkEjLRJLpnEIfig9Ra63l689v58bH15POHHpjkxJTCacUz0WRNVw0SRDxgYTu8sUIxdO5SUaTTsNnTq/jy4saSWUU/rlFhJJ2WMRo5s3YeOKpjHiuigxKBm9GqPAquxFHdtn/YLIctaJQk1HzjbnfwDJWdNwlb78KwKqC2tz124bi9FSeJzpMSYKmy2isKOLzZzVw/dxazHrNiE3kGJdre7FFz7xxo+PFJ1VYae4N0huI57x1gKtmVXPZKWLUMKehiDKrAbVKwuWL0eePk1FUpLRWDHYVakXGve/wqaiPB3lC/whj2G7RlJZCKoWSSIw6ns4m49qn9ZGSU0yYJqIrlK1iv03z+EYqfvgQJXcupvLhnyJptRReeQWVv3wErdNJ4VVXUvHTn3DK7It4fu/zpOU0dv0hJufCbu72h/n2GQ+MfFac/WEN7UMdFKtG9VnSQK3JRS5QPE4svwaom0fBeedRcf+d6AvTgIRkdaIpLiHu1aErlLBUZCiMj9hLw9Eawx1NsfEYFnyoNVAkhvHHo9DF+dkOweQY+axsivg7q9AtOgtmrRmHwcGJhssn0vVGk5mcQgexcGiry8+KPR5+/ubew53O/OJbiLmu58KplUgS7POMJvTmPrG0v6nCNurzieVWmiqsPLdJ2Hc7zadyT+pWXkifxh53aNRzHUgJ37vKbqIoS+jbAmZ+OjDIw+pxmLQm1BYzEUshpe4uJJOJXZYKii06BsMJur1RxhQfZWRTWA1XPA4zrj9isUnlwv7Z2u3PWTEg1PuDl03mh5dPYWFjKRq1inKbgW5flPYh8UwUQyEWmxBN3n0H7xlwIpAn9I8whu2WVq3w+Q6cGE253Ug6He0Iy2P8tPkAWNoHCBug0FKM7dJLKf6f/8G6aBEAmqIirB8XCbfUFgu2Cy/kE2M/QTwjhpj25qWw/PujGxIeYJzewbTS/RZbDCul1mVoAmLFsKFxPy84S3Y4xowQYN08VAYDtiuuEYEgJgeotWjLypC0GirnulE9dw02SdgpaklNlUUoM4fBgVFjxKQxHdvDG/Zaj0Ohi/OzxGXcb+SSI/TakY9MZRQZT3xSp2FCjyTSo3air7XWEoilUKsk/m95Kzt6Aoc83+01I8UamVRupcpuPEihN/cGUUkwwVlw0LlXzKxiR0+QfZ4wgbjC3+WzSaFhyaYeblnSnSvXk9Rj0qmxm7Q4LILQVw1omR+JU188JlcuVSFsmMTEqcgqNTNrhXiQlZEJzSNi8ifBYDtikeGRRiCWGqXQQYw+rp5dg0YtaLXabsLli9GeXTSnNhdhtoj1G6GObk4G8lEuH2EMK/Q+U4I6xPL/YQ8cIN3vRuN00hnqokBXwIS6mbiMYI0pBI2MIoAjYVbZLCrMFfRGenH0boNgEBZ8baRA2H2w0nWMgfqz4V/fR62Bins+g/nSa0eOF9ZAJ4L4nU0iQqQ82yHorWIFZZZsS++7FyUexZBZC8kIdp0M7UuoKqhCm/XKP9P0Gc6qOuvYLY0TRujDCn0/sp5xg+iMikZWE98/935M2mPsbN4DXD7RqR9KofujvSxqKuOl7X2sa/cyufJgsnMH4jitBnQaFQ3FFto8o0VBc1+QhhILRp36oHOnVQsrqHMoQjCeospuxB9J8cSqDqZbRmyid10ZquxGJEnCYRZRLVt7o9yrvpuHz/hMrpx5TAPs2c72EvHcZtUW8dpOMQ9UXzwysX48aCwrQJLEDoZlBxD6gaiyG3l7r4e2wQgFBg0asx2z4mXv2RdTM3nCCWnPgcgr9ANwMtLnAixatIhp06bR1NTEbbfdRiabtfDDxDChe7Pi6UCFnna70TqddAQ7qLPWUWospS/LOyGzhFFz6LQIB0Ilqbh4jFhmXRQZym05lkN44GBilCS4/FERYy2psV1/B5qSkpHjWX8Zx1hBhjNuGInPliQRzZDtJEwzZ2I+40w46x742LcorD4NGLFbAGqsNZxdffYx3U/uunD8lou5BJBGE7rRDqdcNyrefFbZLCY5Dr1q83gwYrmkMWgMoAiNV1VQQyieZmyphWKLPmedHIihSDKnmuuLzbR5IvijyVydm7v8TCw/dMdfnCXnoXCSYCxFoVHHxHIrKgm+csVZuXJ7ghqq7KIzG75WNJlhT9H8kZEaUDZZzD0syYjvZFbdiL1XfzTL5Rhh0mlydZVZj0boJtzBBC39IRqKzUhGO+pIL5f83wNMP2/eCWnPgcgT+gE4Welzn3nmGbZu3cqOHTvweDw8++yzJ7T+94NMjtAFcRwY6ZJyZxV6sJM6ax1atZahEvESxyza9zRBd0PTDdw78UbGJ1PZ5Ej74VAKHcRn1z0Hn/gV6A9QWDNugEt+CdbyQ1/wgh/Bgq8f8tCwEt2f0N8zJn8SFj0E2XzZ7xsaHXzy9zDrc0cvexIwYrkIgaFkTJCxQEZ01jajNjcReCh4I8mcr33FzCrSssw9z25FURTuf34HQ5EEnz615pDnFmXJ2RtJEoynsRo13LdoAr+6dgZzG2tAJ5RGQDFTZRftMenU6LORJVWFo0csRZ+8jNfOuY5NJuHnN1XY0KlVFFt02Iza9/2MDsRwxM6xKHSAzV0+0QlMvVq866/ff8LaciDyhH4ATkb6XBAJv0Ak6Eomk+85WuFkYFihD+eT3j8WvSfUQ6zPRcCqpj/SnyO/aLkgw8RR1MmBsOqs3GCfJl64VAQS2WvJYuf6w1oX5dNg2qcOUWGFIPXDYcw5UH3opevDsfDDOcTfF/QFMPd/ROTE8WLqlSOTrB8QvvjkZp5e35WzXGKpDKmMjJw2kkkU448Jr7fQpGVSuZW9AyGS6YOjXYbCiZwNMrnSxtcumMibuwaY+b03WbK5h8XnjOO0MYeezDXr1Og0KoYiQqFbDVpm1RWxaHK2k8528n4sOXIUtovoCIY/G4bGbkdzxVUgSZRYREhhpd14wtT5MIZ9dOdRFbpoXyqj0FBigfHnwml3wPrfi/1YTwL+bT30H677YW6TgxOFxqJGvnzql49a7mSlzz3vvPNYt24d559/PldcccWxN/wkYXhSdCin0EcI/Z+b/sr8tMKbMZGXpc5WB0C6ygm4SVvfh5/r329mP+wWqjsyKMLTjte6eA8Ybx/PnTPuZFH9og/smv9OiKcyLN3ay+YuH55QAq1aIpVRGAwnSHgWgayn1y+U+7BCT2UUWgfCo3KcKIoyynIBuPH0OjKywj5PhCq7kdvOHnPQ9YchSRLFZp2wXOKC0EfB4kQJdPOlRVO5dPpIygWHRU9vIH4QoQPMbRCdx7B6vv/CiSIs8QTiypnVqCSJMUeZaK0qGvmN5DqVj31bxNfvt4DpROKY7lSSpEXALwA18KiiKA8dcLwG+CNQmC3zFUVRXj7Bbf3AcLLS57722mvE43GuvfZa3nrrLT6ejQb5oBFvbmbg4Z9hmiUWc4x46MJycT/8MFOf/hMAWyUXoMqpWXVtNbCNjO19TDL594u9DQ+IiU/vPvH/451cfA9QSSpunnLzB3a9DwNLt/by5i43P796+kGjweFIlGG7pbGsgJb+EO5ggkx44qgyhSYttQ5BRs19QSZVWPntin10eaN89YKJJNJyznIBQdI3n3nsqaGLLDq8kQTBmLBcRsFSimQo5Nb5o8mvKKfQDxYVU6sKMWhVOfW8cOKJf69KCvRH7KiG4SzQo1FJ/5+99w6T5CrP9u/qnHvyzM7O7M5sDso5SyAyWJKFBciAjbDJYILMh42NAz9s2T8MTmAMGDDmE2AQQQIkRJBACGkVVlqlzZqdnZw6d1d3V3XX+f44VdXdk3Z2Z1baUPd16Zrpruq4o6effs573peKIVhnib/bCzd+acWfk8URBV1RFDfweeDlwAjwmKIodwkhdted9pfAd4QQX1AUZRtwN9C3nCe2FCd9PDke7XMBAoEA119/PXfeeeeLJuiFHY9QePBBuVQPpExtrubyVDMZEl/7GumWKslz1vHsWumqe6OyJCywYSP/e6UL18VH/oOeQ3oIUAAhHbpWgB9/WHYMXHv5CrwyB4s7d43yiz1T3HJ5P+f0Nm4sssTaqtaQux9zTGVruxcPm7XT8aCX/rYwAa+L3WNZ7o9Mcds9ewn53LzrKvk30Fon6EdLS9jPRLZMUa/OdeiXvBc2v3rObezIpWWu2fJ5XPzdDWfS27LyFUFHi8ftYlVTgOFkccVjn4VYSgB4EXBQCDEghNCAbwPXzzpHANZ3sThw/PpDvkCsZPvcfD7P+Pg4IDP0n/zkJ2xZYsOk44E1AKD45JPoXoWKR6HkhUouS/aee1D0Cv/3uijXfu67hFs66Qp32SVznZEuvneFC2/nIs5HCPj+u+A/LoWvXwe6dIKkh+TWdpAO/Rd/C9P74PVfln29HY6JiUyJP/zqoxycqi1qW4uYd+ycW+9slRb+zllyu/vmLvkVbTJX21h2aEbGcfGgD7dLYUtXjO88Psz7bn8ClyKrTA5Oy8erj1yOlrawz/7wiM1euFxz8bzrJ9bjrW6a/9vz68/v4aL+4zeI+WjoaQqxKh4g5Hth0u2lPMpqoP6vYgS4eNY5fwP8TFGUDwBh4GXMg6Io7wTeCbBmzfwr3ycSt956K5/73OeWfT+FQoHrrruOcrmMYRi85CUv4d3vfvcKPMNjwxrRZagq5bCsD1b9UMwkGP/W/Uy1w1XXvo2QN8THL/k4Oa0mFNY29Pqa5TlMPA1Pfxvat8ChX8PQw3KRMj0EG18B03ulQ99/D2x9nTzmcExUqgYf+NYTPDaY4qUHO9jQESVV0BjLlPC5Xdy1a4y/fO02At5aHfjATIGuWICPvnIzXfEAZ5r15fUOfbDOoQO855r1/OCJUYI+N9u7Y3zqJ3vYNSw3G7WYi6LHQkvYh6rJCps5kcsC3HheD81hH9HZjv4E5J1XrbP71bwQrNTHxs3AfwshPqMoyqXANxRFOUMI0bAsLoT4EvAlgAsuuOCE7Cd7PNrndnZ28pg5+f5EwBqiC1D0ye3uqn8C9Rf3EcwXGbhhLe86S/bVvnbNtQ237QxJZ77opqJd3wS3D97yPfiXs2DwQVhzKRSmoKVP1l7P7JcCf/78kZbDrSiSLQAAIABJREFUwnxjx2EqVYNbLu/n8/c/z2ODKRSltkloj1kz/vYr+vnPXz/PdZ97kO3dcT77hrNRFIWB6QL9bWF6W0J8/DVbeXJIfsBP1gn6UEJOB7KaT71yexev3C6bhFnu/+kROSN2OZFLa6T2YTAnclmAratiC9a2n2hY4+teKJYSuYwCvXWXe8zr6vkj4DsAQoiHgQBwfKagOiybSp2gqz7B+vh67j3fxcFuhR1bXNzwJ/+G2zV3Zx/I2u23bH3LwptwKho8/R3Y8lrZ8Gj1eVLQ0+aXvKa1sqJl4NfyctdZK/nSTgu+9cgQn7//IEII7nxqlCs3trGuLWwvclqbgP74yn5uvmgNXreLHzw5yqGZAkIIBqbztUU6sKtApuoiF61q0BSaX2BXm9UlTw2bgr6MyKX+w2BO5OJw1CxF0B8DNiqK0q8oig94E3DXrHOGgGsBFEXZihT06ZV8og4rRzWZRPFLZ6R6DdY1rePe81184vUaT3zgpXR3LzzYwO1y87GLPmYvks7hwL1QTMI55jZ9ayTbxNPyctMaWdFSNnuDWH1LHJbMZLbETF7j4ecTDEwXuGpjOz1m3xCQDrorFqAt4ue2G8/k32+WnQx3DCTtTTz1i3Qhc1v+ZFYKutslq2IW2owTD3qJBTykVJ2g172sfLi+QmapDt1hYY4o6EKICvB+4F5gD7Ka5TlFUT6pKMp15mm3Au9QFOUp4FvA28SJOqLHgWoyaZcsFn0Ka6Jr8Lrk/0zXb5i93n2UjD4BLk9t7mXfFWBU4EcflOPSOs+olSiGOyD6wpUrngqUK1USZiZrdUG8ZF0rPc1BO3Kxygst+tvCdETl6DerwmV9e63sNGwKspWhdzfJkr/FdldaVSQty4hboNHdLzVDd1iYJW1zE0LcLYTYJIRYL4T4O/O6vxJC3GX+vlsIcbkQ4mwhxDlCiJ8dzyftcPSUDxxg8tOfxigWMVSV4Pnngc9LyQ9RX5SOUAdN/iau7jmKfibzkR6SUYvb/J+z9xJQ3HIYxOu/IjcTWZuIHHe+KP/920Pcv6+xTcJUthaLPDqYJOr3sK07Rk9ziJSqkyxocgNQXcasKAqXrGtlx0CCfZNygbvBofulQ08UNPweF21mrr2YoFubepYTtwD2LlNwHPpK4Gz9P03I3H03ya98ldKzzwLg7eiAt97Iw1sUIr4Ib9z8Rj543gft7oPHTHqooWES/ghc/ifw2s/aU9Zth+4I+qL86y8P8H8fbuybPZ6RLrojKoXwov4W3C7FFti7nxmnYgjO7GmsQrpkXStTuTL/eM9e1reHG+q0fW4XHjNmiQY8tJjzWhfK0KG2qWc5C6JQ+0BwuxQ7+nE4dhxBP02omNOH1J1y2pC7pYX8W1/LI1tcRLwRbjnjFn5v0wq0I5gt6CC3O19QV83iOPQjUtKrpFTdzsUtJsxY5PpzZA25tdXdEvQ7do6gKHDxrDrsS9bJy+WKwefffJ6dk4N08JaYRvwemkxBX4pDX07JItSabcUCnhOiv9HJjiPoszhe7XMtrrvuOs4444wjn7jCVCbk9CH1SVPQm5vJa7JEM+JbmV7R6CXITzQMZpiXNZfKenQrZ3eYg1VCOJJSG0YDTpoO/S2XrOXqTe285izZyMpyzLuG02ztitmibNHfFuZ3zu7m0zedzZauuSV/VqVLJOCh2XTms++jntntbI8Vq9mWU+GyMjiCPovj1T4X4Pvf/z6RyAqJ51GiT8ke5MUnZf8ZT0sLeV0KetQ7d5rMMZGR48TmOPTZxFbBm7/r7A5dhAlTuAtalbSq29ePZ0qEfW7WtIT4+tsvsndLtkV8dltZy7XXoygK/37zuVx3dve8j2c59LDPQ7MZoywmsnaGvszIBWQtupOfrwyOoM/ieLXPzefzfPazn+Uv//L49UJejMqEFHQjK2uU3a2t9g7QFXPoVjfFIwm6wxGZqNvkUx+7TGZLdMYDc+IJRanl6Fa8cjRYDj0a8NjZedMigr6uPcw1m9u5bP3yt5u87qxVvPrMrmXfj8MJ3D534u//nvKelW2f69+6ha6Pf/yI5x2P9rmf+MQnuPXWWwmFjm/ToOJTT1Hav5/mm26yrzMKhYZpRIrXiyscth16xLtMQZ/eD4MPyGoWcAR9BbAcOsjYxVrkHM8UF5yU09McYmCmcEx9TOoz9JYlZOh+j5v/vuWio36c+XjXEjoXOiyNE1bQX0xWun3url27eP755/nnf/5nBgcHV/CZNqJPTDD87vdQzedp+t3fRfHIf17dXBD1rV2Ldvgw7pYWFEUhr+XxuXz43Mv82vzkN+Chf4NNr5Y16NEFpgg5LJmJbMnuU97o0MtcvIADf8X2Tjqi/kWz74WwatEjAQ9n9zZxTm/TSbO93qHGCSvoS3HSx5OVbJ/78MMP8/jjj9PX10elUmFqaoprrrmGX/3qVyv2fIVhMHrrn1JNyb4c+tgYPrMBWmVSLoiGLrlECnqrFIS8nl+ZuKUoH5P990BzHyzQNsBh6UxkSqxpCTGVK9sbhgxDMJktLejQ33zxWt588bGN1QuZkUvY76G7KcgP3+e0Mz4ZcTL0BVjJ9rnvec97GBsbY3BwkAcffJBNmzatqJgD6CMjFHfuJGr2WNfqvgnokzI/D18svyJ7mk1B1/JEfSuwIGoJOjhxywoxkS3RFQ80bOmfKZSpGIJVR5hleSyEzcglusLTfRxeWBxBX4Rbb731uFS7HA8q5vOMvsIU9EOHasfMBdHQxbLrsbtFCnpOzxH2rkDj/VIGzNYBjqCvDBOZEp2xgLmlv2hfB0eeZXksWP1YIo6gn9Q4/3qzOB7tc+vp6+vjWXO35kpSmZaC7t+wAVc8TrnOoVemJnHF43haWwlfcYXdxyWv5VemZLGYgnXXgJqA/mW2DnCgagimcmVWxQPEg1UeOjiDEIInDstvQuvaV770NWxu/4845YMnNY6gnyJUZmRzS097O76+tWiHBu1j+sSkPWFozX992b4+r+dpDa5ALXgxLdvgvuWO5d/XacR4psi+iRzXbG7smZ3Il6kagq5YAK0qKGhy1+h3d45wxuoYGzpWXtCDdpWLs/5xMuNELqcIlZkZcLlwNzfj7+tvyNArk5N4uuZ2NcyWs4sPqlgqpTQEm5d/P6cRqlbhrV95lLd97TGGEo3fAq1+LV3xIGeYXRP//PtP89xYlpvOX6Bt8TKxq1z8jkM/mTnhBP107rq7nNdenZnB3dqC4nbj6++jMjGBYcZF+uTknBmghjBIlpK0BZe5MaSqg5aHYNORz3Ww+es7n+P56TyKAnc8MdJwzNpU1BULcPG6Vm6+aA33PjeJ160suNNzudh16AHnS/vJzAkl6IFAgEQicVqKuhCCRCJBIHBsC16V6Rk8be0A+Pr6ANAOH8YoFKgmEni7G4UgXU5TEZXlRy5FObXGcehLp1Cu8N2dI/zhpX1csaGN7+0cwTBqf/PPm0OcrZ2ff/0727iwr5mbLui1t+WvNJs6ozSHvPZjOpycnFAfxz09PYyMjDA9fXoOOwoEAvT09BzVbfIPPED40kupzMzgaZNu29ffD8hKF6NUAiHwb97ScLuZolxEXbZDt0oWA45DXyrZkuzNsqkzyrlrmvjgt3ex41DC3kb/yECSTZ0RW7wDXjffedelx/U5nd3bxJN/9Yrj+hgOx58TStC9Xi/9phg5HJnS/v0Mv/NdrLrtNiozM/g3bgTkjlDcbkp79+HNyFFvga3HSdBLjkM/WgrlCiDjjVds68TrVnhg/wyXrW9Drxo8Ppjk9ec3frA7rWUdlsIJFbk4HB360BAApWeeoZJI2A7dFQgQPOMM1Mceo7RnL+54HE9XY/OjRDEBrKBDdzL0BsYzRXvc22xyJSnoUb+HgNdNc8hHyhwr9+xohoJWnbdjooPDkXAE/SRGHx0FoPDoI6DreNpr4hy66CKKzzxD8ckn8G/dOsfhrVzk4jj0+fjUj/fw/m8+Me+xvOnQrQ6HzSEfKVUK+o6BJMAxNdhycHAE/SRGMwVdO/g8gO3QQQo6lQrlAwcJbNky57YzxRmCniAhzzK7PzoZ+rxM58qMpYvzHrMjF0vQw946QU+wqTNiz/V0cDgaHEE/idFHxxouu+sF/bxzwS1L0Wbn5yAFvTXQuvxs1srQA/HFzzvNyJZ0UqqOVjHmHLMjl0C9Q5cLpbuG05y/1nHnDseGI+gnMfroKO66mMUqWwRwhcMEzVF3/i1b59w2UUzIuMWowuTuIz/Y5HNgmOI0tVfWn4OMXPwxcJ9Q6+svOtmifH8ShfKcY/lZDr0p5COtapT0Kpmizuqmle/V4nB64Aj6SYw+Okrk6qvBJf8Z6zN0gPBVV+KOx/Gvm1s5NFOckYJ+4OfwhUthYpH+MlN74QuXwe4fQmZU/v70d+SxYsqJW+Yha7rw6dw8gl6anaF7Sau6fW571IlbHI4NR9BPUqrZLEYuh3/denzr+lH8flyz5pW2vfOdrLvnbhTv3O3cM6UZuakoL3ulM/CrhR9szFzcG90J47tAVGFmn7yulHYqXGZRqRq2C5/OlcmXK0zVjZTLaxV8Hhc+cwZoc8hHxRB2VYwj6A7HiiPoJylWhYt39WpC556Lb+3auXMmvV48LXPzWK2qkSlnpEMvm90lBx9c+MEmnjF/Pg3jT8vf07JkkmLKEfRZWGIOUtD/7id7eP1/PmTvgM6XKg19x60NRPsn5YzX9ogTuTgcG07weYIihGBKnaIzPLepFjQKeuyjH8Kfzy75vpMlWRrXFmyDlKyQ4fBDMk+vnzaUGYF4T52gPwPWhCNb0NPQMXfR9XQkW9JRgGyxUdB3j2UYThYZTKj0t4XJlysNPVOazaHMtqA7Dt3hGHEc+gnKQ2MP8crvvZKJwsS8x2uC3s2/7fkS7975sSXfd0MNuiZFhHKmJtwAU3vgn7fDvnukM/cEpRs/9IA8Xu/QnQwdgPfd/gQf+vYue2s/wFSuzIAZpewYkJu5CuWK3d0QsGeA7p+UzbpaI8enX4vDqY8j6Ccog9lBqqLaIOiGqtodFLXRUVyhEO6mJg5nDzOSG1norubQIOjlPLhNR1gfuyTNiUf3/52cSLTtOnlZy8tNRIVp0FSnda5JUauyYyDBoZmCXeECsHcia5cpWoKeK83v0A9O5WkJ+fC6nf8tHY4N5y/nBCVVkht2cpaDBkY//BGG3/0eAMoHDuDt7UVRFJKlJDk9R9WoLum+D2cPA9AebJcCHeuG1g2Ngq6ao/cs1372zYCZ0W96de1YVYOQUze983AKvSqYzpVth94U8vLUsOyl0xbxsWNAdhLNl2dl6KZDz5crTtzisCwcQT9BSZVS+DVhC7oQAvWJJ1AffZTCjkdQH3mU6LUvBWqZeL34q7q6YBviuw/dzdaWrTKfL+dlLt53RS1HB+nALRQX9F4MLevk5S2vkT+fMUsXV5+/Qq/65KCkV9GrjRuGbPddrjCZleWHG9ojaOZ5N13Qy2S2zGBCpTArQ48FvVjr2Y6gOywHR9BPUPxPHeCr/1KlNDIMyF2hRk4K9uhH/xQMg/gNNyCEsN18VpMLo1pV4+V3vJw7DswdCbc/tZ/did1cv+F6eYWWB38E+q5szNELCSn0PRdC22bwhaD7XIj1wGo5k5RnvivjGuvyacLv/sdD/MUPnmm4zhJ0gAGzn/l6c/anz+PixnNXA/DYYJJ8uWLXoAO4XQpNQRm7tDtb/h2WgSPoJyjBwUm8VXDtPgBAee8eADyrVlGdniF4wfn41qxBrahohuwDkinLr/epUoqsluXXw7+ec793HbwLj8vDa/pNl62ZDn3t5fKyFbuoMxBqhZu+Dm+6XV73qtvgrT+ASCe4fTJb770IvKdPmd1Yusie8Sx37hqzoxVVq/DUSJotXXLg9vPTBVwKrG2TfXL6WkP0t4XxuBSGEiq5WWWLUItdHIfusByWJOiKorxKUZR9iqIcVBTlzxY45w2KouxWFOU5RVG+ubJP8/TDl5Bu2/O8XOws7dkrZ4a++63y8ivkwINkMWnfxnLoGU0K+87JnVSMWgmdEIK7D93NVauvojlgLmSWTYceW9WYoxemIdwG8dXQul5eF+mA9k1yZ2rcnG3Zd8XKv/gTGMuJlysGP3l6HJD9V/Sq4HfM8XDPT+eJBrx0RuUHXX9bGI/bxaqmAIcSBcoVw972b9FkLow6gu6wHI4o6IqiuIHPA68GtgE3K4qybdY5G4E/By4XQmwHPnQcnutpRSAlq1lChyYBKO3Zg6+/n5HL1nHbTS4OXy7z7GR5rqBny/JnXs+zL7kPQ8gcdyAzwHRxmmt6r6k9kOXQoTFHL8xAaJHWuk1rarc5jdgxkCAe9LKhI8IdO+WH7fNTMmK5cqN8v8YzJWJBjy3O68zopacpxN5x+W8ze3an49AdVoKlOPSLgINCiAEhhAZ8G7h+1jnvAD4vhEgBCCGmVvZpnvoM3Hgjia9+DYCKUSGaljFKfEjm46W9ewhs2UKilOTJDS5yVVnbbOXnUItcLIcO8KOBH/Gq772K2/fczmMTjwFwQVdd5l2uF3QzR598FtQEhGvNvubQ0g+ewGmXn+8YSHJxfws3nd/DzsMphpMqAzMFQj43W1fF7MXNWMDLqrh06BssQW8O2tv7w3McuinoTobusAyWIuirgeG6yyPmdfVsAjYpivJbRVF2KIryqvnuSFGUdyqK8riiKI+frnND56OazVLevYfyvr2AHODckgcDCGXLlA8epDI2TmDrFlLlxnJGq8IF5jr0sDfM7XtuZ7wwzjd2f4NHJx6lK9xFT8QcbyZEbVEUoFN2Z2R6nxm5LDI156qPwh/ceVrl56PpIkNJlUvWtXL5BunGnx7JMDBdoL8tjNftotXcxh8LeNnYGeW//uACO4rpaQ5hzYKenaG3hJ3IxWH5rNSiqAfYCFwD3Ax8WVGUOdsHhRBfEkJcIIS4oL19Efd3mqENDgJQzUghThQTtOTgoNQBEl/+MiDb4FqZ+WxB9yge26Fbwn5Vz1UAvGLtKxjNj3Lf0H1c2HlhreeLrgKi5tBb+mWJ4tiTZn35IpFLrBvWXLLs136i8JUHD/F7X3ho0XMeH5Tv9cXrWtjQEcHjUtg9nuHQTMGOVdrN3DwWlIL9sm2ddhOunuagfV+zIxdroEVH9PT5gHRYeZYi6KNAb93lHvO6ekaAu4QQuhDiELAfKfAOS6Am6GZkMjmEtwr7Nofl5TvvIrB9O+GLLrQduiXaqVKKoCdIa7C1tihazuBW3Hzk/I/wL9f8C5+64lOEvWGqosqFXRfWHthqzGU5dI9fZuNDD8vLi0Uupxj3753iyeH0grX7ACMpOYFoXVuEgNfNho4Iu4bTjKRkjxaoOexYYG6HywZBn+XQ33hhL1/5wwuIh+bezsFhqSxF0B8DNiqK0q8oig94E3DXrHN+iHTnKIrShoxgBlbweZ7S2IKelYKcG5E7OfW+VUw3u3BFIqz+l39G8fnmbCJKlpK0BFqI+WMNDj3mi9EV7uLatdcS9AR5VZ9MwRryc80UdF+0dl3rhlpHxfAy542eJAgh2D2epWqIhk6Js5nOlYn6PQR9soHZtlUxdgwkMQSsbzcF3XTaseA8gt5SG/c3t8rFx7Vb52/E5uCwVI4o6EKICvB+4F5gD/AdIcRziqJ8UlEUs8EH9wIJRVF2A/cDHxVCJOa/R4fZlA/JvimWQ1fHZfVEpHsNX3qNh94vfxlfr/ySNFvQU6UUzf5mYr5YQ4Ye88caHuP9576f2668jd5o3Zetsrmz1BeuXde6UfY7B1mHfhowmS2TLJi1/HV9WGYznS83ZNzbumNUzVB8KQ69M+rH45Jx1+zIxcFhJVhShi6EuFsIsUkIsV4I8XfmdX8lhLjL/F0IIT4ihNgmhDhTCPHt4/mkT1aErjN48++T/01j73FtUDpyI5NBCEFlQjbkiveu56k1BsNrg1z/w+uZKEzM6fGSLCVpDjQT98UbqlxivkZBbwu28bp1r2t8QpqsuLAjF6jVnMNpE7nsHq9VBaXVRQQ9V6atXtBX1d7jOYIenCvYVi06zHXoDg4rgbNT9AWkkkxSfPJJirt22dcJw5CRi9uN0HVEsYiYnsFQINSxCoD7hu9jIDPArqld8wq6Fbks5tDnZb7Ipa1u6eM0iVx2j9V6yWcXcegzuUaHvtUU9Paon2igsUplPocOshYdaGif6+CwUjiC/gJSTaflz5wUkKG3v53pf/03RLGIf/MmeSybxTWTJhf1EDUnAT07I+d97knuoVSVo8xyWs7u49ISaCHui9vlivM59HmxIpcGh75B/vSGwRuce5sTgD/++uPc+9z8feKPhd3jWdxmFLJo5JIrN9SJN4d9dMcDtjuHWoYenydDB7kwGvF7cLmUeY87OCwHxya8gFRTUtCNbA6jVKLw0MMUHpIVJcGzz6a8ew/VTAZfIkehyW+L8jPTshHUrinp7Jv8TeT0nN3HpTnQjG7olKolytUyWS1L3Bc/8hOyHXqdoEe75TCLE9SdV6oGv9gzSU9zkFdu71qR+9w9luXM1XF2DadJLyDoJb1Kbp72tp/63TOI+GvifUFfM3/26i1csXH+9+8dV61b8JiDw3JxHPoLSM2h5+wFUIvg2WfLY5kMoXSJckuEqBmFWKWKz8xIYV8TW0OxUmRSlW0BrAwdZMnikiMXq2yxflHU5ZIu/QQR9B89Ncaffvcp+3JRlwu2KVU74m1H00Xe8J8Pk8iXFzwnX64wmFC5bL1cAM4Ude7fO8X7bpeDsR8ZSHDL1x5lPCO/Gc0W9Jdu6eSi/lo/eK/bxbuvXk/A62Y+NnVGuf6c2fvyHBxWBkfQX0AsQTeyWVvQ2973Pto//GH8G2V2Xc1kiKc0Kh1NtqBb6IZ0j32xPgAOpg4C0BnqtAV8LD+GQBy7Qwe49q/g6nl7sL3g/OTpce7cNWrXhxc1S9AXjkYsfntwhkcHk+ybzC14jtVb5fy1zXjdCmlV55d7J/nJM+MUtSq/fT7B/fumefh5WbTl7OR0OJFxIpcXkGpaOu1qLodhCnro/PMIX3YZ2ojcq1UeHCSgQ7WzrUHQt7ZsZU9SttBdE5WNsfYmZauA1ZHViKp0rMPDvwVY+qKoJwjuWX8Gm15xjK9w5Tk0U0CvCop6lZDPU3PohSM79IFpWcWjlmuTnL6x4zCJfJkPvUyuWew2BX17d5x40EemqDOdk248pWqkzW8Cv94v2xM5vVYcTmQch/4CUsvQs/YmIldcOml3XApw4TkZq3hWr2oQ9Kt7r7Z/XxtbC8C+1D4UFLrCXcSK8gNieERm8kty6OV844LoCUbVEBxKSFG2yglVbemRy6EZ+Q2koNU2C92+4zB37hqzLz83mqUl7KMz5ice9JApakxka4JufRP47UHp0Dsch+5wAuMI+gvEAyMP8PTzsleIzNCloLtNQXdFIuB2U9q9GwDf6h78bj9+c4Dz1T1S0APuAB2hDkA69PZQOz63j3hZttsdLMge3bEdX4ThR2UDru/9MXzxKvj2m6FatxOyvnXuCchYuohWka1/reoTS9AXqxe3sDobWrcpV6ocnMo3lCbuHs+ybVUMRVFoCkmHPpGRmXuqoNvfBPLlCooCLWbzLQeHExFH0F8gfjn0S6YnZDcEI5+nmpLxiyXoiqLgjkYxhqV7DPVKFx71RWnyN7G1ZSsel4fmQLNd/TKlTrE6IhfYuspF4tUqv9HlcOf4gV/Azq9DckCOiiumYO+PYaK2wNjQOvcEZMAUZKgJesmMXPLlii3281E1BIMJ+SFXMLfzH5zKUzEE2ZKOEAK9arBvMse2bvl+xoNeZnIaM+YiqnTotW8CrWEfHrfzv4zDiYvz17lCVLNZRj/yEfTJyXmP57U8YdXMcoVAHxsDlwtXuFZh4o7HUYQgH4B4q2y1GPVFWRtbi9vlZk10Dc2B5oYopjsiz/PmJ3lNXiWPFLmYYcDgb+R/ADf8p/w5WLdLVTuxI5dD5mxOmBu5AKSLC8cu9e7euo21gUivCkq6wcB0Aa1i2Ds+m4JeBmbqH1Mjrep2jXqbk587nOA4gr5CqI89Rvbue0h981vzHs/reaLF2mVtZBh3NIriqv0TWHn6dBya/XJE3Fu2voW3bHsLALeccQu/v+X3GwU9bPbYzY5xQ74mRjHDgPRh2PUtiHTB2sugbdNcQT+BHfqhOoeeLdbmd1qkCgvHLs/XfRhYGbq1AAqQLen2ln/LoceCXvRqrdtiStVJqRpn98h/F6fCxeFExxH0FcLqmJj54Q8RVTMWeOABkl//uvxdMwW9TdYs6yOjuJriMLkb7v0LEMKOX6bjCvGA/P0Nm99gd0q8YcMNXL/heoKeIB5FVqZYkQu5cbZqOhs1DT8KAUuXhnfIMXGKYo6Ye7iWo5/gi6IDMwXWmV0MLTderHPoiy2MWh8GHpdiV7nM3uK/eyyLz+NinbnTc/buzolsCVWrcun6VhTFEXSHEx9H0FcIq2NiZXKSwsM7AEh/9w5mviSHU+TLWUIlqHbLhlf6yAjuWFzm2g9/DtQE7ph0ijNxF1FvdJ5HkSiKYrt0K3IhO4biCfLBZJo354rQfS4EzUHQ1tzPvitAy8kcXQjIjUO4Y0Xfh5VkYLrAmavjuF2KnaFbZYuAXVI4H4dmCkQDHjpjAQpaxW6R29si2xlkSzojqSJrWkJ2Lt5U14s8FvBwyCx77G4K8p6r13OdOXnIweFExRH0FUIbHCRw5pm44nEyP/gBIJtxVVMphGFgZHK4gHKXFFlRLktHXjJ3jKpJ26HnW4O1qUILYAm67dCzY7D6fK4ulvjwzBS0b4a1l8tjfVfKn2tNYT/0G8hPysil7cWdQ6JqFf76zmfniHO+XGEsU2RdW4R40Dtvhj7f5qLxTJH3f/MJ7nl2gnXtEcJ+N2q5ykiqSK5U4dJ1ckdotlghkdfskXFQc+iXN3QnAAAgAElEQVQBr4u+trDt8ptDPv7Pq7ZwzeYT98PPwQEcQV8xtEOD+DdvInzppZSelc20qskkGAbVTAZXVu5WLHTWNvy4YzEoydp0iklcZi16sX1hd24R9UXtGnT0IhSTsObi2gmt6+HCP4Jz3lxriRvthOZ+GHsCZg7UznsReXwwxdcfPszPdzcuJv/NXc8BcMXGNuJBb82h1wl6cp7NRb/ZP8OPnx6nKejl985bTcjnoaBV7GlDZ/bIhmfZkk6iUKY1UhN0y6F3xQI0hXx2PXpzyClVdDg5cAR9Bahms1QTCfx9fXg62qnMyNLBSlIOo9BmpvGapXCZttrMSHdTnAF1kv9oiiMKM7ZDr3YuMFhi913yP6SgWzXo5GTtOS3raz3MWzfC+pfCDf8B9W5/1Vkw8QwkDtbOexGxRLN+wfLOXaPcsXOED7xkA+evbW4UdL1KxO/B73GRVjX+5+FBnhxK2bedNt/nH33gCt56aZ906FqVjJnBrzWnBmWLOsmC1lBXbjn0rniA5rr4pTnsjIVzODlwtv6vANaCqK+/H2EIjEKhYXt/fnqMSFGuUiZbauLgisX4gjbMT5vjvCJ1gJ6LX8WerREqvQt8tb/3L2RL223X8fK1L2e6OC2vz5o7H2PdciZoYbrWBnc2XWfC7jthfBd4AhB7cRtFTZhNr56rW7D85Z4pumIBPmhuz48HvfYCqKpVCXjdRAMehpIqX3nwEDecs5pz18goazpXJhrw2M2xQj4PibxqfyD0moKeLOikVJ3WcG2hMx6U4m45dAvHoTucLDiCvgLYgt7XZ+8ALR84aB8vTI3ZJYuJQAVXJIKRz6OH/dwnVFDgsdReNlz2YT5/c4zLI/M49NRhyAyB4gJN5Q2b31A7ljUduiXoozuhZd38T7brLPlzz4+ko3e9uF/SLIe+ZyyLEAJFUVC1Ks1hn13/HQ967Ty7qFUI+dyE/R4e2D+DIWAkXasHnZ41hCLskw7dyuA7on78HheHk/L+6iOXmkMPEqzrltjkDG52OElwIpcVoHzoELhc+Hp78bTLtrPl/fvs46WZqZqg+8q4YjIjf047jKZA0DB4LH9YDqwop2jyN819EKt+XBgwtbvxWFY29iLWDVteB2e9CXwh5qXrTPlTTUDbAi7+BcRy6LlyLecu6lK0LZpCjZFLyOemOeS1K15GU7MEvW4DUMjvQdUqZIo6HpdCyOcmVvcBUR+5tIR9vHRLB1dtarNjlpDPjd8zfytcB4cTDUfQj5EHRx+0B05og4N4e3pQfD48bZag77fP1RLTRFVBxQUJRcUdlYufD+eeZqNe5WWFIo+Xp1ArKrqh0xRYQNDNvi5MPN14LDcO/hj4o3Dm78GNX1z4iUdX1YY/LxTLLIH9kzl+9NTYkU8EfvrsOM+MZOY9NpEp0Wa6ZCt2KWrVBkGPB71kSzqGIezIpblOiMczRfSq3BU6e5Bz2OemUK6SKerEg14URSEW8DBoCnp95OJ2KXz1bRdy2fo2O3Jx4haHkwlH0I+Rf3rsn/j8rs8DoI+O4euRWbQl6KV9NUGvJBI05yEbcZHVc7ijpkPXh3hZocCFpRIpobNzcifAwg590yvAH5eLmvUc/q3cBboUFKXm0pch6F96YIBbv/sUhiEWPU+vGnz4f5/iiw88P+/xyWyJqza241JqC6OWaFvEg15ZNl+q2GJvLVr2NAcxRM3pz45crJa7KVUjbt4mFvTaJY/1kUs91v07C6IOJxOOoB8j6XKaKVX2yK4mk7hbpZDvM8YRLsV26O54nGoqRXdCkO4MkSln7C3++QB06zoXlqQY/WzwZwDE/XWtb/f/DO75mMzP+66SYlwv6ONPy8tnvXHpT94W9GOvcBlJqWgVg5l8meGkyo+flm59Klfi+0+M2Oc9PZKhqFfnndVZrlRJFDT62sKsa4/YOzmtWMXCyrYzRb0ucpFC/PrzegAYTqmoWoX8rDFxYb+8n/FMyb6f+gHOC3VPbHYcusNJiCPox4AQgqyWtUfAVZNJPC2yyuI/nv0iubALI5eTuXp/P6SzrEpBaVULWS1rO/R8EDoqVVZXqmysKvx44MfALIf+k1vh0S9DqE069K4zYfI5MMx67Ke+BS6vjFqWyqZXQ8d26Nh6zO+BlXcPp4p89beH+MC3nqSkV/nmI0N85DtPkS1JAd8xIPuIzyfoU1lZYtgVC7CuLcxQ0lr4rM7K0KWoposaRdO9n7e2mbN64vzO2avs5zOTk5UwDRm6T677j6WLNAVrDh3kl5WFBNuKdJocQXc4iXAE/RgoVorohk5BL5DNzmCoKu5m2aMlr+VJhGSe625qwt3Win9oikgJxJpVZLUsrqjsn1IIQFe1ghJs5rZMCbciRcxqzEUxJZ35tZ+A//M8NPdJQddV2Ra3qsPT34HNr4ZQy5znuSB9l8N7H1pSH5dnRzM8eijZcF2latgzNkdSKgPTBYSQMzyHk1LocyXZL8YS9Pn6l1v30RUPEAl4KJg9V4rzRC4gPxRUU+xfsrmDu95/BWtbw7gUKejT+blzPy2HPpUr1zl0KfLNoVolzWysyKXFqXBxOIlwBP0YyGq1munpMVme6G6VglrQC6TNjrie1hY8La34MrIvt2ftGgxhoGzbRG7jKjSPdOg097M5l+ATl3yC7nC33P0JMCF3nNoRCcAqOUyakcdg+BFQZ+CsuhLGFUQIwYf+dxd/deezDddPZEtUzex8JFW0W86OpIqMpORrzZcq6FWDxwflpp/5HLpVstgVDxDxe8iXZc8VVZ/t0KWoplUdVavYrhvkUOZV8SAjKZXpnHT8szN0+Vpqbtty6K2LDKsIet2c09tk17c7OJwMOHXox0C9oCcmDhEFPC1S0NWKagu6u7kFd0tNEDxr18Ah0C/dyP3hrYSnskSEgBa5Hf+Gzou4YZsCXrPk0MrKrdpxgI5tEGiCwd9C/DCg1Hq1LIFHBhL0toTobgoe8dynRjIcnMrPGbs2Ulcm+Px03r48klLt3/Plip2fb+qMcGAqj2EIXHWOeLLOoYfN8kK9KqgaokG06x16STca3DvA6uagdOjzCHq47n5iszL0xaYPKYrCD993+aLvj4PDiYbj0I+BTLlWgpeZGAKwI5eCXiBtJhnulhZb6CtuCJpTiLKPf5nJg/fS4TaVv7lf/vzVbfCdP5CbiEAKeqQLInU7R10usw3ug7LyZdVZEJynKmYB3nP7E3zpgYElnXvHzmH5Gme5a0u0m0JeHjqYQJiFLocTqu268+WKXbVy9aZ2u0qlnvFMiZDPTdTvIeL3oFeF/ViBeTb2TGVLaFWjwb2DrHQZNQXdpTSWIob8c6ObWFCK/EIVLg4OJyuOoNdRSaVQd+484nn1Dr0wLXdpeszIRdVV0mHpQktRH2NeudCXavUTM4U3kzrEpNtFp1XyZ+3q3CMXRUkNyp8TzzTGLRZ9V8hzhh4+KncuhCBb1O0Fy8Uo6VXu2jWGS4FyxbBHv4F04ooCF/a12AKuKPDYYNKOYvKlij2Uor9NfsLNnjA0mS3RFQugKAphU6Qtl10v2n6Pm46onwNTMtoJemcLeojxTJGxTImWsL8hF6936E2zHHq98Ds4nAo4gl5H8n/+h6G33YLQFxe8bL7WGbA0I0sX3S0tVIwKpWrJjlzuzz3Bvw58DYBMZ9guR8zmRpnyuOkomVN1WkyHXjQXH9NDUNFgeu/Cgg4gqrXfl0C5YlAxhD1jczGeHc2QLVW4apNs9lXv0kdSRTqjAXv4BMBZPU08Xbd5qFCWgu7zuOzIZrbTn8iW6IzJZmVhvxReq7nWfC7cFvRZx7Z2RTEE3PPM+JwhFPOVP1rRizPw2eFUwxF0k11TuyhPjiN03e6WuBDZnNxqv9owW+R6vbgiEdSKXBBMR6RD3GuMMe6V1+U6w8RSsj47XUoy7XbTmU+ANzx3yER6CKb3gKHPL+gd22WOjgJrLl3ya7R6idf3FF8Ia1DydnM8W6Ogq/Q0B+lplll/e9TP1q6o7c5BbuXPlnRiAW/DomY907kyHTEpwJagz5gOfXZO3tMcsnd3zhb7V27v4totHRS06hxBt+4XatGNVeXiRC4OpxqOoCNjklt+egtDI7JHij4xsej5mfw4biHo16uQzuJpaZFNpXQp3u41PWgeGOpQmI6D6oNMW5Hmb70Fj+LmUb+XqqLQWalAIN5YchhokoI+34KohcsFm18j3flR5OeWM1+KQ5/Jy3hknRWX1InxaLpoCnrQPCds/2516pWRS4VY0NOwqGkhhGjou7IUh14xPzBmRy4ul8I/3XQ2a1pCbOxoLMWcz6H3NIfwe1xs7jxy33kHh5OJJVW5KIryKuBfATfwX0KIf1jgvNcDdwAXCiEeX7FneZzJ63kqooIwOyUOD+xi9faNRGYNUB7MDBL1RckWZ4gaBp26jidTwIi3MJIbQatKEeztO4s//MgIF/VcRiE/wh99aIg3uQz8gwVe4u/k54YU1I5qFcJxKeqKC9q3yrFxlqB7w7U4ZjbX/Tuw+Lb72RyNQ7eGR/SZ8zYtMa5UDcbTJXrODtFrCXp72HbrXbEAKVWjoNUcuiWk6TpBL2hVinrNUUfMxcuZeTJ0wL5/mBu5gNwI9POPXIV3VvdIv8eF26VQNYS99b896ufZv30lXrfjZxxOLY74F60oihv4PPBqYBtws6Io2+Y5Lwp8EHhkpZ/k8cZ21ln58xu//ldu33P7nPPe/Yt385nHP0O2lCJuGHTqJQJ5jWerQ/zNw39DQZeRwHmd56F4vLxx8xu5sOtCqm6FiCbz3+vHBxCmjbUdusstW9lufpVsf2sJeud2eWw+3B5wH92ml4JWafi5GMmCRizgsR20NSJu70SOiiHoa5MiHgt4OLe32XboPc1BIn4vOXNRNBb02pl1tk7QZ5cY2pFLfqHIpVZmWV/SWI/f424oiwRZfmh9ONQPgXbE3OFUZCkO/SLgoBBiAEBRlG8D1wOzerjy/wH/CHx0RZ/hC4CVffvMnYbRjEaqXJuCIzSNzIE9jOZHifliNGtZYlWDzkqVuAr7mytMq9MUKlLQ1zet54E3PUDUF0WtqHz/wPeJqnLB8PL0FK2xXhJuRW4qCph9W971ALh98MCnZTvcUmbFNwyp5k5M6+dizOTLtEb8tqu1HPr3nhjB53bx8q2dBLxuHv7zawl63UyZAt3THGImr5EvV8iWKvS2hAh43QS8roa5oXME3Tc7cmn807QGU8DcyOVIhH0e9KrhtMF1OOVZik1ZDQzXXR4xr7NRFOU8oFcI8ZPF7khRlHcqivK4oiiPT09PH/WTPV6ougpC4C+YHfiyNdcOkL3nHsZu+n1as4LB7CCZikrMMFin68RUqMYjpMtp26GHvWF7iPPFqy7G7/LRrZXAH8MD3KjEaPaEaTGMmqD7QtJ1N60BBGi5+RdEl8HROvSWsI+o34OiSEHXKgZ37hrj5ds7baEP+z24XAodUT+tYR9bV0UJ+912lYvlzpuCvoYMfUGHbvZjmR25dDfVRvfNF7ksRsjvpinoLIA6nPos+3unoigu4LPArUc6VwjxJSHEBUKIC9rb25f70CuGWlEJlsEtW7DQnBe2awfQp6ZQDIN1E4JipciAUSJmGJxZ0AhqsLpnK9ly1v4QCHtr5XwdoQ5+ftn/z7VqEbb/LgDvbT6Xu176BfnmB+o6K4Ip6CbzLYgu53WaQl7SjYaKlPlI5DVawz5cLsWe6Xnf3imSBY3fMzsc1uNyKdx36zXccnk/Eb+HXEm3M3SQcUf9wup0zuy7Yi+Kmhn6ApGL3+Om06yImS32RyLs8zTELQ4OpypLEfRRoLfuco95nUUUOAP4laIog8AlwF2KolywUk/yeKNWVHuikHAptOQaHbqRzQHQNylFsKhA3B3AKEth8bS2UhEVu51uvaADNKsp+UaffTN4gni6z6Gp40xZ0VIv4FC7rLiW1Q1xPgp1UYt6BJeeKGh2WZ8lxj97boLWsI8rN7bNe5t4yIvX7SLi9zKT19Crwt6VGa+bOgQyWnG7FLvbod/jxutWSKrzO3SoLYwebeSyKh5oiGwcHE5VlpKhPwZsVBSlHynkbwJ+3zoohMgA9v/hiqL8CvjTk6nKpagXbUEvrGqiZSKFasYnANWcrH5ZN1UTklighUpZCrivtQ2qMJqXn3MhzyzxSJtb+VedBX/yJITbZbzy/sfMevI6YqtBccvhEwuNkTtG6kVc1apEA/O7VsMQpFTN3njTZDr0lKqxrTuG5wgLitGAhzFzzme9Qx9O1j4kp3Nl2iK+hkXMsN9DWtXxupV5Fy17moPsPJw66sjlM284+6jOd3A4WTmiQxdCVID3A/cCe4DvCCGeUxTlk4qiXHe8n+ALgXTo0n2Pd/nwVYFMzj5uOfQNM26C5hi4eLiTiiqFJdjRDcBYfgwFhaBnVuOr9JDsZ+4LQ2yVFHOQPVo8s7JdtwfaNkLPhSv8Khsd+mK16JmiTtUQ9tb4WNBLuqgzMF1gXVt4wdtZhP1uyhXDvi3UPhQsZk8WgtrC6Oy4xWJTZ5RYwIPfc3RJYTTgXfDDy8HhVGJJdehCiLuBu2dd91cLnHvN8p/WC4uq1yKXPS0lNgK+RN4+bjn0eLLMZqODXYwTi/WgTvtQvB7CW7fBuHToIW8IRZnVYzs9NDdaWYw/uLPWcXEFme3QFyJh1qBbkUtTyMeuoTT5coX+JQh6xF8TT2tXphXbWD1hpvONw5yhlqMvlJH/0RX93HDu6rnvr4ODA+DsFAWkQ4+Ygv5ck3TjgWQtcimnU2imxpw9Y+bKTf2oUz6CG1bRFJNb98cL44Q98wje0Qp6tAsCsaN/IUegoC3NoVubiqzIJR70kDPPX9d+5KEY0cDc1rdNIS9Fvcq1n/k1H/jWk/M7dLPSZaE684DXzeoltP11cDhdOW0EXa/qFCvFeY+pukpMFRgKDLVL9xdK187VMikOmIWaGyeksMV83ZRSXkIbO+2RceVqmVCds9arBiVNh/Tw0Qn6CqNqFaqGQC0v0aGblSZW5FJf8rc0hz63B7kl7KPpIr/YMyn7uEQD895uocjFwcFhcU4bQf/kjk/y9p++HSHmluupFZVWzU8+AKkoGApE0mX7XC2TYqxFQWlvZfWgdO4tI1VAIbS+hagvikuRb2V9hcs/3rOXd33xp1Atv2iCLoTgmk//iq8/NEhBq+J1yw+sxWrRZ0culhj7PK4lOeT6hljWomi7Kd5/fEU/QoAhWDBDP9qyRAcHB8lpI+jDuWGeTTzL7uRuhGGgj47a/2m5LM2ah3xQwXAp5IMKMVVQrpb57ehvcRWK9KzaRNPLX0Fkd5Lbx4qEnhtCcQmC3QHcLjcxn4xIbEGvlDkwlSc/LTssEu16UV53QasylStzYCqHqlVoM3PrxXaLJszGXFZJobWJqL81PGdr/XzUO3Qrfrl2awd3vPtS/uK1W7moXzYjWzhycQTdweFYOG1G0OU0mY3fefBO2u77Ecmvf90+9vomP8kWD/mgBxCoYS8xVUetqNz2m0/y6QpctunlxK+4mtQ3v8XqAxFSgz8j2AkuIUvxmvxNpMtpWbKYm4B/OYvu8N8yXkmCD4h0vgivurZlfzpXplCWzbDGM6VFHXqyUCYW8OAzq0ksh17f/3wxLBH3e1x2fOJ1u7igTwr5Tef38OihJKvisyMXea4TuTg4HBunnaDffehu3jy4De/q1bS9971ohw/Dl75EOFPmqXUhQKMYCRFV0+S1PJnkGAD+eAuBM7bjb/My9VAZXBN0X+cHTUYw1vCKkDcEiYNQLdOl7qeiWCWKiwt61RALTqBfjNlzOmeTUWuCXtSr9Jqbc1StSqVq4FKUObeXm4pq7tma9LOU/BxqDj22wO7M15/XQ1c8wDm9jTX4IcehOzgsi9MmcslpOfoivWTKGTKjB/GtXUvT62+k/QPvJx/x4BKQD0jHWApHiRdgUp0kWJI5uuv+j6OUMsQ3SfFr/+AHCfXHoSzLG62F0bA3DOZEo4g2RTvmFJ/IrCEWdVSqBi/77K/5/P0Hj+o1jWeKbP/re3no4MIDOayxb5ZDj4e8eFwKhXKFN3zxYd57+xNz1hVG00Va66b5tJnRyMbOI1e4QC06iQXm9wsul8KVG9vnlB9GHEF3cFgWp4WgV40qeT3PZfEN8nImjbtJCrDi9bLzHOlac6agF0NNxFS5USgsW47gVgqQeJ6WjVl633EBre/4Y/BFQJtP0OUO0naRoF1JU/FGwbvwYuIDB6Y5NFPgqeH0Ub2uJw6nKepVfrZ7csFzrJa10/kyBa1CxO8h5HOTL1d4djTLT5+b4CsPHrLPH0mpPDmUtkfPAaxvj/C1t13I687qXtLzsiKXhRz6QlhzRZ3IxcHh2Dj1BT09RP6f1gPQ7QoQMQxcec0WdO7/ex7cIDcOuXwVhHCTDzQTLcF4Zphw2XToPgOSAyjlJLfn/SRUXQp6WUY5lqCHvCHboXcqKdqVDEX//L1PLO7YKRdOR1Lzl1Xum8hxzid/xkhKbbh+97h0/zsGEkzlSlzwqZ9z/96pxpdvRi56VZBWdUI+D2G/h8GEilY1iAU8/MM9e5kyhz1//wnZvuDG8xoaavKSLR1L7iFuRy5HuTvTWRR1cFgep6yg2zFC4iA5TQp2rFphdbmCt1iVgl7V4fGvcrDLyyOvq9LRHaI4dAtpr2x9m5wcImQ5dK+A8V0AjGhhDkzmwV/n0M2eLGFPzaGvQjr0nKduxNwsUgWNX+yewqUwR7AtHh1MklZ1np8uNFy/e0y+rr0TOb764CAzeY3fHGiMX2YPZg773IR8bvZPyA+iG8/roWIIhlNFhBDcsXOEy9a3NkwIOlpCPjeKcvQOPXKEjUUODg6Lc8oJ+tjHPsbOv/0IL/3uSxnPj6MPD5H7Xhu9U4KYrrGuUEEBKegHf4EoTKMqkD+zi7Pd41TVDUy7ZVacnRomLPfYSIc++gQASRFjMlsyHboU9IZF0XqHTpq0q3nB5/s/Dx9Gqxpcf85qsqXKHAEGOGQK+exju8ez9LVK4f2v3wyY12UazknPuk3ILx36hOnIz1wtn3eqoPHUSIahpMrr52mPezQoimy523SUgh5yNhY5OCyLU07Qs4/sYPw3P2emOMPh3GHKzw9A0c1F+wVRrciavKy/djU3wa7bKYXbEQgCkW42KsN4qDChSIdenJqsZehew3boCWKMZ0rgj9pVLvMtivqVCmuVSRLK/IOcHxtM8m/3HeC1Z63i5dtkFczoPLHLoRn5oZGpm/gzky8zmS3zhgt7CXrdVAxBU8jL7rFswyLnQg7d4gxL0FWNIbMb4tm9s3q0HwOfu/k83nnVuqO6TeQIvVwcHBwW55QSdCEEemKGeNaczKMXqCQSAGwfEkTLeboLsgtg2VeBfT9F3X49AN5oD36lwpm+CUYNWZ6nz0wSKYPiEiixDjB7pCfqHbpeAMOgJSBjlYg3AvkpSh4pim5FMFmdK5BCCD78v7voaQ7yDzeeac/MnC92GZiZ69D3jMu45ZyeJi7oa8bvcfHOq9aRLVUYrWtbkFF1Ouo28IT8HntHZlvEZ08CSqu6veW/Jdy44edYuGJj21H3ILcy9/qdpg4ODkvn1BJ0VcWjG7TkQTEEBb1ANSUrRzaNCiL5NB2qdK8JfQwMHbXrDACUSB8Al0XGSZpiHC5CUwlcXgNlw0vsx0mIGOOZoszQAbQ857Sfwycu+QSXdF4IhWnGQpvt88eqcxttZYo6I6kib71kLdGA186sZy+MahXD7iNeP/HHys+3rorx56/eyn+8+TwuWdfacMx6nJ7moN1yNuxz29FGT3OIiN+Dx6WQUjWSBQ2XwlFHJSvFho4It914Ji/bunCJp4ODw8KcUoKuJ5MAuAyIq5DX8lQyUtx8FfAPJGlVpUOfNOTCpRqUcYgI9lIUPs7xDpHzRTAUiBcEcbWK269A93kAGEIhTYSJbFn2Nwe44+0kf/Bn/NfdnYyPjYMwOOTdaD+vIS0657laGfaquHTmzSEvIZ97jqAPJQtY0+IyRZ2JTInL/+E+PvOz/XTHAzSHfWzrjnHt1k62dEVRFJmtW6SLGk0hn73NPuhz2+WBPc1BFEWhKeQjpWokzDmiS9nefzxQFIWbL1rjLIo6OBwjp5SgZyeG7N9bs7LpVjWdR/cJDIBDWWKmXo5U5Q7QouWylQD7RC8bjEEMxUU+CLEiRMsCV8gPzWsBSBHBwMVEpgjmIGgO/hzx7PfYP5lnYFAuTu6lj6r59h4qzt1hOZ6Rgt4Vl0KrKAo9zcE5kcuAuSDqUuQC557xLKPpIq88o4u/vm57w7khn4d1beE5Dj0e9NaGMfs8tmBa3wqaQ15SBRm5tISdYcoODicrp4Sg3/UHH+Bn//ifpCdrgt6eE+QLU1RyKoW4YLgDiiMVvLqfigtGyiOguFC9Zj+Rqp/dxlq6iwcAgRqEmArhErijYbtbYkLE2NARYTpXpmo24lJdYTpEghaylFLjAAzqTWTd0v0PaVFKutxq/9avPMJvD84waQt6bcNRT3NojkM/ZObnm7tiZIo60zmZc/+fV27mldvnNvza1h3nvr1TXHbbL/nVvinSqinodcOYrUESVm7fbDr0ZEFzBN3B4STmpBd0Q9Pof+w+qr++n9xUbXZ1V9Ygnxmimi9TCAqGu6GU8lCliWJQMFachkgnqiEF0jB87BZr8VeybFaGUYOCWEEQNIK4Vm+BuJyTnSTGOb1NGAKmWy9g17p38+HSOwA4yzNMNTsBwGApQs7bgYGLBDGSBY3DSZXfHJjhF3smmciWUBQaFiwXcuhtER+9zUEyqs60uXA5u1OhxTuu7OemC3pIF3XufW6CXKnS4NBDDQ7dFPSw145cWiPLXxB1cHB4cTj5BP3xr8E/nwlVWclS3H8AjzAIpA+jTo/bp3VkoVDOUC3o5EMKM20Ghu6imHCj+98SGt8AABRLSURBVAUjVRWiq1ArUkCNqofdhoxVXu7aSTkoaMlDsADujjUQiFH2xkmIqN1U6sFhjTfsu4bA+isBuCAwYpcsHlCDFIOd6P4WKep5za4nPzRTYCJTojXsb9h92dMcJFuq2AOWAQZm8qxri9AU8toOPRrwLFirfVZPE7fdeBZnro6zY0CuKTSFGiOX+R26TiKvNfRwcXBwOLk4+QQdIDNkC2f6mecAaMlm0Wam0TzgDhu05QQFLU9FNciE3WTbzHLF8TyKXzDgcZGPdqKapYiVio+9Yg0Chdd4n0ALCjrT4CuUib7sWgDu6/sw/119lb0Z519/uR+AT7zxSoit5gz3EJ7iNMIXIV3xsafvrYxd+GcAJAplBsx68oHpAhPZ0pz2sddu7STgdfGn332KqiEo/7/27j04rrO84/j32ftFu6u7dYttyReCGiexYxORixPikMYpxClQam4lJTTlkhaG0k7aZNIMZaYEpp22U1oaSgqlFBhCGUwbChRooYUkNiYhTpwQ23ESObIl37SyVivt5ekf5+xqLUuWnEi7OvLzmdF4dXQkPX539dOr97znffMFnjg0zK90Jp09Occmpt26bTq9HcnycE0qGuRN67u4c+uFpGJBbuht4yOvX0tPs3P9oD4W4sToBMNjORtyMcbDvBfoSXeNkbRzUTO9Zw8AobwSODhAOgahaJ6GU8qxEyfRPJxIBBhvKII400XqgnmKIuyORMo99Hw+SIYI2thDL/vJR51zj2+7krrNmwH47/AWno9fUu7Zvnh8jOt7W51NI9rWsab4HKnsIfIxZ9rdRGcfXPp2wNk0ohSw/ScyvHA8w7Lk6YG+qqWOj227iJ/sP8Znf3yAX/QPk80VeW1PE/WxENlckf4TmTM2V55Ob/vkVMn6WJDlTTHed42zpk1bKsLvb1lTns3SEAuSd6fS2JCLMd7lwUBvd/4dcQI9u/dJiu4su8SBI2TifgLRAslTQmHYCdBjcR9Rf5FQwhmmaQwWCKqyyzdBJpdBEMbdXaCl7WIA+nuUHZcL47e/tfytB9JZ2lIRGuMhQu5QyVsuc2+Tb1tHe+55+nKPMtjuzFnvSEXprI8SCfrY89JweS2Wojq99Kk9dHA2f7h6TTNf+MlB/m/fUUTgNd2N5XVR9g+NzrmHXpKaZV55aWciwIZcjPEwDwZ6qYc+gBaLBA4c4Gk3UyOjOSaiEGxKUTciREtLx8aUpC9IpN55PxRW1o2P8+3jL3HwxCDRQJSxXNFZVKp9HQDHGsL8y3V+vrVntLyC4ZHhLG3JCCLCslSY5rowm9e4y8y2rcOH8kSxmx90vg9wdvgJBXxctqKBhw8c57mjo7xq2eSc9LZpAl1E2L5pOQPDWT7/k4Nc2JakPhYq3+xzajw/p0Bf05oo7x+aip49pBsqQtyGXIzxLu8FerQB/GFIO/uBBrJZdq6d/G8UIgUCHRcQzEPjSWfdliPRAolAnEiLM7vDXxdl09g4h+U433vxW/S19zE6UXBmf7g99FzRmaP9zZ+d5MHd/RSLygvHM3S6wy2/u3kV97yxl0DpombPtezr3Mb7cx/i0RdHiQb9tLlDKn3dTewdSDM0Ms51FXdBtiXPDHRw9t9MRYOczOTo63GWFKjsZc8l0EMBH2taE2d87nQaYpMfb66zQDfGq7wX6CLOsMvIANm9ewF4uktIu9O5JZTjUMRZf7xj0BkXTscgEUoQbncuAvoTcTZlsyBKWJLce8W9ZMbzzuwPN9DHC865WohxYGiUIyNZxnIFelqc4+/sW8HNl1Rs+BBJse+1n6RfW3nkwDFWNk9uqNy3qql82iVd9eWNmqfroYOz2mDpa5du56+vCN25jKHD5LDLbIFeH6vsodsYujFe5c17rJOdkH6JsX2Pkff7eKEFBlN+kmMF/OEiXzrVxK3A5U87p6djkFi5mVhPGyl/P8HWnawf2U9x+GLWNtxCQ6SB0Yn9Tg89sYzC5R/g0E4/Od8QWohx8Ogo+wed8e9VZ9lXs9RzHhwZZ1P35BroF3eliAR9ZHNFelri9DTHOXpqfMZAB3jv1d2MZHNctdr55VQZyq0z9Oyn2r7pAhpiwfJmzzMp9dBruY6LMeaV814PHSDRDumXyDz6KM93pfD5gpysc4IoHC6yO76WQ61hmkZA/Uo2BIme1+HbfAcd932Cw+EEGY3TOvE7jI44PeHMRL68xol/65+Tjm4he+RmRHyM5Qr89ICzcUR3y8yBXnmTUE9F8IcDfi5b0YAIrGiK0eN+jZmGXABWNMX5q+3ryysP1leMg8+1h75xZSN3/VrvrOeVflnUch0XY8wr581AT3ZQOOoMuTzfWeCCQoZAnXPBMxoqsOWKy3lunTMbJhdRECkvbwvw78Ur+Y/YNjYsb+Cwewv+6HihvAohTO5wX+ohf3/v4Gnj4tNprgjaninBf+sV3dx2ZTfhgJ9tl3byzr7l57RMbCISoLSn8lzG0M9FwO8jFQ3aBVFjPM6zgZ45rFAscqjjJI0FpT7mzicPRNj86i5W3NLHRABG3SW5O+uc2TGqyudPXsITq99PWyrM4Mg4haKe1kMHymPl77jcWcPl6cMjdDfHz9ipvlI05CfhhnS3e9NOyet7l3H3G5ze8mtXNfHxW9ad03/Z5xOSkSA+WZiZKA0xC3RjvM6jY+gdZAbDiB8e7/IRC6zmwpZH2LsySDDaSk9DjBdHOvn6lT6WaQFRP81Rp6d9OJ3lRCZHb0cSEaFQVI6eGnd66BXLtt54URuHh7NsefUyYiE/mYnCWYdbSloSYUbG8+Ue/nxKRZ3xcP8CDIu8eUOX3VRkjMd5M9ATHWQGQ0QaxxkI17EiuJbl0f+lu+8w/5S/hGtSEY7XtfGNK3zUFYNorh7BCcHS0rK97UlOuBtGHB7OOj308GQPfdPKRjatdIZpupvjPPlS+qwXREuaE2HS2dysM0tejlQ0WN5Ieb793pY1s59kjFnUvBnoyQ7GRwLEV2XI+ZREtJX9gTWszT/DschyQgEf8YRzsfOUz0d+opGTmRwN8VA50C9sT3LQvRV/YDg7OQ99GqVAn0sPfetFbeUlbufb1nVtVGwXaowxp5lToIvIjcBfA37gH1X1E1M+/hHgvUAeGALeo6rPz3OtZRpuRPM+xlLOPOumSDPPxtezdvgZxhLdANTFJ9cKL+YauPube7jponaeGkizssnZeq20lkr/iQwT+eJpY+iVSuPpPVPGxafz21d2v6L/29l84NrVC/a1jTHeN+tFURHxA58GtgK9wNtEZOpcuJ8DG1X1YuBB4JPzXWilwpgzM+XE8ksBaI01s7fpBh4r9pBtcW4MiocnwzcVbOVHvxziji/v5sfPHi3fcNMUDxH0C999ylm5sasxynRe96oWNq5oYO2yM7eSM8aYxWIus1xeA+xT1QOqOgF8BdhWeYKq/lBVSzszPAx0zW+ZpyumnWGTgRZn9cCORCvjTa/mlomP09Ts3FofC0zuOP+nW6/ikT/ZwqqWOk6N58srEfp8QmsiwqPPHScW8nND75k7AAGsX97Ag++/gugMPXhjjFkM5hLoncCLFe/3u8dmchvw7ek+ICK3i8guEdk1NDQ09yqnKKRHADjqdy5qdiZbyrevl/bJDPgCRMUJ4M66TmKhAH/3jg2saomzeW1L+WuV7tbcelH7Oc0LN8aYxWZeE0xE3glsBK6Z7uOqej9wP8DGjRtf9uW94ojTQz/udzataE+kGIw5u/yU1ioHiEcaGBs7Skedc4F07bIE3/+Da0/7WqVALy+Da4wxHjWXQD8EXFDxfpd77DQicj1wF3CNqi7MNA9XwR1yORHIoyo0x+u4uCtEd3OcCys2dqgL1pEeT5fnoE+nr6eJofQ4l1esvWKMMV40l0DfCawRkW6cIN8OvL3yBBFZD/wDcKOqDs57lVNMBnoBimHi4SDLklF++NFrTzsvFozRUdeBT2YeWXpX3wre1bdiIcs1xpiqmDXQVTUvIncA38GZtviAqj4pIh8DdqnqDuBTQB3wNffW+BdU9eaFKro44oyhnwzm0GyY6AwbJq9rXofaxG1jzHliTmPoqvoQ8NCUY/dUPL5+nus6q0J6BPx+0r5xRMMz3gp/d9/d1SzLGGNqypOLcxVH0vgTCcY1i6itP2KMMeDRQC+kR/Alk0wUx/Azt80ejDFmqfNmoLs99JwFujHGlHky0IvpEXzJBDkdIyjT365vjDHnG08GutNDT1IgS9BngW6MMeDRQC/10IuME7JAN8YYwKProRdGRpC6OlRyRCzQjTEG8GAPXScm0LExCnFnumLEH5vlM4wx5vzguUAvnDoFQD7mBHo0YIFujDHgwUAvrYU+EXP27IwF5n8zZmOM8SLPBXrBXcclG3XWb6kLWg/dGGPAi4Hu9tBPhZzS4yHroRtjDHgw0MsrLQacVRST4dk3bjbGmPOB5wK91EM/GXR2K0qELNCNMQY8GOilHvoJfwGAlPXQjTEG8OCNRXXXXUewvZ3j7AegPmqBbowx4MEeeri7m+RNN3Eqn0HVRzJkd4oaYwx4MNBLRidGoRgiFvbcHxnGGLMgPBvop3IZtBgmFrJAN8YY8HCgj+VH0eLMG0QbY8z5xsOBnoFimGjIAt0YY8DLgV4oDblYoBtjDHg40CcKY2gxRMSGXIwxBvBwoI8Xx/BpBL9Pal2KMcYsCp4MdFVlQkcJSKTWpRhjzKLhyUDvH+knp6OEi521LsUYYxYNTwb6ziM7AYgV19a4EmOMWTy8GeiHdxLQJAm/9dCNMabEc4Guquw8vJNYca3dJWqMMRU8F+j9I/0cyRwhlFttd4kaY0wFzwX63/70PwHIpFfaTUXGGFPBc4He29pFIX0JR46l7LZ/Y4ypMKdAF5EbReQZEdknIndO8/GwiHzV/fgjIrJyvgsteff6rfzhhj8DxIZcjDGmwqxXFUXED3waeD3QD+wUkR2q+lTFabcBJ1R1tYhsB+4DfnMhCga49YqVZCYK9PU0LdS3MMYYz5lLD/01wD5VPaCqE8BXgG1TztkGfMF9/CCwRUQW7J58EeGDr1vNZSsaFupbGGOM58wl0DuBFyve73ePTXuOquaBYeCM7rOI3C4iu0Rk19DQ0Mur2BhjzLSqelFUVe9X1Y2qurGlpaWa39oYY5a8uQT6IeCCive73GPTniMiASAFHJuPAo0xxszNXAJ9J7BGRLpFJARsB3ZMOWcH8G738VuAH6iqzl+ZxhhjZjPrLBdVzYvIHcB3AD/wgKo+KSIfA3ap6g7gc8AXRWQfcBwn9I0xxlTRnBZDUdWHgIemHLun4nEW+I35Lc0YY8y58NydosYYY6ZngW6MMUuE1OrapYgMAc+/zE9vBo7OYznzabHWZnWdG6vr3C3W2pZaXStUddp53zUL9FdCRHap6sZa1zGdxVqb1XVurK5zt1hrO5/qsiEXY4xZIizQjTFmifBqoN9f6wLOYrHWZnWdG6vr3C3W2s6bujw5hm6MMeZMXu2hG2OMmcIC3RhjlgjPBfps2+FVsY4LROSHIvKUiDwpIh9yj98rIodE5DH37aYa1HZQRJ5wv/8u91ijiHxPRJ51/63q7iAi8qqKNnlMRNIi8uFatZeIPCAigyKyp+LYtG0kjr9xX3O/EJENVa7rUyLytPu9vyEi9e7xlSIyVtF2n6lyXTM+dyLyx257PSMiv7pQdZ2ltq9W1HVQRB5zj1elzc6SDwv7GlNVz7zhLA62H+gBQsDjQG+NamkHNriPE8AvgV7gXuCjNW6ng0DzlGOfBO50H98J3Ffj5/EwsKJW7QVsBjYAe2ZrI+Am4NuAAH3AI1Wu6wYg4D6+r6KulZXn1aC9pn3u3J+Dx4Ew0O3+zPqrWduUj/8FcE812+ws+bCgrzGv9dDnsh1eVajqgKrudh+PAHs5cyenxaRym8AvALfUsJYtwH5Vfbl3Cr9iqvojnJVBK83URtuAf1bHw0C9iLRXqy5V/a46O4EBPIyzJ0FVzdBeM9kGfEVVx1X1OWAfzs9u1WsTEQHeCnx5ob7/DDXNlA8L+hrzWqDPZTu8qhORlcB64BH30B3un00PVHtow6XAd0XkZyJyu3tsmaoOuI8PA8tqUFfJdk7/Aat1e5XM1EaL6XX3HpyeXEm3iPxcRP5HRK6uQT3TPXeLqb2uBo6o6rMVx6raZlPyYUFfY14L9EVHROqArwMfVtU08PfAKuBSYADnz71qu0pVNwBbgQ+KyObKD6rzN15N5quKs0nKzcDX3EOLob3OUMs2momI3AXkgS+5hwaA5aq6HvgI8K8ikqxiSYvyuZvibZzeeahqm02TD2UL8RrzWqDPZTu8qhGRIM6T9SVV/TcAVT2iqgVVLQKfZQH/1JyJqh5y/x0EvuHWcKT0J5z772C163JtBXar6hG3xpq3V4WZ2qjmrzsRuRV4A/AONwhwhzSOuY9/hjNWvbZaNZ3luat5e0F5O8w3AV8tHatmm02XDyzwa8xrgT6X7fCqwh2b+xywV1X/suJ45bjXrwN7pn7uAtcVF5FE6THOBbU9nL5N4LuBb1azrgqn9Zhq3V5TzNRGO4Dfcmci9AHDFX82LzgRuRH4I+BmVc1UHG8REb/7uAdYAxyoYl0zPXc7gO0iEhaRbreuR6tVV4XrgadVtb90oFptNlM+sNCvsYW+2jvfbzhXg3+J85v1rhrWcRXOn0u/AB5z324Cvgg84R7fAbRXua4enBkGjwNPltoIaAK+DzwL/BfQWIM2i+NsHp6qOFaT9sL5pTIA5HDGK2+bqY1wZh582n3NPQFsrHJd+3DGV0uvs8+4577ZfY4fA3YDb6xyXTM+d8Bdbns9A2yt9nPpHv888L4p51alzc6SDwv6GrNb/40xZonw2pCLMcaYGVigG2PMEmGBbowxS4QFujHGLBEW6MYYs0RYoBtjzBJhgW6MMUvE/wP4OxUsVtn4TwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for k,v in histories.items():\n",
    "    plt.plot(v)\n",
    "plt.legend(histories.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'N = 4'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_data_for_ai' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-3e7c07c4dae5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mpred_generator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_data_for_ai\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mgame_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequested_action\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_generator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'create_data_for_ai' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "pred_generator = create_data_for_ai(h, w)\n",
    "game_state, requested_action = next(data_generator)\n",
    "\n",
    "move_probs = model.predict(game_state)[0].reshape(3,3)\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "\n",
    "axs[0].imshow(game_state[0])\n",
    "axs[1].imshow(move_probs, cmap = 'gray')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from game.game_state import Game_Environment\n",
    "from matplotlib import pyplot as plt\n",
    "from resnet import resnet_rl, lr_schedule\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import random\n",
    "class DQN:\n",
    "    #https://towardsdatascience.com/reinforcement-learning-w-keras-openai-dqns-1eed3a5338c\n",
    "\n",
    "    def __init__(self, envlist):\n",
    "        self.envlist = envlist\n",
    "        self.memory  = deque(maxlen=2000)\n",
    "        \n",
    "        self.input_shape = self.envlist[0].state.shape\n",
    "        self.classes = len(self.envlist[0].action_space)\n",
    "        self.action_space = self.envlist[0].action_space\n",
    "        \n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.01\n",
    "        self.tau = .05\n",
    "        self.model = self.create_model(3)\n",
    "        self.target_model = self.create_model(3)\n",
    "        \n",
    "    def create_model(self, n):\n",
    "        model = resnet_rl(input_shape=self.input_shape , \n",
    "                          depth=n*9+2, \n",
    "                          num_classes =  self.classes)\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        return model\n",
    "    \n",
    "    def remember(self, state, action, reward, new_state, done):\n",
    "        self.memory.append([state, action, reward, new_state, done])\n",
    "        \n",
    "    def replay(self):\n",
    "        batch_size = 500\n",
    "        if len(self.memory) < batch_size: \n",
    "            return\n",
    "        samples = random.sample(self.memory, batch_size)\n",
    "        states, actions, rewards, new_states, done = map(list, zip(*samples))\n",
    "        targets = self.calculate_Q(states, actions, rewards, new_states, done)\n",
    "        self.model.fit(np.stack(states), targets, epochs=1, verbose=1)\n",
    "        self.target_train()\n",
    "        self.update_epsilon()\n",
    "    \n",
    "    \n",
    "    def calculate_Q(self, states, actions, rewards, new_states, done):\n",
    "        done = np.array(done)\n",
    "        targets = self.target_model.predict(np.stack(states))\n",
    "        if(True):\n",
    "            Q_future = self.target_model.predict(np.stack(new_states)[~done]).max(axis = 1)\n",
    "            rewards = np.array(rewards)\n",
    "            np.array(rewards[~done]) + Q_future * self.gamma\n",
    "        for i,v in enumerate(actions):\n",
    "            targets[i, v] += rewards[i]\n",
    "        return(targets)\n",
    "    \n",
    "    def target_train(self):\n",
    "        weights = self.model.get_weights()\n",
    "        self.target_model.set_weights(weights)\n",
    "        \n",
    "    def explore(self, states, probs = False):\n",
    "        action_rewards = self.model.predict(states)\n",
    "        action_rewards -= action_rewards.min(axis=1)[:, np.newaxis]\n",
    "        action_rewards += action_rewards.max(axis=1)[:, np.newaxis] * self.epsilon\n",
    "        action_rewards /= action_rewards.sum(axis=1)[:, np.newaxis]\n",
    "        actions = np.apply_along_axis(lambda x: random.choices(self.action_space, weights=x), 1, action_rewards)\n",
    "        if(probs == True):\n",
    "            return(np.round(action_rewards, 2).reshape(3,3))\n",
    "        return actions\n",
    "    \n",
    "    def exploit(self, states, probs = False):\n",
    "        action_rewards = self.model.predict(states)\n",
    "        action_rewards -= action_rewards.min(axis=1)[:, np.newaxis]\n",
    "        action_rewards /= action_rewards.sum(axis=1)[:, np.newaxis]\n",
    "        if(probs == True):\n",
    "            return(np.round(action_rewards, 2).reshape(3,3))\n",
    "        actions = np.apply_along_axis(lambda x: random.choices(self.action_space, weights=x), 1, action_rewards)\n",
    "        return actions\n",
    "    \n",
    "    def update_epsilon(self):\n",
    "        self.epsilon *= self.epsilon_decay\n",
    "        self.epsilon = max(self.epsilon, self.epsilon_min)\n",
    "        print(\"Epsilon set to {}\".format(self.epsilon))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting trial 0\n",
      "Starting trial 1\n",
      "Starting trial 2\n",
      "Starting trial 3\n",
      "Starting trial 4\n",
      "Starting trial 5\n",
      "Starting trial 6\n",
      "Starting trial 7\n",
      "Starting trial 8\n",
      "Starting trial 9\n",
      "Starting trial 10\n",
      "Starting trial 11\n",
      "Starting trial 12\n",
      "Starting trial 13\n",
      "Starting trial 14\n",
      "Starting trial 15\n",
      "Starting trial 16\n",
      "Starting trial 17\n",
      "Starting trial 18\n",
      "Starting trial 19\n",
      "Starting trial 20\n",
      "Starting trial 21\n",
      "Starting trial 22\n",
      "Starting trial 23\n",
      "Starting trial 24\n",
      "Starting trial 25\n",
      "Starting trial 26\n",
      "Starting trial 27\n",
      "Starting trial 28\n",
      "Starting trial 29\n",
      "Starting trial 30\n",
      "Starting trial 31\n",
      "Starting trial 32\n",
      "Starting trial 33\n",
      "Starting trial 34\n",
      "Starting trial 35\n",
      "Starting trial 36\n",
      "Starting trial 37\n",
      "Starting trial 38\n",
      "Starting trial 39\n",
      "Starting trial 40\n",
      "Starting trial 41\n",
      "Starting trial 42\n",
      "Starting trial 43\n",
      "Starting trial 44\n",
      "Starting trial 45\n",
      "Starting trial 46\n",
      "Starting trial 47\n",
      "Starting trial 48\n",
      "Starting trial 49\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 3s 6ms/sample - loss: 2.3455\n",
      "Epsilon set to 0.995\n",
      "Starting trial 50\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 415us/sample - loss: 1.1802\n",
      "Epsilon set to 0.990025\n",
      "Starting trial 51\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 406us/sample - loss: 0.7865\n",
      "Epsilon set to 0.985074875\n",
      "Starting trial 52\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 405us/sample - loss: 0.6749\n",
      "Epsilon set to 0.9801495006250001\n",
      "Starting trial 53\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 395us/sample - loss: 0.6399\n",
      "Epsilon set to 0.9752487531218751\n",
      "Starting trial 54\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 403us/sample - loss: 0.6284\n",
      "Epsilon set to 0.9703725093562657\n",
      "Starting trial 55\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 413us/sample - loss: 0.6201\n",
      "Epsilon set to 0.9655206468094844\n",
      "Starting trial 56\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 423us/sample - loss: 0.6089\n",
      "Epsilon set to 0.960693043575437\n",
      "Starting trial 57\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 411us/sample - loss: 0.5994\n",
      "Epsilon set to 0.9558895783575597\n",
      "Starting trial 58\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 413us/sample - loss: 0.5888\n",
      "Epsilon set to 0.9511101304657719\n",
      "Starting trial 59\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 415us/sample - loss: 0.5798\n",
      "Epsilon set to 0.946354579813443\n",
      "Starting trial 60\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 407us/sample - loss: 0.5703\n",
      "Epsilon set to 0.9416228069143757\n",
      "Starting trial 61\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 429us/sample - loss: 0.5609\n",
      "Epsilon set to 0.9369146928798039\n",
      "Starting trial 62\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 412us/sample - loss: 0.5511\n",
      "Epsilon set to 0.9322301194154049\n",
      "Starting trial 63\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 419us/sample - loss: 0.5433\n",
      "Epsilon set to 0.9275689688183278\n",
      "Starting trial 64\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 407us/sample - loss: 0.5324\n",
      "Epsilon set to 0.9229311239742362\n",
      "Starting trial 65\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 403us/sample - loss: 0.5233\n",
      "Epsilon set to 0.918316468354365\n",
      "Starting trial 66\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 425us/sample - loss: 0.5112\n",
      "Epsilon set to 0.9137248860125932\n",
      "Starting trial 67\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 421us/sample - loss: 0.5005\n",
      "Epsilon set to 0.9091562615825302\n",
      "Starting trial 68\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 383us/sample - loss: 0.4938\n",
      "Epsilon set to 0.9046104802746175\n",
      "Starting trial 69\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 391us/sample - loss: 0.4832\n",
      "Epsilon set to 0.9000874278732445\n",
      "Starting trial 70\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 394us/sample - loss: 0.4720\n",
      "Epsilon set to 0.8955869907338783\n",
      "Starting trial 71\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 409us/sample - loss: 0.4647\n",
      "Epsilon set to 0.8911090557802088\n",
      "Starting trial 72\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 409us/sample - loss: 0.4537\n",
      "Epsilon set to 0.8866535105013078\n",
      "Starting trial 73\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 392us/sample - loss: 0.4460\n",
      "Epsilon set to 0.8822202429488013\n",
      "Starting trial 74\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 387us/sample - loss: 0.4386\n",
      "Epsilon set to 0.8778091417340573\n",
      "Starting trial 75\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 391us/sample - loss: 0.4297\n",
      "Epsilon set to 0.8734200960253871\n",
      "Starting trial 76\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 389us/sample - loss: 0.4194\n",
      "Epsilon set to 0.8690529955452602\n",
      "Starting trial 77\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 401us/sample - loss: 0.4127\n",
      "Epsilon set to 0.8647077305675338\n",
      "Starting trial 78\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 401us/sample - loss: 0.4044\n",
      "Epsilon set to 0.8603841919146962\n",
      "Starting trial 79\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 408us/sample - loss: 0.3963\n",
      "Epsilon set to 0.8560822709551227\n",
      "Starting trial 80\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 405us/sample - loss: 0.3891\n",
      "Epsilon set to 0.851801859600347\n",
      "Starting trial 81\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 411us/sample - loss: 0.3823\n",
      "Epsilon set to 0.8475428503023453\n",
      "Starting trial 82\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 417us/sample - loss: 0.3761\n",
      "Epsilon set to 0.8433051360508336\n",
      "Starting trial 83\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 394us/sample - loss: 0.3662\n",
      "Epsilon set to 0.8390886103705794\n",
      "Starting trial 84\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 393us/sample - loss: 0.3574\n",
      "Epsilon set to 0.8348931673187264\n",
      "Starting trial 85\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 401us/sample - loss: 0.3510\n",
      "Epsilon set to 0.8307187014821328\n",
      "Starting trial 86\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 403us/sample - loss: 0.3451\n",
      "Epsilon set to 0.8265651079747222\n",
      "Starting trial 87\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 399us/sample - loss: 0.3389\n",
      "Epsilon set to 0.8224322824348486\n",
      "Starting trial 88\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 385us/sample - loss: 0.3351\n",
      "Epsilon set to 0.8183201210226743\n",
      "Starting trial 89\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 405us/sample - loss: 0.3285\n",
      "Epsilon set to 0.8142285204175609\n",
      "Starting trial 90\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 383us/sample - loss: 0.3213\n",
      "Epsilon set to 0.810157377815473\n",
      "Starting trial 91\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 407us/sample - loss: 0.3131\n",
      "Epsilon set to 0.8061065909263957\n",
      "Starting trial 92\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 396us/sample - loss: 0.3090\n",
      "Epsilon set to 0.8020760579717637\n",
      "Starting trial 93\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 405us/sample - loss: 0.3020\n",
      "Epsilon set to 0.798065677681905\n",
      "Starting trial 94\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 397us/sample - loss: 0.2968\n",
      "Epsilon set to 0.7940753492934954\n",
      "Starting trial 95\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 416us/sample - loss: 0.2931\n",
      "Epsilon set to 0.7901049725470279\n",
      "Starting trial 96\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 393us/sample - loss: 0.2889\n",
      "Epsilon set to 0.7861544476842928\n",
      "Starting trial 97\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 419us/sample - loss: 0.2803\n",
      "Epsilon set to 0.7822236754458713\n",
      "Starting trial 98\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 415us/sample - loss: 0.2789\n",
      "Epsilon set to 0.778312557068642\n",
      "Starting trial 99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 413us/sample - loss: 0.2723\n",
      "Epsilon set to 0.7744209942832988\n",
      "Starting trial 100\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 401us/sample - loss: 0.2663\n",
      "Epsilon set to 0.7705488893118823\n",
      "Starting trial 101\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 414us/sample - loss: 0.2638\n",
      "Epsilon set to 0.7666961448653229\n",
      "Starting trial 102\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 393us/sample - loss: 0.2581\n",
      "Epsilon set to 0.7628626641409962\n",
      "Starting trial 103\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 411us/sample - loss: 0.2544\n",
      "Epsilon set to 0.7590483508202912\n",
      "Starting trial 104\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 411us/sample - loss: 0.2487\n",
      "Epsilon set to 0.7552531090661897\n",
      "Starting trial 105\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 394us/sample - loss: 0.2439\n",
      "Epsilon set to 0.7514768435208588\n",
      "Starting trial 106\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 413us/sample - loss: 0.2389\n",
      "Epsilon set to 0.7477194593032545\n",
      "Starting trial 107\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 389us/sample - loss: 0.2347\n",
      "Epsilon set to 0.7439808620067382\n",
      "Starting trial 108\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 410us/sample - loss: 0.2336\n",
      "Epsilon set to 0.7402609576967045\n",
      "Starting trial 109\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 401us/sample - loss: 0.2262\n",
      "Epsilon set to 0.736559652908221\n",
      "Starting trial 110\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 403us/sample - loss: 0.2244\n",
      "Epsilon set to 0.7328768546436799\n",
      "Starting trial 111\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 399us/sample - loss: 0.2232\n",
      "Epsilon set to 0.7292124703704616\n",
      "Starting trial 112\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 396us/sample - loss: 0.2172\n",
      "Epsilon set to 0.7255664080186093\n",
      "Starting trial 113\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 399us/sample - loss: 0.2151\n",
      "Epsilon set to 0.7219385759785162\n",
      "Starting trial 114\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 405us/sample - loss: 0.2103\n",
      "Epsilon set to 0.7183288830986236\n",
      "Starting trial 115\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 407us/sample - loss: 0.2089\n",
      "Epsilon set to 0.7147372386831305\n",
      "Starting trial 116\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 397us/sample - loss: 0.2032\n",
      "Epsilon set to 0.7111635524897149\n",
      "Starting trial 117\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 389us/sample - loss: 0.2031\n",
      "Epsilon set to 0.7076077347272662\n",
      "Starting trial 118\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 399us/sample - loss: 0.1982\n",
      "Epsilon set to 0.7040696960536299\n",
      "Starting trial 119\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 400us/sample - loss: 0.1993\n",
      "Epsilon set to 0.7005493475733617\n",
      "Starting trial 120\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 415us/sample - loss: 0.1950\n",
      "Epsilon set to 0.697046600835495\n",
      "Starting trial 121\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 398us/sample - loss: 0.1916\n",
      "Epsilon set to 0.6935613678313175\n",
      "Starting trial 122\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 408us/sample - loss: 0.1889\n",
      "Epsilon set to 0.6900935609921609\n",
      "Starting trial 123\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 405us/sample - loss: 0.1848\n",
      "Epsilon set to 0.6866430931872001\n",
      "Starting trial 124\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 392us/sample - loss: 0.1850\n",
      "Epsilon set to 0.6832098777212641\n",
      "Starting trial 125\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 381us/sample - loss: 0.1849\n",
      "Epsilon set to 0.6797938283326578\n",
      "Starting trial 126\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 401us/sample - loss: 0.1791\n",
      "Epsilon set to 0.6763948591909945\n",
      "Starting trial 127\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 405us/sample - loss: 0.1748\n",
      "Epsilon set to 0.6730128848950395\n",
      "Starting trial 128\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 456us/sample - loss: 0.1753\n",
      "Epsilon set to 0.6696478204705644\n",
      "Starting trial 129\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 429us/sample - loss: 0.1728\n",
      "Epsilon set to 0.6662995813682115\n",
      "Starting trial 130\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 409us/sample - loss: 0.1703\n",
      "Epsilon set to 0.6629680834613705\n",
      "Starting trial 131\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 441us/sample - loss: 0.1673\n",
      "Epsilon set to 0.6596532430440636\n",
      "Starting trial 132\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 398us/sample - loss: 0.1613\n",
      "Epsilon set to 0.6563549768288433\n",
      "Starting trial 133\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 437us/sample - loss: 0.1639\n",
      "Epsilon set to 0.653073201944699\n",
      "Starting trial 134\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 397us/sample - loss: 0.1606\n",
      "Epsilon set to 0.6498078359349755\n",
      "Starting trial 135\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 389us/sample - loss: 0.1669\n",
      "Epsilon set to 0.6465587967553006\n",
      "Starting trial 136\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 379us/sample - loss: 0.1562\n",
      "Epsilon set to 0.6433260027715241\n",
      "Starting trial 137\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 416us/sample - loss: 0.1578\n",
      "Epsilon set to 0.6401093727576664\n",
      "Starting trial 138\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 404us/sample - loss: 0.1541\n",
      "Epsilon set to 0.6369088258938781\n",
      "Starting trial 139\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 403us/sample - loss: 0.1547\n",
      "Epsilon set to 0.6337242817644086\n",
      "Starting trial 140\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 399us/sample - loss: 0.1502\n",
      "Epsilon set to 0.6305556603555866\n",
      "Starting trial 141\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 413us/sample - loss: 0.1475\n",
      "Epsilon set to 0.6274028820538087\n",
      "Starting trial 142\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 410us/sample - loss: 0.1441\n",
      "Epsilon set to 0.6242658676435396\n",
      "Starting trial 143\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 403us/sample - loss: 0.1460\n",
      "Epsilon set to 0.6211445383053219\n",
      "Starting trial 144\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 401us/sample - loss: 0.1451\n",
      "Epsilon set to 0.6180388156137953\n",
      "Starting trial 145\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 406us/sample - loss: 0.1447\n",
      "Epsilon set to 0.6149486215357263\n",
      "Starting trial 146\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 407us/sample - loss: 0.1383\n",
      "Epsilon set to 0.6118738784280476\n",
      "Starting trial 147\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 407us/sample - loss: 0.1407\n",
      "Epsilon set to 0.6088145090359074\n",
      "Starting trial 148\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 415us/sample - loss: 0.1376\n",
      "Epsilon set to 0.6057704364907278\n",
      "Starting trial 149\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 413us/sample - loss: 0.1349\n",
      "Epsilon set to 0.6027415843082742\n",
      "Starting trial 150\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 399us/sample - loss: 0.1371\n",
      "Epsilon set to 0.5997278763867329\n",
      "Starting trial 151\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 406us/sample - loss: 0.1360\n",
      "Epsilon set to 0.5967292370047992\n",
      "Starting trial 152\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 397us/sample - loss: 0.1333\n",
      "Epsilon set to 0.5937455908197752\n",
      "Starting trial 153\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 397us/sample - loss: 0.1309\n",
      "Epsilon set to 0.5907768628656763\n",
      "Starting trial 154\n",
      "Train on 500 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 0s 413us/sample - loss: 0.1272\n",
      "Epsilon set to 0.5878229785513479\n",
      "Starting trial 155\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 418us/sample - loss: 0.1274\n",
      "Epsilon set to 0.5848838636585911\n",
      "Starting trial 156\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 393us/sample - loss: 0.1326\n",
      "Epsilon set to 0.5819594443402982\n",
      "Starting trial 157\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 391us/sample - loss: 0.1281\n",
      "Epsilon set to 0.5790496471185967\n",
      "Starting trial 158\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 385us/sample - loss: 0.1284\n",
      "Epsilon set to 0.5761543988830038\n",
      "Starting trial 159\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 395us/sample - loss: 0.1242\n",
      "Epsilon set to 0.5732736268885887\n",
      "Starting trial 160\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 383us/sample - loss: 0.1242\n",
      "Epsilon set to 0.5704072587541458\n",
      "Starting trial 161\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 398us/sample - loss: 0.1222\n",
      "Epsilon set to 0.567555222460375\n",
      "Starting trial 162\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 393us/sample - loss: 0.1233\n",
      "Epsilon set to 0.5647174463480732\n",
      "Starting trial 163\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 401us/sample - loss: 0.1193\n",
      "Epsilon set to 0.5618938591163328\n",
      "Starting trial 164\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 418us/sample - loss: 0.1189\n",
      "Epsilon set to 0.5590843898207511\n",
      "Starting trial 165\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 399us/sample - loss: 0.1215\n",
      "Epsilon set to 0.5562889678716474\n",
      "Starting trial 166\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 401us/sample - loss: 0.1213\n",
      "Epsilon set to 0.5535075230322891\n",
      "Starting trial 167\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 408us/sample - loss: 0.1192\n",
      "Epsilon set to 0.5507399854171277\n",
      "Starting trial 168\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 409us/sample - loss: 0.1167\n",
      "Epsilon set to 0.547986285490042\n",
      "Starting trial 169\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 405us/sample - loss: 0.1168\n",
      "Epsilon set to 0.5452463540625918\n",
      "Starting trial 170\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 408us/sample - loss: 0.1148\n",
      "Epsilon set to 0.5425201222922789\n",
      "Starting trial 171\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 426us/sample - loss: 0.1136\n",
      "Epsilon set to 0.5398075216808175\n",
      "Starting trial 172\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 401us/sample - loss: 0.1094\n",
      "Epsilon set to 0.5371084840724134\n",
      "Starting trial 173\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 426us/sample - loss: 0.1096\n",
      "Epsilon set to 0.5344229416520513\n",
      "Starting trial 174\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 429us/sample - loss: 0.1118\n",
      "Epsilon set to 0.531750826943791\n",
      "Starting trial 175\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 415us/sample - loss: 0.1110\n",
      "Epsilon set to 0.5290920728090721\n",
      "Starting trial 176\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 407us/sample - loss: 0.1154\n",
      "Epsilon set to 0.5264466124450268\n",
      "Starting trial 177\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 396us/sample - loss: 0.1060\n",
      "Epsilon set to 0.5238143793828016\n",
      "Starting trial 178\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 425us/sample - loss: 0.1101\n",
      "Epsilon set to 0.5211953074858876\n",
      "Starting trial 179\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 401us/sample - loss: 0.1065\n",
      "Epsilon set to 0.5185893309484582\n",
      "Starting trial 180\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 405us/sample - loss: 0.1114\n",
      "Epsilon set to 0.5159963842937159\n",
      "Starting trial 181\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 415us/sample - loss: 0.1132\n",
      "Epsilon set to 0.5134164023722473\n",
      "Starting trial 182\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 411us/sample - loss: 0.1072\n",
      "Epsilon set to 0.510849320360386\n",
      "Starting trial 183\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 398us/sample - loss: 0.1096\n",
      "Epsilon set to 0.5082950737585841\n",
      "Starting trial 184\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 415us/sample - loss: 0.1111\n",
      "Epsilon set to 0.5057535983897912\n",
      "Starting trial 185\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 383us/sample - loss: 0.1059\n",
      "Epsilon set to 0.5032248303978422\n",
      "Starting trial 186\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 401us/sample - loss: 0.1044\n",
      "Epsilon set to 0.500708706245853\n",
      "Starting trial 187\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 402us/sample - loss: 0.1059\n",
      "Epsilon set to 0.4982051627146237\n",
      "Starting trial 188\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 395us/sample - loss: 0.1041\n",
      "Epsilon set to 0.49571413690105054\n",
      "Starting trial 189\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 403us/sample - loss: 0.1032\n",
      "Epsilon set to 0.4932355662165453\n",
      "Starting trial 190\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 386us/sample - loss: 0.1038\n",
      "Epsilon set to 0.4907693883854626\n",
      "Starting trial 191\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 444us/sample - loss: 0.1038\n",
      "Epsilon set to 0.4883155414435353\n",
      "Starting trial 192\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 401us/sample - loss: 0.1054\n",
      "Epsilon set to 0.4858739637363176\n",
      "Starting trial 193\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 411us/sample - loss: 0.2095\n",
      "Epsilon set to 0.483444593917636\n",
      "Starting trial 194\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 404us/sample - loss: 4.6960\n",
      "Epsilon set to 0.4810273709480478\n",
      "Starting trial 195\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 405us/sample - loss: 114549.9967\n",
      "Epsilon set to 0.47862223409330756\n",
      "Starting trial 196\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 397us/sample - loss: 92674980.0320\n",
      "Epsilon set to 0.47622912292284103\n",
      "Starting trial 197\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 403us/sample - loss: 340236877.3120\n",
      "Epsilon set to 0.4738479773082268\n",
      "Starting trial 198\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 407us/sample - loss: 683524516.8640\n",
      "Epsilon set to 0.47147873742168567\n",
      "Starting trial 199\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 407us/sample - loss: 684784673.7920\n",
      "Epsilon set to 0.46912134373457726\n",
      "Starting trial 200\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 411us/sample - loss: 221029394.3040\n",
      "Epsilon set to 0.46677573701590436\n",
      "Starting trial 201\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 411us/sample - loss: 65218527.4240\n",
      "Epsilon set to 0.46444185833082485\n",
      "Starting trial 202\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 405us/sample - loss: 13278276.8560\n",
      "Epsilon set to 0.46211964903917074\n",
      "Starting trial 203\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 390us/sample - loss: 4956584.0480\n",
      "Epsilon set to 0.4598090507939749\n",
      "Starting trial 204\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 403us/sample - loss: 2171778.7420\n",
      "Epsilon set to 0.457510005540005\n",
      "Starting trial 205\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 403us/sample - loss: 961153.7540\n",
      "Epsilon set to 0.45522245551230495\n",
      "Starting trial 206\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 410us/sample - loss: 360918.5685\n",
      "Epsilon set to 0.4529463432347434\n",
      "Starting trial 207\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 395us/sample - loss: 174601.7575\n",
      "Epsilon set to 0.4506816115185697\n",
      "Starting trial 208\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 393us/sample - loss: 106180.8823\n",
      "Epsilon set to 0.4484282034609769\n",
      "Starting trial 209\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 404us/sample - loss: 40659.3529\n",
      "Epsilon set to 0.446186062443672\n",
      "Starting trial 210\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 408us/sample - loss: 36773.4569\n",
      "Epsilon set to 0.4439551321314536\n",
      "Starting trial 211\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 415us/sample - loss: 16842.9539\n",
      "Epsilon set to 0.4417353564707963\n",
      "Starting trial 212\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 410us/sample - loss: 11330.6400\n",
      "Epsilon set to 0.43952667968844233\n",
      "Starting trial 213\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 390us/sample - loss: 9891.4113\n",
      "Epsilon set to 0.43732904629000013\n",
      "Starting trial 214\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 393us/sample - loss: 7426.0005\n",
      "Epsilon set to 0.4351424010585501\n",
      "Starting trial 215\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 397us/sample - loss: 3908.6089\n",
      "Epsilon set to 0.43296668905325736\n",
      "Starting trial 216\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 407us/sample - loss: 4830.8891\n",
      "Epsilon set to 0.43080185560799106\n",
      "Starting trial 217\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 413us/sample - loss: 2821.3518\n",
      "Epsilon set to 0.4286478463299511\n",
      "Starting trial 218\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 395us/sample - loss: 1615.5180\n",
      "Epsilon set to 0.42650460709830135\n",
      "Starting trial 219\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 399us/sample - loss: 2110.1553\n",
      "Epsilon set to 0.42437208406280985\n",
      "Starting trial 220\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 413us/sample - loss: 1471.8895\n",
      "Epsilon set to 0.4222502236424958\n",
      "Starting trial 221\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 409us/sample - loss: 1621.1377\n",
      "Epsilon set to 0.42013897252428334\n",
      "Starting trial 222\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 391us/sample - loss: 2251.7878\n",
      "Epsilon set to 0.4180382776616619\n",
      "Starting trial 223\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 394us/sample - loss: 1235.7854\n",
      "Epsilon set to 0.4159480862733536\n",
      "Starting trial 224\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 407us/sample - loss: 964.12730s - loss: 994.266\n",
      "Epsilon set to 0.41386834584198684\n",
      "Starting trial 225\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 400us/sample - loss: 876.3843\n",
      "Epsilon set to 0.4117990041127769\n",
      "Starting trial 226\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 413us/sample - loss: 632.4165\n",
      "Epsilon set to 0.40974000909221303\n",
      "Starting trial 227\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 381us/sample - loss: 1883.1015\n",
      "Epsilon set to 0.40769130904675194\n",
      "Starting trial 228\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 390us/sample - loss: 1166.2113\n",
      "Epsilon set to 0.40565285250151817\n",
      "Starting trial 229\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 407us/sample - loss: 1111.0979\n",
      "Epsilon set to 0.4036245882390106\n",
      "Starting trial 230\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 393us/sample - loss: 768.8568\n",
      "Epsilon set to 0.4016064652978155\n",
      "Starting trial 231\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 405us/sample - loss: 1115.9678\n",
      "Epsilon set to 0.3995984329713264\n",
      "Starting trial 232\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 399us/sample - loss: 403.9656\n",
      "Epsilon set to 0.3976004408064698\n",
      "Starting trial 233\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 389us/sample - loss: 1299.6915\n",
      "Epsilon set to 0.39561243860243744\n",
      "Starting trial 234\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 404us/sample - loss: 691.7961\n",
      "Epsilon set to 0.3936343764094253\n",
      "Starting trial 235\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 405us/sample - loss: 439.8968\n",
      "Epsilon set to 0.39166620452737816\n",
      "Starting trial 236\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 387us/sample - loss: 579.6747\n",
      "Epsilon set to 0.3897078735047413\n",
      "Starting trial 237\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 408us/sample - loss: 552.2855\n",
      "Epsilon set to 0.3877593341372176\n",
      "Starting trial 238\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 399us/sample - loss: 405.0852\n",
      "Epsilon set to 0.3858205374665315\n",
      "Starting trial 239\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 414us/sample - loss: 1144.7671\n",
      "Epsilon set to 0.38389143477919885\n",
      "Starting trial 240\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 393us/sample - loss: 682.4892\n",
      "Epsilon set to 0.3819719776053028\n",
      "Starting trial 241\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 397us/sample - loss: 784.4775\n",
      "Epsilon set to 0.3800621177172763\n",
      "Starting trial 242\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 389us/sample - loss: 487.6782\n",
      "Epsilon set to 0.37816180712868996\n",
      "Starting trial 243\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 417us/sample - loss: 848.8520\n",
      "Epsilon set to 0.37627099809304654\n",
      "Starting trial 244\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 449us/sample - loss: 1144.1221\n",
      "Epsilon set to 0.3743896431025813\n",
      "Starting trial 245\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 397us/sample - loss: 625.9661\n",
      "Epsilon set to 0.37251769488706843\n",
      "Starting trial 246\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 408us/sample - loss: 776.7298\n",
      "Epsilon set to 0.3706551064126331\n",
      "Starting trial 247\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 397us/sample - loss: 578.4212\n",
      "Epsilon set to 0.36880183088056995\n",
      "Starting trial 248\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 407us/sample - loss: 866.3833\n",
      "Epsilon set to 0.3669578217261671\n",
      "Starting trial 249\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 392us/sample - loss: 730.5235\n",
      "Epsilon set to 0.36512303261753626\n",
      "Starting trial 250\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 417us/sample - loss: 576.2346\n",
      "Epsilon set to 0.3632974174544486\n",
      "Starting trial 251\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 404us/sample - loss: 1228.6770\n",
      "Epsilon set to 0.3614809303671764\n",
      "Starting trial 252\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 405us/sample - loss: 808.4278\n",
      "Epsilon set to 0.3596735257153405\n",
      "Starting trial 253\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 409us/sample - loss: 405.1256\n",
      "Epsilon set to 0.3578751580867638\n",
      "Starting trial 254\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 385us/sample - loss: 776.1919\n",
      "Epsilon set to 0.35608578229633\n",
      "Starting trial 255\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 414us/sample - loss: 885.4852\n",
      "Epsilon set to 0.3543053533848483\n",
      "Starting trial 256\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 407us/sample - loss: 552.3499\n",
      "Epsilon set to 0.35253382661792404\n",
      "Starting trial 257\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 389us/sample - loss: 925.9304\n",
      "Epsilon set to 0.3507711574848344\n",
      "Starting trial 258\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 394us/sample - loss: 659.7507\n",
      "Epsilon set to 0.34901730169741024\n",
      "Starting trial 259\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 398us/sample - loss: 698.4024\n",
      "Epsilon set to 0.3472722151889232\n",
      "Starting trial 260\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 401us/sample - loss: 558.6850\n",
      "Epsilon set to 0.3455358541129786\n",
      "Starting trial 261\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 405us/sample - loss: 853.9503\n",
      "Epsilon set to 0.3438081748424137\n",
      "Starting trial 262\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 404us/sample - loss: 768.9719\n",
      "Epsilon set to 0.3420891339682016\n",
      "Starting trial 263\n",
      "Train on 500 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 0s 390us/sample - loss: 638.8549\n",
      "Epsilon set to 0.3403786882983606\n",
      "Starting trial 264\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 397us/sample - loss: 520.4453\n",
      "Epsilon set to 0.3386767948568688\n",
      "Starting trial 265\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 406us/sample - loss: 730.4677\n",
      "Epsilon set to 0.33698341088258443\n",
      "Starting trial 266\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 410us/sample - loss: 465.6542\n",
      "Epsilon set to 0.3352984938281715\n",
      "Starting trial 267\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 387us/sample - loss: 428.5059\n",
      "Epsilon set to 0.33362200135903064\n",
      "Starting trial 268\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 393us/sample - loss: 612.2673\n",
      "Epsilon set to 0.33195389135223546\n",
      "Starting trial 269\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 399us/sample - loss: 411.5083\n",
      "Epsilon set to 0.3302941218954743\n",
      "Starting trial 270\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 413us/sample - loss: 412.6297\n",
      "Epsilon set to 0.32864265128599696\n",
      "Starting trial 271\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 397us/sample - loss: 692.6823\n",
      "Epsilon set to 0.326999438029567\n",
      "Starting trial 272\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 408us/sample - loss: 297.8712\n",
      "Epsilon set to 0.3253644408394192\n",
      "Starting trial 273\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 442us/sample - loss: 946.7168\n",
      "Epsilon set to 0.3237376186352221\n",
      "Starting trial 274\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 401us/sample - loss: 325.6626\n",
      "Epsilon set to 0.322118930542046\n",
      "Starting trial 275\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 401us/sample - loss: 901.4670\n",
      "Epsilon set to 0.32050833588933575\n",
      "Starting trial 276\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 384us/sample - loss: 547.9219\n",
      "Epsilon set to 0.31890579420988907\n",
      "Starting trial 277\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 391us/sample - loss: 389.9758\n",
      "Epsilon set to 0.3173112652388396\n",
      "Starting trial 278\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 411us/sample - loss: 493.3898\n",
      "Epsilon set to 0.3157247089126454\n",
      "Starting trial 279\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 397us/sample - loss: 235.1899\n",
      "Epsilon set to 0.3141460853680822\n",
      "Starting trial 280\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 393us/sample - loss: 450.7957\n",
      "Epsilon set to 0.3125753549412418\n",
      "Starting trial 281\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 401us/sample - loss: 306.2523\n",
      "Epsilon set to 0.31101247816653554\n",
      "Starting trial 282\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 395us/sample - loss: 423.1955\n",
      "Epsilon set to 0.30945741577570285\n",
      "Starting trial 283\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 419us/sample - loss: 454.8002\n",
      "Epsilon set to 0.3079101286968243\n",
      "Starting trial 284\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 406us/sample - loss: 395.5705\n",
      "Epsilon set to 0.3063705780533402\n",
      "Starting trial 285\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 422us/sample - loss: 403.2315\n",
      "Epsilon set to 0.30483872516307353\n",
      "Starting trial 286\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 391us/sample - loss: 175.5622\n",
      "Epsilon set to 0.3033145315372582\n",
      "Starting trial 287\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 418us/sample - loss: 661.4171\n",
      "Epsilon set to 0.3017979588795719\n",
      "Starting trial 288\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 417us/sample - loss: 430.6135\n",
      "Epsilon set to 0.30028896908517405\n",
      "Starting trial 289\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 417us/sample - loss: 504.2647\n",
      "Epsilon set to 0.2987875242397482\n",
      "Starting trial 290\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 383us/sample - loss: 612.2115\n",
      "Epsilon set to 0.29729358661854943\n",
      "Starting trial 291\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 414us/sample - loss: 674.6325\n",
      "Epsilon set to 0.29580711868545667\n",
      "Starting trial 292\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 410us/sample - loss: 526.8961\n",
      "Epsilon set to 0.2943280830920294\n",
      "Starting trial 293\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 405us/sample - loss: 472.3494\n",
      "Epsilon set to 0.29285644267656924\n",
      "Starting trial 294\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 402us/sample - loss: 385.1990\n",
      "Epsilon set to 0.2913921604631864\n",
      "Starting trial 295\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 395us/sample - loss: 480.4822\n",
      "Epsilon set to 0.28993519966087045\n",
      "Starting trial 296\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 405us/sample - loss: 434.7230\n",
      "Epsilon set to 0.2884855236625661\n",
      "Starting trial 297\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 396us/sample - loss: 431.5083\n",
      "Epsilon set to 0.28704309604425327\n",
      "Starting trial 298\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 406us/sample - loss: 303.6500\n",
      "Epsilon set to 0.285607880564032\n",
      "Starting trial 299\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 385us/sample - loss: 298.6651\n",
      "Epsilon set to 0.28417984116121187\n",
      "Starting trial 300\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 414us/sample - loss: 443.6554\n",
      "Epsilon set to 0.2827589419554058\n",
      "Starting trial 301\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 376us/sample - loss: 373.9310\n",
      "Epsilon set to 0.28134514724562876\n",
      "Starting trial 302\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 397us/sample - loss: 256.7422\n",
      "Epsilon set to 0.2799384215094006\n",
      "Starting trial 303\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 405us/sample - loss: 303.4492\n",
      "Epsilon set to 0.27853872940185365\n",
      "Starting trial 304\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 408us/sample - loss: 120.9110\n",
      "Epsilon set to 0.27714603575484437\n",
      "Starting trial 305\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 413us/sample - loss: 126.2017\n",
      "Epsilon set to 0.2757603055760701\n",
      "Starting trial 306\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 407us/sample - loss: 101.2630\n",
      "Epsilon set to 0.2743815040481898\n",
      "Starting trial 307\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 393us/sample - loss: 181.7661\n",
      "Epsilon set to 0.2730095965279488\n",
      "Starting trial 308\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 389us/sample - loss: 140.5514\n",
      "Epsilon set to 0.27164454854530906\n",
      "Starting trial 309\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 417us/sample - loss: 55.3304\n",
      "Epsilon set to 0.2702863258025825\n",
      "Starting trial 310\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 416us/sample - loss: 65.7178\n",
      "Epsilon set to 0.2689348941735696\n",
      "Starting trial 311\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 411us/sample - loss: 68.1771\n",
      "Epsilon set to 0.26759021970270175\n",
      "Starting trial 312\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 401us/sample - loss: 43.1557\n",
      "Epsilon set to 0.2662522686041882\n",
      "Starting trial 313\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 411us/sample - loss: 48.6358\n",
      "Epsilon set to 0.2649210072611673\n",
      "Starting trial 314\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 405us/sample - loss: 43.1645\n",
      "Epsilon set to 0.26359640222486147\n",
      "Starting trial 315\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 393us/sample - loss: 44.5071\n",
      "Epsilon set to 0.26227842021373715\n",
      "Starting trial 316\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 432us/sample - loss: 70.2582\n",
      "Epsilon set to 0.2609670281126685\n",
      "Starting trial 317\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 402us/sample - loss: 57.7003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon set to 0.25966219297210513\n",
      "Starting trial 318\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 397us/sample - loss: 26.0252\n",
      "Epsilon set to 0.2583638820072446\n",
      "Starting trial 319\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 409us/sample - loss: 50.8645\n",
      "Epsilon set to 0.2570720625972084\n",
      "Starting trial 320\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 410us/sample - loss: 38.9359\n",
      "Epsilon set to 0.25578670228422234\n",
      "Starting trial 321\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 398us/sample - loss: 59.4929\n",
      "Epsilon set to 0.25450776877280124\n",
      "Starting trial 322\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 425us/sample - loss: 55.8180\n",
      "Epsilon set to 0.2532352299289372\n",
      "Starting trial 323\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 383us/sample - loss: 52.1021\n",
      "Epsilon set to 0.2519690537792925\n",
      "Starting trial 324\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 386us/sample - loss: 64.3854\n",
      "Epsilon set to 0.2507092085103961\n",
      "Starting trial 325\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 391us/sample - loss: 33.2592\n",
      "Epsilon set to 0.2494556624678441\n",
      "Starting trial 326\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 480us/sample - loss: 42.1566\n",
      "Epsilon set to 0.24820838415550486\n",
      "Starting trial 327\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 398us/sample - loss: 27.5732\n",
      "Epsilon set to 0.24696734223472733\n",
      "Starting trial 328\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 402us/sample - loss: 45.3793\n",
      "Epsilon set to 0.2457325055235537\n",
      "Starting trial 329\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 397us/sample - loss: 61.2737\n",
      "Epsilon set to 0.24450384299593592\n",
      "Starting trial 330\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 417us/sample - loss: 78.1763\n",
      "Epsilon set to 0.24328132378095624\n",
      "Starting trial 331\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 420us/sample - loss: 39.9414\n",
      "Epsilon set to 0.24206491716205145\n",
      "Starting trial 332\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 401us/sample - loss: 22.2410\n",
      "Epsilon set to 0.2408545925762412\n",
      "Starting trial 333\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 437us/sample - loss: 24.5735\n",
      "Epsilon set to 0.23965031961336\n",
      "Starting trial 334\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 408us/sample - loss: 30.4519\n",
      "Epsilon set to 0.2384520680152932\n",
      "Starting trial 335\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 399us/sample - loss: 45.4244\n",
      "Epsilon set to 0.23725980767521673\n",
      "Starting trial 336\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 387us/sample - loss: 53.0874\n",
      "Epsilon set to 0.23607350863684065\n",
      "Starting trial 337\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 412us/sample - loss: 16.6949\n",
      "Epsilon set to 0.23489314109365644\n",
      "Starting trial 338\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 391us/sample - loss: 20.6497\n",
      "Epsilon set to 0.23371867538818816\n",
      "Starting trial 339\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 387us/sample - loss: 31.6900\n",
      "Epsilon set to 0.23255008201124722\n",
      "Starting trial 340\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 399us/sample - loss: 21.8551\n",
      "Epsilon set to 0.231387331601191\n",
      "Starting trial 341\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 395us/sample - loss: 24.0467\n",
      "Epsilon set to 0.23023039494318503\n",
      "Starting trial 342\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 397us/sample - loss: 86.4049\n",
      "Epsilon set to 0.2290792429684691\n",
      "Starting trial 343\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 410us/sample - loss: 81.0835\n",
      "Epsilon set to 0.22793384675362674\n",
      "Starting trial 344\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 411us/sample - loss: 53.1628\n",
      "Epsilon set to 0.22679417751985861\n",
      "Starting trial 345\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 393us/sample - loss: 32.2516\n",
      "Epsilon set to 0.22566020663225933\n",
      "Starting trial 346\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 412us/sample - loss: 24.0754\n",
      "Epsilon set to 0.22453190559909803\n",
      "Starting trial 347\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 410us/sample - loss: 35.9529\n",
      "Epsilon set to 0.22340924607110255\n",
      "Starting trial 348\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 397us/sample - loss: 29.8626\n",
      "Epsilon set to 0.22229219984074702\n",
      "Starting trial 349\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 405us/sample - loss: 16.3679\n",
      "Epsilon set to 0.2211807388415433\n",
      "Starting trial 350\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 398us/sample - loss: 17.8836\n",
      "Epsilon set to 0.22007483514733558\n",
      "Starting trial 351\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 403us/sample - loss: 14.9609\n",
      "Epsilon set to 0.2189744609715989\n",
      "Starting trial 352\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 425us/sample - loss: 15.1299\n",
      "Epsilon set to 0.2178795886667409\n",
      "Starting trial 353\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 408us/sample - loss: 38.6379\n",
      "Epsilon set to 0.2167901907234072\n",
      "Starting trial 354\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 405us/sample - loss: 23.2544\n",
      "Epsilon set to 0.21570623976979014\n",
      "Starting trial 355\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 403us/sample - loss: 32.6281\n",
      "Epsilon set to 0.21462770857094118\n",
      "Starting trial 356\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 418us/sample - loss: 43.7109\n",
      "Epsilon set to 0.21355457002808648\n",
      "Starting trial 357\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 397us/sample - loss: 55.9123\n",
      "Epsilon set to 0.21248679717794605\n",
      "Starting trial 358\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 406us/sample - loss: 37.4174\n",
      "Epsilon set to 0.21142436319205632\n",
      "Starting trial 359\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 400us/sample - loss: 45.8603\n",
      "Epsilon set to 0.21036724137609603\n",
      "Starting trial 360\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 399us/sample - loss: 30.0856\n",
      "Epsilon set to 0.20931540516921554\n",
      "Starting trial 361\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 421us/sample - loss: 24.1468\n",
      "Epsilon set to 0.20826882814336947\n",
      "Starting trial 362\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 420us/sample - loss: 34.0484\n",
      "Epsilon set to 0.20722748400265262\n",
      "Starting trial 363\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 384us/sample - loss: 48.7156\n",
      "Epsilon set to 0.20619134658263935\n",
      "Starting trial 364\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 411us/sample - loss: 66.4053\n",
      "Epsilon set to 0.20516038984972615\n",
      "Starting trial 365\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 381us/sample - loss: 68.6412\n",
      "Epsilon set to 0.2041345879004775\n",
      "Starting trial 366\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 404us/sample - loss: 83.7435\n",
      "Epsilon set to 0.2031139149609751\n",
      "Starting trial 367\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 421us/sample - loss: 64.0157\n",
      "Epsilon set to 0.20209834538617025\n",
      "Starting trial 368\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 409us/sample - loss: 98.7345\n",
      "Epsilon set to 0.2010878536592394\n",
      "Starting trial 369\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 390us/sample - loss: 55.6222\n",
      "Epsilon set to 0.2000824143909432\n",
      "Starting trial 370\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 391us/sample - loss: 95.2883\n",
      "Epsilon set to 0.19908200231898848\n",
      "Starting trial 371\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 387us/sample - loss: 55.3639\n",
      "Epsilon set to 0.19808659230739353\n",
      "Starting trial 372\n",
      "Train on 500 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 0s 402us/sample - loss: 107.5673\n",
      "Epsilon set to 0.19709615934585656\n",
      "Starting trial 373\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 391us/sample - loss: 280.9351\n",
      "Epsilon set to 0.19611067854912728\n",
      "Starting trial 374\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 393us/sample - loss: 1439.5301\n",
      "Epsilon set to 0.19513012515638165\n",
      "Starting trial 375\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 422us/sample - loss: 3623.4075\n",
      "Epsilon set to 0.19415447453059972\n",
      "Starting trial 376\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 396us/sample - loss: 12045.2853\n",
      "Epsilon set to 0.19318370215794672\n",
      "Starting trial 377\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 411us/sample - loss: 18923.2219\n",
      "Epsilon set to 0.192217783647157\n",
      "Starting trial 378\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 443us/sample - loss: 11604.7339\n",
      "Epsilon set to 0.1912566947289212\n",
      "Starting trial 379\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 412us/sample - loss: 21297.7010\n",
      "Epsilon set to 0.1903004112552766\n",
      "Starting trial 380\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 392us/sample - loss: 15114.3496\n",
      "Epsilon set to 0.18934890919900021\n",
      "Starting trial 381\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 429us/sample - loss: 13530.3986\n",
      "Epsilon set to 0.18840216465300522\n",
      "Starting trial 382\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 396us/sample - loss: 34542.4726\n",
      "Epsilon set to 0.18746015382974018\n",
      "Starting trial 383\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 393us/sample - loss: 51099.1910\n",
      "Epsilon set to 0.1865228530605915\n",
      "Starting trial 384\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 415us/sample - loss: 17315.0762\n",
      "Epsilon set to 0.18559023879528855\n",
      "Starting trial 385\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 427us/sample - loss: 26109.0240\n",
      "Epsilon set to 0.1846622876013121\n",
      "Starting trial 386\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 406us/sample - loss: 3521.9191\n",
      "Epsilon set to 0.18373897616330553\n",
      "Starting trial 387\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 407us/sample - loss: 24286.8523\n",
      "Epsilon set to 0.182820281282489\n",
      "Starting trial 388\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 383us/sample - loss: 29652.2163\n",
      "Epsilon set to 0.18190617987607657\n",
      "Starting trial 389\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 396us/sample - loss: 6592.6368\n",
      "Epsilon set to 0.18099664897669618\n",
      "Starting trial 390\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 401us/sample - loss: 19171.4934\n",
      "Epsilon set to 0.1800916657318127\n",
      "Starting trial 391\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 401us/sample - loss: 9764.8681\n",
      "Epsilon set to 0.17919120740315364\n",
      "Starting trial 392\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 408us/sample - loss: 4122.9588\n",
      "Epsilon set to 0.17829525136613786\n",
      "Starting trial 393\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 403us/sample - loss: 6206.4439\n",
      "Epsilon set to 0.17740377510930716\n",
      "Starting trial 394\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 399us/sample - loss: 1912.6199\n",
      "Epsilon set to 0.17651675623376062\n",
      "Starting trial 395\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 411us/sample - loss: 1656.8174\n",
      "Epsilon set to 0.1756341724525918\n",
      "Starting trial 396\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 411us/sample - loss: 1219.0343\n",
      "Epsilon set to 0.17475600159032884\n",
      "Starting trial 397\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 395us/sample - loss: 1836.3491\n",
      "Epsilon set to 0.17388222158237718\n",
      "Starting trial 398\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 409us/sample - loss: 1778.6771\n",
      "Epsilon set to 0.1730128104744653\n",
      "Starting trial 399\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 413us/sample - loss: 438.7457\n",
      "Epsilon set to 0.17214774642209296\n",
      "Starting trial 400\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 409us/sample - loss: 526.6817\n",
      "Epsilon set to 0.1712870076899825\n",
      "Starting trial 401\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 405us/sample - loss: 219.2866\n",
      "Epsilon set to 0.17043057265153258\n",
      "Starting trial 402\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 391us/sample - loss: 70.1211\n",
      "Epsilon set to 0.16957841978827493\n",
      "Starting trial 403\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 407us/sample - loss: 219.0311\n",
      "Epsilon set to 0.16873052768933355\n",
      "Starting trial 404\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 383us/sample - loss: 134.1841\n",
      "Epsilon set to 0.1678868750508869\n",
      "Starting trial 405\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 411us/sample - loss: 85.6576\n",
      "Epsilon set to 0.16704744067563246\n",
      "Starting trial 406\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 409us/sample - loss: 117.3597\n",
      "Epsilon set to 0.1662122034722543\n",
      "Starting trial 407\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 387us/sample - loss: 193.7699\n",
      "Epsilon set to 0.16538114245489302\n",
      "Starting trial 408\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 405us/sample - loss: 328.1537\n",
      "Epsilon set to 0.16455423674261854\n",
      "Starting trial 409\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 406us/sample - loss: 205.0337\n",
      "Epsilon set to 0.16373146555890544\n",
      "Starting trial 410\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 411us/sample - loss: 273.6901\n",
      "Epsilon set to 0.16291280823111093\n",
      "Starting trial 411\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 416us/sample - loss: 396.0092\n",
      "Epsilon set to 0.16209824418995536\n",
      "Starting trial 412\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 379us/sample - loss: 295.8945\n",
      "Epsilon set to 0.16128775296900558\n",
      "Starting trial 413\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 401us/sample - loss: 327.9057\n",
      "Epsilon set to 0.16048131420416054\n",
      "Starting trial 414\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 395us/sample - loss: 289.2837\n",
      "Epsilon set to 0.15967890763313974\n",
      "Starting trial 415\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 399us/sample - loss: 3504.7182\n",
      "Epsilon set to 0.15888051309497406\n",
      "Starting trial 416\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 413us/sample - loss: 4741.4796\n",
      "Epsilon set to 0.1580861105294992\n",
      "Starting trial 417\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 431us/sample - loss: 86481.9487\n",
      "Epsilon set to 0.1572956799768517\n",
      "Starting trial 418\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 411us/sample - loss: 57132.8426\n",
      "Epsilon set to 0.15650920157696743\n",
      "Starting trial 419\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 409us/sample - loss: 17043.3890\n",
      "Epsilon set to 0.1557266555690826\n",
      "Starting trial 420\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 402us/sample - loss: 73372.3397\n",
      "Epsilon set to 0.1549480222912372\n",
      "Starting trial 421\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 413us/sample - loss: 27332.3831\n",
      "Epsilon set to 0.15417328217978102\n",
      "Starting trial 422\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 417us/sample - loss: 134613.9915\n",
      "Epsilon set to 0.1534024157688821\n",
      "Starting trial 423\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 401us/sample - loss: 21705.5334\n",
      "Epsilon set to 0.1526354036900377\n",
      "Starting trial 424\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 403us/sample - loss: 6876.7201\n",
      "Epsilon set to 0.1518722266715875\n",
      "Starting trial 425\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 389us/sample - loss: 5685.5318\n",
      "Epsilon set to 0.15111286553822956\n",
      "Starting trial 426\n",
      "Train on 500 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 0s 399us/sample - loss: 23967.1030\n",
      "Epsilon set to 0.15035730121053842\n",
      "Starting trial 427\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - ETA: 0s - loss: 71263.0211 - 0s 387us/sample - loss: 171079.0721\n",
      "Epsilon set to 0.14960551470448571\n",
      "Starting trial 428\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 401us/sample - loss: 63557.6193\n",
      "Epsilon set to 0.14885748713096328\n",
      "Starting trial 429\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 397us/sample - loss: 11876.8326\n",
      "Epsilon set to 0.14811319969530845\n",
      "Starting trial 430\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 407us/sample - loss: 20331.8977\n",
      "Epsilon set to 0.1473726336968319\n",
      "Starting trial 431\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 413us/sample - loss: 19918.9958\n",
      "Epsilon set to 0.14663577052834775\n",
      "Starting trial 432\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 410us/sample - loss: 36506.7399\n",
      "Epsilon set to 0.14590259167570602\n",
      "Starting trial 433\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 423us/sample - loss: 15166.5142\n",
      "Epsilon set to 0.1451730787173275\n",
      "Starting trial 434\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 397us/sample - loss: 195386.9991\n",
      "Epsilon set to 0.14444721332374086\n",
      "Starting trial 435\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 390us/sample - loss: 125098.9052\n",
      "Epsilon set to 0.14372497725712216\n",
      "Starting trial 436\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 389us/sample - loss: 308562.2906\n",
      "Epsilon set to 0.14300635237083656\n",
      "Starting trial 437\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 383us/sample - loss: 36781.7401\n",
      "Epsilon set to 0.14229132060898236\n",
      "Starting trial 438\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 401us/sample - loss: 525143.2092\n",
      "Epsilon set to 0.14157986400593744\n",
      "Starting trial 439\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 408us/sample - loss: 332254.0645\n",
      "Epsilon set to 0.14087196468590776\n",
      "Starting trial 440\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 389us/sample - loss: 399126.6570\n",
      "Epsilon set to 0.14016760486247823\n",
      "Starting trial 441\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 391us/sample - loss: 713726.0514\n",
      "Epsilon set to 0.13946676683816583\n",
      "Starting trial 442\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 395us/sample - loss: 1408069.7945\n",
      "Epsilon set to 0.138769433003975\n",
      "Starting trial 443\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 393us/sample - loss: 1261177.9470\n",
      "Epsilon set to 0.13807558583895513\n",
      "Starting trial 444\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 401us/sample - loss: 1660813.4585\n",
      "Epsilon set to 0.13738520790976036\n",
      "Starting trial 445\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 411us/sample - loss: 1090121.1652\n",
      "Epsilon set to 0.13669828187021155\n",
      "Starting trial 446\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 386us/sample - loss: 1006309.7196\n",
      "Epsilon set to 0.13601479046086049\n",
      "Starting trial 447\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 387us/sample - loss: 877951.1192\n",
      "Epsilon set to 0.1353347165085562\n",
      "Starting trial 448\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 403us/sample - loss: 313877.7743\n",
      "Epsilon set to 0.1346580429260134\n",
      "Starting trial 449\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 408us/sample - loss: 498416.0952\n",
      "Epsilon set to 0.13398475271138335\n",
      "Starting trial 450\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 396us/sample - loss: 98243.0385\n",
      "Epsilon set to 0.13331482894782642\n",
      "Starting trial 451\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 415us/sample - loss: 157918.0443\n",
      "Epsilon set to 0.13264825480308728\n",
      "Starting trial 452\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 409us/sample - loss: 293117.0002\n",
      "Epsilon set to 0.13198501352907185\n",
      "Starting trial 453\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 400us/sample - loss: 128339.5411\n",
      "Epsilon set to 0.1313250884614265\n",
      "Starting trial 454\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 405us/sample - loss: 255127.0898\n",
      "Epsilon set to 0.13066846301911936\n",
      "Starting trial 455\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 415us/sample - loss: 95177.6546\n",
      "Epsilon set to 0.13001512070402377\n",
      "Starting trial 456\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 408us/sample - loss: 74840.1216\n",
      "Epsilon set to 0.12936504510050365\n",
      "Starting trial 457\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 403us/sample - loss: 30257.7768\n",
      "Epsilon set to 0.12871821987500112\n",
      "Starting trial 458\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 423us/sample - loss: 25304.6879\n",
      "Epsilon set to 0.12807462877562611\n",
      "Starting trial 459\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 407us/sample - loss: 39071.4815\n",
      "Epsilon set to 0.12743425563174798\n",
      "Starting trial 460\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 415us/sample - loss: 53898.0343\n",
      "Epsilon set to 0.12679708435358925\n",
      "Starting trial 461\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 413us/sample - loss: 45260.8311\n",
      "Epsilon set to 0.1261630989318213\n",
      "Starting trial 462\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 412us/sample - loss: 15011.5397\n",
      "Epsilon set to 0.1255322834371622\n",
      "Starting trial 463\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 390us/sample - loss: 62386.3660\n",
      "Epsilon set to 0.12490462201997637\n",
      "Starting trial 464\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 393us/sample - loss: 51276.7135\n",
      "Epsilon set to 0.1242800989098765\n",
      "Starting trial 465\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 391us/sample - loss: 40037.3728\n",
      "Epsilon set to 0.12365869841532712\n",
      "Starting trial 466\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 393us/sample - loss: 18706.9412\n",
      "Epsilon set to 0.12304040492325048\n",
      "Starting trial 467\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 411us/sample - loss: 9909.1865\n",
      "Epsilon set to 0.12242520289863423\n",
      "Starting trial 468\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 403us/sample - loss: 4395.9165\n",
      "Epsilon set to 0.12181307688414106\n",
      "Starting trial 469\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 396us/sample - loss: 4828.0103\n",
      "Epsilon set to 0.12120401149972035\n",
      "Starting trial 470\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 409us/sample - loss: 3315.0018\n",
      "Epsilon set to 0.12059799144222175\n",
      "Starting trial 471\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 407us/sample - loss: 3712.0801\n",
      "Epsilon set to 0.11999500148501063\n",
      "Starting trial 472\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 409us/sample - loss: 5592.9727\n",
      "Epsilon set to 0.11939502647758558\n",
      "Starting trial 473\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 410us/sample - loss: 5875.5597\n",
      "Epsilon set to 0.11879805134519765\n",
      "Starting trial 474\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 405us/sample - loss: 5931.0699\n",
      "Epsilon set to 0.11820406108847166\n",
      "Starting trial 475\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - ETA: 0s - loss: 5289.70 - 0s 399us/sample - loss: 5230.7339\n",
      "Epsilon set to 0.1176130407830293\n",
      "Starting trial 476\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 399us/sample - loss: 5316.5242\n",
      "Epsilon set to 0.11702497557911415\n",
      "Starting trial 477\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 421us/sample - loss: 3186.6011\n",
      "Epsilon set to 0.11643985070121858\n",
      "Starting trial 478\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 418us/sample - loss: 6179.9377\n",
      "Epsilon set to 0.11585765144771248\n",
      "Starting trial 479\n",
      "Train on 500 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 0s 396us/sample - loss: 3457.2096\n",
      "Epsilon set to 0.11527836319047392\n",
      "Starting trial 480\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 409us/sample - loss: 4565.4798\n",
      "Epsilon set to 0.11470197137452155\n",
      "Starting trial 481\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 433us/sample - loss: 5498.9711\n",
      "Epsilon set to 0.11412846151764894\n",
      "Starting trial 482\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 390us/sample - loss: 4492.9267\n",
      "Epsilon set to 0.1135578192100607\n",
      "Starting trial 483\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 409us/sample - loss: 4373.8965\n",
      "Epsilon set to 0.11299003011401039\n",
      "Starting trial 484\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 425us/sample - loss: 1658.3357\n",
      "Epsilon set to 0.11242507996344034\n",
      "Starting trial 485\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 397us/sample - loss: 2781.7101\n",
      "Epsilon set to 0.11186295456362313\n",
      "Starting trial 486\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 401us/sample - loss: 3086.9777\n",
      "Epsilon set to 0.11130363979080501\n",
      "Starting trial 487\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 401us/sample - loss: 2218.9895\n",
      "Epsilon set to 0.11074712159185099\n",
      "Starting trial 488\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 394us/sample - loss: 3803.3395\n",
      "Epsilon set to 0.11019338598389174\n",
      "Starting trial 489\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 411us/sample - loss: 1829.4145\n",
      "Epsilon set to 0.10964241905397228\n",
      "Starting trial 490\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 425us/sample - loss: 3128.4849\n",
      "Epsilon set to 0.10909420695870241\n",
      "Starting trial 491\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 409us/sample - loss: 2734.6233\n",
      "Epsilon set to 0.1085487359239089\n",
      "Starting trial 492\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 388us/sample - loss: 3143.8460\n",
      "Epsilon set to 0.10800599224428936\n",
      "Starting trial 493\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 425us/sample - loss: 1502.6446\n",
      "Epsilon set to 0.10746596228306791\n",
      "Starting trial 494\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 401us/sample - loss: 1413.7310\n",
      "Epsilon set to 0.10692863247165257\n",
      "Starting trial 495\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 395us/sample - loss: 2172.7618\n",
      "Epsilon set to 0.1063939893092943\n",
      "Starting trial 496\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 397us/sample - loss: 1668.4415\n",
      "Epsilon set to 0.10586201936274783\n",
      "Starting trial 497\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 405us/sample - loss: 2869.8681\n",
      "Epsilon set to 0.10533270926593409\n",
      "Starting trial 498\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 387us/sample - loss: 1043.7025\n",
      "Epsilon set to 0.10480604571960442\n",
      "Starting trial 499\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 397us/sample - loss: 1536.9925\n",
      "Epsilon set to 0.1042820154910064\n",
      "Starting trial 500\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 407us/sample - loss: 2663.3529\n",
      "Epsilon set to 0.10376060541355137\n",
      "Starting trial 501\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 400us/sample - loss: 2220.0956\n",
      "Epsilon set to 0.1032418023864836\n",
      "Starting trial 502\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 418us/sample - loss: 1830.0612\n",
      "Epsilon set to 0.10272559337455119\n",
      "Starting trial 503\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 395us/sample - loss: 757.5410\n",
      "Epsilon set to 0.10221196540767843\n",
      "Starting trial 504\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 399us/sample - loss: 2318.4947\n",
      "Epsilon set to 0.10170090558064004\n",
      "Starting trial 505\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 396us/sample - loss: 2700.1565\n",
      "Epsilon set to 0.10119240105273684\n",
      "Starting trial 506\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 387us/sample - loss: 2157.5522\n",
      "Epsilon set to 0.10068643904747315\n",
      "Starting trial 507\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 409us/sample - loss: 1595.9818\n",
      "Epsilon set to 0.10018300685223579\n",
      "Starting trial 508\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 393us/sample - loss: 2141.4719\n",
      "Epsilon set to 0.0996820918179746\n",
      "Starting trial 509\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 403us/sample - loss: 1390.9423\n",
      "Epsilon set to 0.09918368135888474\n",
      "Starting trial 510\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 413us/sample - loss: 2133.3946\n",
      "Epsilon set to 0.09868776295209031\n",
      "Starting trial 511\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 418us/sample - loss: 1451.6295\n",
      "Epsilon set to 0.09819432413732986\n",
      "Starting trial 512\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 417us/sample - loss: 975.3228\n",
      "Epsilon set to 0.09770335251664321\n",
      "Starting trial 513\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 399us/sample - loss: 3170.5663\n",
      "Epsilon set to 0.09721483575406\n",
      "Starting trial 514\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 410us/sample - loss: 1514.5834\n",
      "Epsilon set to 0.09672876157528969\n",
      "Starting trial 515\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 398us/sample - loss: 1177.7675\n",
      "Epsilon set to 0.09624511776741324\n",
      "Starting trial 516\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 409us/sample - loss: 1343.1645\n",
      "Epsilon set to 0.09576389217857617\n",
      "Starting trial 517\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 405us/sample - loss: 1511.9936\n",
      "Epsilon set to 0.09528507271768329\n",
      "Starting trial 518\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 402us/sample - loss: 1134.2795\n",
      "Epsilon set to 0.09480864735409487\n",
      "Starting trial 519\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 396us/sample - loss: 1324.9070\n",
      "Epsilon set to 0.0943346041173244\n",
      "Starting trial 520\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 403us/sample - loss: 2294.8964\n",
      "Epsilon set to 0.09386293109673778\n",
      "Starting trial 521\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 403us/sample - loss: 1702.4661\n",
      "Epsilon set to 0.09339361644125409\n",
      "Starting trial 522\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 406us/sample - loss: 1020.4878\n",
      "Epsilon set to 0.09292664835904782\n",
      "Starting trial 523\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 397us/sample - loss: 2911.8698\n",
      "Epsilon set to 0.09246201511725258\n",
      "Starting trial 524\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 389us/sample - loss: 1509.2314\n",
      "Epsilon set to 0.09199970504166631\n",
      "Starting trial 525\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 390us/sample - loss: 2346.5143\n",
      "Epsilon set to 0.09153970651645797\n",
      "Starting trial 526\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 425us/sample - loss: 1247.1007\n",
      "Epsilon set to 0.09108200798387568\n",
      "Starting trial 527\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 439us/sample - loss: 767.7592\n",
      "Epsilon set to 0.0906265979439563\n",
      "Starting trial 528\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 405us/sample - loss: 1390.7883\n",
      "Epsilon set to 0.09017346495423652\n",
      "Starting trial 529\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 411us/sample - loss: 1320.5426\n",
      "Epsilon set to 0.08972259762946533\n",
      "Starting trial 530\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 407us/sample - loss: 1153.5409\n",
      "Epsilon set to 0.089273984641318\n",
      "Starting trial 531\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 412us/sample - loss: 1794.2308\n",
      "Epsilon set to 0.0888276147181114\n",
      "Starting trial 532\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 389us/sample - loss: 2348.6190\n",
      "Epsilon set to 0.08838347664452084\n",
      "Starting trial 533\n",
      "Train on 500 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 0s 381us/sample - loss: 2752.3142\n",
      "Epsilon set to 0.08794155926129824\n",
      "Starting trial 534\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 392us/sample - loss: 2127.5139\n",
      "Epsilon set to 0.08750185146499175\n",
      "Starting trial 535\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 404us/sample - loss: 2843.9904\n",
      "Epsilon set to 0.08706434220766679\n",
      "Starting trial 536\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 401us/sample - loss: 2463.5068\n",
      "Epsilon set to 0.08662902049662846\n",
      "Starting trial 537\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 401us/sample - loss: 1989.4806\n",
      "Epsilon set to 0.08619587539414532\n",
      "Starting trial 538\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 402us/sample - loss: 1265.9852\n",
      "Epsilon set to 0.08576489601717459\n",
      "Starting trial 539\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 407us/sample - loss: 1397.7040\n",
      "Epsilon set to 0.08533607153708872\n",
      "Starting trial 540\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 395us/sample - loss: 1764.0156\n",
      "Epsilon set to 0.08490939117940327\n",
      "Starting trial 541\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 404us/sample - loss: 2506.4992\n",
      "Epsilon set to 0.08448484422350626\n",
      "Starting trial 542\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 409us/sample - loss: 801.3342\n",
      "Epsilon set to 0.08406242000238873\n",
      "Starting trial 543\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 391us/sample - loss: 1003.2458\n",
      "Epsilon set to 0.08364210790237678\n",
      "Starting trial 544\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 414us/sample - loss: 917.5220\n",
      "Epsilon set to 0.0832238973628649\n",
      "Starting trial 545\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 398us/sample - loss: 1410.0628\n",
      "Epsilon set to 0.08280777787605056\n",
      "Starting trial 546\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 406us/sample - loss: 1036.2846\n",
      "Epsilon set to 0.08239373898667031\n",
      "Starting trial 547\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 395us/sample - loss: 1225.6218\n",
      "Epsilon set to 0.08198177029173696\n",
      "Starting trial 548\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 402us/sample - loss: 1330.1443\n",
      "Epsilon set to 0.08157186144027828\n",
      "Starting trial 549\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 379us/sample - loss: 705.2878\n",
      "Epsilon set to 0.0811640021330769\n",
      "Starting trial 550\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 412us/sample - loss: 590.6828\n",
      "Epsilon set to 0.08075818212241151\n",
      "Starting trial 551\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 399us/sample - loss: 427.5540\n",
      "Epsilon set to 0.08035439121179945\n",
      "Starting trial 552\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 413us/sample - loss: 361.9223\n",
      "Epsilon set to 0.07995261925574046\n",
      "Starting trial 553\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 393us/sample - loss: 571.9501\n",
      "Epsilon set to 0.07955285615946175\n",
      "Starting trial 554\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 423us/sample - loss: 707.0036\n",
      "Epsilon set to 0.07915509187866444\n",
      "Starting trial 555\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 392us/sample - loss: 587.8274\n",
      "Epsilon set to 0.07875931641927113\n",
      "Starting trial 556\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 422us/sample - loss: 895.8472\n",
      "Epsilon set to 0.07836551983717477\n",
      "Starting trial 557\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 389us/sample - loss: 823.1619\n",
      "Epsilon set to 0.07797369223798889\n",
      "Starting trial 558\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 383us/sample - loss: 464.6123\n",
      "Epsilon set to 0.07758382377679894\n",
      "Starting trial 559\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 418us/sample - loss: 802.1856\n",
      "Epsilon set to 0.07719590465791494\n",
      "Starting trial 560\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 413us/sample - loss: 535.9795\n",
      "Epsilon set to 0.07680992513462537\n",
      "Starting trial 561\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 413us/sample - loss: 644.7601\n",
      "Epsilon set to 0.07642587550895225\n",
      "Starting trial 562\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 407us/sample - loss: 453.7777\n",
      "Epsilon set to 0.07604374613140748\n",
      "Starting trial 563\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 405us/sample - loss: 634.9990\n",
      "Epsilon set to 0.07566352740075044\n",
      "Starting trial 564\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 411us/sample - loss: 974.8425\n",
      "Epsilon set to 0.07528520976374668\n",
      "Starting trial 565\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 412us/sample - loss: 1029.8829\n",
      "Epsilon set to 0.07490878371492794\n",
      "Starting trial 566\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 397us/sample - loss: 3058.9036\n",
      "Epsilon set to 0.0745342397963533\n",
      "Starting trial 567\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 405us/sample - loss: 3375.9603\n",
      "Epsilon set to 0.07416156859737154\n",
      "Starting trial 568\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 420us/sample - loss: 2719.7340\n",
      "Epsilon set to 0.07379076075438468\n",
      "Starting trial 569\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 396us/sample - loss: 12893.2849\n",
      "Epsilon set to 0.07342180695061275\n",
      "Starting trial 570\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 407us/sample - loss: 8908.3975\n",
      "Epsilon set to 0.07305469791585968\n",
      "Starting trial 571\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 403us/sample - loss: 4059.1441\n",
      "Epsilon set to 0.07268942442628039\n",
      "Starting trial 572\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 392us/sample - loss: 4456.8054\n",
      "Epsilon set to 0.07232597730414898\n",
      "Starting trial 573\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 413us/sample - loss: 4161.4614\n",
      "Epsilon set to 0.07196434741762824\n",
      "Starting trial 574\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 387us/sample - loss: 5731.5976\n",
      "Epsilon set to 0.0716045256805401\n",
      "Starting trial 575\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 400us/sample - loss: 5824.2554\n",
      "Epsilon set to 0.0712465030521374\n",
      "Starting trial 576\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 413us/sample - loss: 3779.4985\n",
      "Epsilon set to 0.0708902705368767\n",
      "Starting trial 577\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 383us/sample - loss: 3017.0824\n",
      "Epsilon set to 0.07053581918419231\n",
      "Starting trial 578\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 401us/sample - loss: 1846.2998\n",
      "Epsilon set to 0.07018314008827135\n",
      "Starting trial 579\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 389us/sample - loss: 1515.9919\n",
      "Epsilon set to 0.06983222438783\n",
      "Starting trial 580\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 393us/sample - loss: 1492.9860\n",
      "Epsilon set to 0.06948306326589085\n",
      "Starting trial 581\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 394us/sample - loss: 505.0537\n",
      "Epsilon set to 0.0691356479495614\n",
      "Starting trial 582\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 390us/sample - loss: 568.2720\n",
      "Epsilon set to 0.06878996970981359\n",
      "Starting trial 583\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 409us/sample - loss: 896.4752\n",
      "Epsilon set to 0.06844601986126451\n",
      "Starting trial 584\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 411us/sample - loss: 295.6110\n",
      "Epsilon set to 0.06810378976195819\n",
      "Starting trial 585\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 397us/sample - loss: 556.1535\n",
      "Epsilon set to 0.0677632708131484\n",
      "Starting trial 586\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 403us/sample - loss: 2003.9646\n",
      "Epsilon set to 0.06742445445908266\n",
      "Starting trial 587\n",
      "Train on 500 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 0s 395us/sample - loss: 1159.0754\n",
      "Epsilon set to 0.06708733218678724\n",
      "Starting trial 588\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 405us/sample - loss: 965.0828\n",
      "Epsilon set to 0.0667518955258533\n",
      "Starting trial 589\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 415us/sample - loss: 7078.8022\n",
      "Epsilon set to 0.06641813604822402\n",
      "Starting trial 590\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 407us/sample - loss: 6432.8346\n",
      "Epsilon set to 0.0660860453679829\n",
      "Starting trial 591\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 389us/sample - loss: 4516.2537\n",
      "Epsilon set to 0.06575561514114299\n",
      "Starting trial 592\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 412us/sample - loss: 165.9123\n",
      "Epsilon set to 0.06542683706543727\n",
      "Starting trial 593\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 401us/sample - loss: 3608.6984\n",
      "Epsilon set to 0.06509970288011008\n",
      "Starting trial 594\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 405us/sample - loss: 5395.9844\n",
      "Epsilon set to 0.06477420436570952\n",
      "Starting trial 595\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 398us/sample - loss: 5979.7383\n",
      "Epsilon set to 0.06445033334388098\n",
      "Starting trial 596\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 415us/sample - loss: 5590.2600\n",
      "Epsilon set to 0.06412808167716157\n",
      "Starting trial 597\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 407us/sample - loss: 20375.4354\n",
      "Epsilon set to 0.06380744126877576\n",
      "Starting trial 598\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 388us/sample - loss: 124456.5866\n",
      "Epsilon set to 0.06348840406243188\n",
      "Starting trial 599\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 409us/sample - loss: 464649.3642\n",
      "Epsilon set to 0.06317096204211972\n",
      "Starting trial 600\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 415us/sample - loss: 558339.5051\n",
      "Epsilon set to 0.06285510723190912\n",
      "Starting trial 601\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 401us/sample - loss: 5265.9348\n",
      "Epsilon set to 0.06254083169574957\n",
      "Starting trial 602\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 393us/sample - loss: 23560.3335\n",
      "Epsilon set to 0.062228127537270826\n",
      "Starting trial 603\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 411us/sample - loss: 79148.6132\n",
      "Epsilon set to 0.06191698689958447\n",
      "Starting trial 604\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 399us/sample - loss: 216399.9994\n",
      "Epsilon set to 0.061607401965086545\n",
      "Starting trial 605\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 404us/sample - loss: 836917.3146\n",
      "Epsilon set to 0.06129936495526111\n",
      "Starting trial 606\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 397us/sample - loss: 65257.1943\n",
      "Epsilon set to 0.0609928681304848\n",
      "Starting trial 607\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 399us/sample - loss: 623002.3277\n",
      "Epsilon set to 0.060687903789832374\n",
      "Starting trial 608\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 392us/sample - loss: 38161.0573\n",
      "Epsilon set to 0.06038446427088321\n",
      "Starting trial 609\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 404us/sample - loss: 174475.8470\n",
      "Epsilon set to 0.06008254194952879\n",
      "Starting trial 610\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 403us/sample - loss: 1080621.4657\n",
      "Epsilon set to 0.05978212923978115\n",
      "Starting trial 611\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 400us/sample - loss: 34900.6395\n",
      "Epsilon set to 0.05948321859358224\n",
      "Starting trial 612\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 403us/sample - loss: 44860.1449\n",
      "Epsilon set to 0.05918580250061433\n",
      "Starting trial 613\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 421us/sample - loss: 73284.3565\n",
      "Epsilon set to 0.058889873488111255\n",
      "Starting trial 614\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 400us/sample - loss: 399982.8945\n",
      "Epsilon set to 0.058595424120670696\n",
      "Starting trial 615\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 406us/sample - loss: 850027.0994\n",
      "Epsilon set to 0.05830244700006734\n",
      "Starting trial 616\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 403us/sample - loss: 770256.9160\n",
      "Epsilon set to 0.058010934765067\n",
      "Starting trial 617\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 411us/sample - loss: 1515440.0415\n",
      "Epsilon set to 0.05772088009124167\n",
      "Starting trial 618\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 411us/sample - loss: 2776347.8867\n",
      "Epsilon set to 0.05743227569078546\n",
      "Starting trial 619\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 419us/sample - loss: 2926045.7680\n",
      "Epsilon set to 0.05714511431233153\n",
      "Starting trial 620\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 417us/sample - loss: 5756145.9280\n",
      "Epsilon set to 0.05685938874076987\n",
      "Starting trial 621\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 417us/sample - loss: 3246380.8390\n",
      "Epsilon set to 0.056575091797066025\n",
      "Starting trial 622\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 396us/sample - loss: 3809895.1960\n",
      "Epsilon set to 0.056292216338080694\n",
      "Starting trial 623\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 405us/sample - loss: 3055770.0360\n",
      "Epsilon set to 0.05601075525639029\n",
      "Starting trial 624\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 408us/sample - loss: 3988759.6345\n",
      "Epsilon set to 0.05573070148010834\n",
      "Starting trial 625\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 405us/sample - loss: 3148039.9410\n",
      "Epsilon set to 0.0554520479727078\n",
      "Starting trial 626\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 383us/sample - loss: 3219491.6716\n",
      "Epsilon set to 0.05517478773284426\n",
      "Starting trial 627\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 411us/sample - loss: 1604293.1510\n",
      "Epsilon set to 0.05489891379418004\n",
      "Starting trial 628\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 396us/sample - loss: 1484979.6397\n",
      "Epsilon set to 0.05462441922520914\n",
      "Starting trial 629\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 395us/sample - loss: 952719.0223\n",
      "Epsilon set to 0.0543512971290831\n",
      "Starting trial 630\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 389us/sample - loss: 778450.1425\n",
      "Epsilon set to 0.05407954064343768\n",
      "Starting trial 631\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 395us/sample - loss: 720209.3339\n",
      "Epsilon set to 0.05380914294022049\n",
      "Starting trial 632\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 407us/sample - loss: 586493.6713\n",
      "Epsilon set to 0.05354009722551939\n",
      "Starting trial 633\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 407us/sample - loss: 500018.8108\n",
      "Epsilon set to 0.05327239673939179\n",
      "Starting trial 634\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 388us/sample - loss: 490295.8335\n",
      "Epsilon set to 0.053006034755694834\n",
      "Starting trial 635\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 411us/sample - loss: 302651.8418\n",
      "Epsilon set to 0.052741004581916356\n",
      "Starting trial 636\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 417us/sample - loss: 203009.7889\n",
      "Epsilon set to 0.052477299559006776\n",
      "Starting trial 637\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 396us/sample - loss: 158899.6842\n",
      "Epsilon set to 0.052214913061211746\n",
      "Starting trial 638\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 404us/sample - loss: 127658.2544\n",
      "Epsilon set to 0.05195383849590569\n",
      "Starting trial 639\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 395us/sample - loss: 190518.4359\n",
      "Epsilon set to 0.05169406930342616\n",
      "Starting trial 640\n",
      "Train on 500 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 0s 397us/sample - loss: 282431.1664\n",
      "Epsilon set to 0.05143559895690903\n",
      "Starting trial 641\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 417us/sample - loss: 321883.8294\n",
      "Epsilon set to 0.051178420962124486\n",
      "Starting trial 642\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 393us/sample - loss: 714791.2110\n",
      "Epsilon set to 0.05092252885731386\n",
      "Starting trial 643\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 411us/sample - loss: 909364.1785\n",
      "Epsilon set to 0.05066791621302729\n",
      "Starting trial 644\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 403us/sample - loss: 1253406.3038\n",
      "Epsilon set to 0.05041457663196215\n",
      "Starting trial 645\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 393us/sample - loss: 556282.2618\n",
      "Epsilon set to 0.050162503748802344\n",
      "Starting trial 646\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 387us/sample - loss: 747037.0827\n",
      "Epsilon set to 0.049911691230058335\n",
      "Starting trial 647\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 407us/sample - loss: 336895.5598\n",
      "Epsilon set to 0.04966213277390804\n",
      "Starting trial 648\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 427us/sample - loss: 383845.8527\n",
      "Epsilon set to 0.0494138221100385\n",
      "Starting trial 649\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 389us/sample - loss: 218643.3040\n",
      "Epsilon set to 0.04916675299948831\n",
      "Starting trial 650\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 406us/sample - loss: 267083.0203\n",
      "Epsilon set to 0.04892091923449087\n",
      "Starting trial 651\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 385us/sample - loss: 194029.8886\n",
      "Epsilon set to 0.04867631463831842\n",
      "Starting trial 652\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 423us/sample - loss: 73088.6380\n",
      "Epsilon set to 0.048432933065126825\n",
      "Starting trial 653\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 401us/sample - loss: 70802.5760\n",
      "Epsilon set to 0.048190768399801194\n",
      "Starting trial 654\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 411us/sample - loss: 63369.3760\n",
      "Epsilon set to 0.04794981455780219\n",
      "Starting trial 655\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 419us/sample - loss: 179412.4728\n",
      "Epsilon set to 0.04771006548501318\n",
      "Starting trial 656\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 407us/sample - loss: 324435.3846\n",
      "Epsilon set to 0.047471515157588115\n",
      "Starting trial 657\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 413us/sample - loss: 2721951.9390\n",
      "Epsilon set to 0.047234157581800176\n",
      "Starting trial 658\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 453us/sample - loss: 4257657.6738\n",
      "Epsilon set to 0.046997986793891174\n",
      "Starting trial 659\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 406us/sample - loss: 6547301.2475\n",
      "Epsilon set to 0.04676299685992172\n",
      "Starting trial 660\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 415us/sample - loss: 4444867.6727\n",
      "Epsilon set to 0.04652918187562211\n",
      "Starting trial 661\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 405us/sample - loss: 10990195.8620\n",
      "Epsilon set to 0.046296535966244\n",
      "Starting trial 662\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 398us/sample - loss: 8419667.0425\n",
      "Epsilon set to 0.046065053286412784\n",
      "Starting trial 663\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 399us/sample - loss: 7109604.3280\n",
      "Epsilon set to 0.04583472801998072\n",
      "Starting trial 664\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 417us/sample - loss: 7028201.5840\n",
      "Epsilon set to 0.045605554379880814\n",
      "Starting trial 665\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 383us/sample - loss: 3373268.5850\n",
      "Epsilon set to 0.04537752660798141\n",
      "Starting trial 666\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 406us/sample - loss: 2267218.7789\n",
      "Epsilon set to 0.0451506389749415\n",
      "Starting trial 667\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 407us/sample - loss: 1548350.4543\n",
      "Epsilon set to 0.044924885780066794\n",
      "Starting trial 668\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 421us/sample - loss: 456029.2426\n",
      "Epsilon set to 0.04470026135116646\n",
      "Starting trial 669\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 395us/sample - loss: 431401.8125\n",
      "Epsilon set to 0.04447676004441063\n",
      "Starting trial 670\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 419us/sample - loss: 235409.9037\n",
      "Epsilon set to 0.04425437624418858\n",
      "Starting trial 671\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 411us/sample - loss: 115332.6964\n",
      "Epsilon set to 0.04403310436296763\n",
      "Starting trial 672\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 387us/sample - loss: 253010.5029\n",
      "Epsilon set to 0.043812938841152796\n",
      "Starting trial 673\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 400us/sample - loss: 202107.9321\n",
      "Epsilon set to 0.04359387414694703\n",
      "Starting trial 674\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 391us/sample - loss: 49429.2974\n",
      "Epsilon set to 0.043375904776212296\n",
      "Starting trial 675\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 405us/sample - loss: 71302.3239\n",
      "Epsilon set to 0.043159025252331236\n",
      "Starting trial 676\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 418us/sample - loss: 32680.9870\n",
      "Epsilon set to 0.04294323012606958\n",
      "Starting trial 677\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 391us/sample - loss: 60059.0177\n",
      "Epsilon set to 0.04272851397543923\n",
      "Starting trial 678\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 396us/sample - loss: 29566.0935\n",
      "Epsilon set to 0.04251487140556204\n",
      "Starting trial 679\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 396us/sample - loss: 18386.9034\n",
      "Epsilon set to 0.04230229704853423\n",
      "Starting trial 680\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 405us/sample - loss: 23917.3319\n",
      "Epsilon set to 0.04209078556329156\n",
      "Starting trial 681\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 411us/sample - loss: 19170.2611\n",
      "Epsilon set to 0.0418803316354751\n",
      "Starting trial 682\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 424us/sample - loss: 19679.4896\n",
      "Epsilon set to 0.041670929977297724\n",
      "Starting trial 683\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 403us/sample - loss: 17066.6509\n",
      "Epsilon set to 0.04146257532741124\n",
      "Starting trial 684\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 411us/sample - loss: 18055.7957\n",
      "Epsilon set to 0.04125526245077418\n",
      "Starting trial 685\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 406us/sample - loss: 13960.9535\n",
      "Epsilon set to 0.04104898613852031\n",
      "Starting trial 686\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 413us/sample - loss: 10461.0524\n",
      "Epsilon set to 0.04084374120782771\n",
      "Starting trial 687\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 391us/sample - loss: 21674.8472\n",
      "Epsilon set to 0.04063952250178857\n",
      "Starting trial 688\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 399us/sample - loss: 27355.9827\n",
      "Epsilon set to 0.04043632488927963\n",
      "Starting trial 689\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 400us/sample - loss: 20186.5809\n",
      "Epsilon set to 0.04023414326483323\n",
      "Starting trial 690\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 401us/sample - loss: 16170.8954\n",
      "Epsilon set to 0.040032972548509065\n",
      "Starting trial 691\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 415us/sample - loss: 13381.5127\n",
      "Epsilon set to 0.03983280768576652\n",
      "Starting trial 692\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 408us/sample - loss: 11319.1216\n",
      "Epsilon set to 0.03963364364733769\n",
      "Starting trial 693\n",
      "Train on 500 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 0s 403us/sample - loss: 19461.5859\n",
      "Epsilon set to 0.039435475429100995\n",
      "Starting trial 694\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 411us/sample - loss: 20207.0228\n",
      "Epsilon set to 0.03923829805195549\n",
      "Starting trial 695\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 416us/sample - loss: 13919.7305\n",
      "Epsilon set to 0.03904210656169572\n",
      "Starting trial 696\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 409us/sample - loss: 14191.5931\n",
      "Epsilon set to 0.03884689602888724\n",
      "Starting trial 697\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 413us/sample - loss: 12466.3484\n",
      "Epsilon set to 0.0386526615487428\n",
      "Starting trial 698\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 393us/sample - loss: 20774.8713\n",
      "Epsilon set to 0.03845939824099909\n",
      "Starting trial 699\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 407us/sample - loss: 24351.0490\n",
      "Epsilon set to 0.03826710124979409\n",
      "Starting trial 700\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 391us/sample - loss: 15285.4108\n",
      "Epsilon set to 0.038075765743545126\n",
      "Starting trial 701\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 407us/sample - loss: 13135.2705\n",
      "Epsilon set to 0.0378853869148274\n",
      "Starting trial 702\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 387us/sample - loss: 21898.9502\n",
      "Epsilon set to 0.03769595998025326\n",
      "Starting trial 703\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 411us/sample - loss: 19902.7086\n",
      "Epsilon set to 0.03750748018035199\n",
      "Starting trial 704\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 397us/sample - loss: 18306.1617\n",
      "Epsilon set to 0.037319942779450235\n",
      "Starting trial 705\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 414us/sample - loss: 24208.8074\n",
      "Epsilon set to 0.037133343065552986\n",
      "Starting trial 706\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 407us/sample - loss: 16172.5570\n",
      "Epsilon set to 0.03694767635022522\n",
      "Starting trial 707\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 413us/sample - loss: 22304.7814\n",
      "Epsilon set to 0.036762937968474095\n",
      "Starting trial 708\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 402us/sample - loss: 18195.9923\n",
      "Epsilon set to 0.03657912327863173\n",
      "Starting trial 709\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 391us/sample - loss: 21584.0970\n",
      "Epsilon set to 0.036396227662238566\n",
      "Starting trial 710\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 393us/sample - loss: 19377.9104\n",
      "Epsilon set to 0.03621424652392737\n",
      "Starting trial 711\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 399us/sample - loss: 13433.9450\n",
      "Epsilon set to 0.036033175291307735\n",
      "Starting trial 712\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 408us/sample - loss: 26188.0672\n",
      "Epsilon set to 0.03585300941485119\n",
      "Starting trial 713\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 399us/sample - loss: 15253.5376\n",
      "Epsilon set to 0.035673744367776934\n",
      "Starting trial 714\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 402us/sample - loss: 29651.4392\n",
      "Epsilon set to 0.03549537564593805\n",
      "Starting trial 715\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 432us/sample - loss: 21896.4050\n",
      "Epsilon set to 0.035317898767708356\n",
      "Starting trial 716\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 405us/sample - loss: 16448.6905\n",
      "Epsilon set to 0.03514130927386981\n",
      "Starting trial 717\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 385us/sample - loss: 13063.7133\n",
      "Epsilon set to 0.03496560272750046\n",
      "Starting trial 718\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 409us/sample - loss: 13310.5707\n",
      "Epsilon set to 0.03479077471386296\n",
      "Starting trial 719\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 403us/sample - loss: 40458.2492\n",
      "Epsilon set to 0.03461682084029365\n",
      "Starting trial 720\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 411us/sample - loss: 15013.6609\n",
      "Epsilon set to 0.034443736736092176\n",
      "Starting trial 721\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 394us/sample - loss: 26272.3361\n",
      "Epsilon set to 0.034271518052411715\n",
      "Starting trial 722\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 403us/sample - loss: 13465.1693\n",
      "Epsilon set to 0.034100160462149656\n",
      "Starting trial 723\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 413us/sample - loss: 14890.7110\n",
      "Epsilon set to 0.03392965965983891\n",
      "Starting trial 724\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 406us/sample - loss: 17379.6308\n",
      "Epsilon set to 0.033760011361539714\n",
      "Starting trial 725\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 401us/sample - loss: 19482.8938\n",
      "Epsilon set to 0.03359121130473201\n",
      "Starting trial 726\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 397us/sample - loss: 20114.8619\n",
      "Epsilon set to 0.033423255248208356\n",
      "Starting trial 727\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 421us/sample - loss: 10452.3484\n",
      "Epsilon set to 0.03325613897196732\n",
      "Starting trial 728\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 392us/sample - loss: 15414.4390\n",
      "Epsilon set to 0.03308985827710748\n",
      "Starting trial 729\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 389us/sample - loss: 10139.8692\n",
      "Epsilon set to 0.032924408985721944\n",
      "Starting trial 730\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 401us/sample - loss: 9405.7658\n",
      "Epsilon set to 0.03275978694079333\n",
      "Starting trial 731\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 412us/sample - loss: 6073.1652\n",
      "Epsilon set to 0.032595988006089364\n",
      "Starting trial 732\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 385us/sample - loss: 16310.4003\n",
      "Epsilon set to 0.032433008066058915\n",
      "Starting trial 733\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 393us/sample - loss: 11002.9534\n",
      "Epsilon set to 0.03227084302572862\n",
      "Starting trial 734\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 396us/sample - loss: 8810.4510\n",
      "Epsilon set to 0.032109488810599975\n",
      "Starting trial 735\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 413us/sample - loss: 21020.0719\n",
      "Epsilon set to 0.031948941366546975\n",
      "Starting trial 736\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 397us/sample - loss: 14676.0674\n",
      "Epsilon set to 0.03178919665971424\n",
      "Starting trial 737\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 402us/sample - loss: 11285.5166\n",
      "Epsilon set to 0.03163025067641567\n",
      "Starting trial 738\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 395us/sample - loss: 6893.6980\n",
      "Epsilon set to 0.03147209942303359\n",
      "Starting trial 739\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 399us/sample - loss: 15771.8647\n",
      "Epsilon set to 0.03131473892591842\n",
      "Starting trial 740\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 395us/sample - loss: 8077.7832\n",
      "Epsilon set to 0.031158165231288826\n",
      "Starting trial 741\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 424us/sample - loss: 15999.8426\n",
      "Epsilon set to 0.03100237440513238\n",
      "Starting trial 742\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 429us/sample - loss: 17886.2129\n",
      "Epsilon set to 0.030847362533106718\n",
      "Starting trial 743\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 443us/sample - loss: 14390.0684\n",
      "Epsilon set to 0.030693125720441184\n",
      "Starting trial 744\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 428us/sample - loss: 21453.6116\n",
      "Epsilon set to 0.030539660091838977\n",
      "Starting trial 745\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 403us/sample - loss: 9571.1525\n",
      "Epsilon set to 0.03038696179137978\n",
      "Starting trial 746\n",
      "Train on 500 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 0s 403us/sample - loss: 8720.8955\n",
      "Epsilon set to 0.030235026982422884\n",
      "Starting trial 747\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 420us/sample - loss: 17382.8150\n",
      "Epsilon set to 0.030083851847510768\n",
      "Starting trial 748\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 386us/sample - loss: 5192.1283\n",
      "Epsilon set to 0.029933432588273214\n",
      "Starting trial 749\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 399us/sample - loss: 8852.5338\n",
      "Epsilon set to 0.029783765425331846\n",
      "Starting trial 750\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 419us/sample - loss: 16412.0618\n",
      "Epsilon set to 0.029634846598205186\n",
      "Starting trial 751\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 426us/sample - loss: 7409.6671\n",
      "Epsilon set to 0.02948667236521416\n",
      "Starting trial 752\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 405us/sample - loss: 12325.4016\n",
      "Epsilon set to 0.029339239003388088\n",
      "Starting trial 753\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 419us/sample - loss: 16358.1792\n",
      "Epsilon set to 0.029192542808371146\n",
      "Starting trial 754\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 402us/sample - loss: 13117.2607\n",
      "Epsilon set to 0.02904658009432929\n",
      "Starting trial 755\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 393us/sample - loss: 18173.2845\n",
      "Epsilon set to 0.028901347193857643\n",
      "Starting trial 756\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 385us/sample - loss: 7687.6167\n",
      "Epsilon set to 0.028756840457888354\n",
      "Starting trial 757\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 401us/sample - loss: 7248.4076\n",
      "Epsilon set to 0.02861305625559891\n",
      "Starting trial 758\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 405us/sample - loss: 7348.9049\n",
      "Epsilon set to 0.028469990974320916\n",
      "Starting trial 759\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 397us/sample - loss: 9363.3433\n",
      "Epsilon set to 0.02832764101944931\n",
      "Starting trial 760\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 416us/sample - loss: 5559.6825\n",
      "Epsilon set to 0.028186002814352063\n",
      "Starting trial 761\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 393us/sample - loss: 20556.2913\n",
      "Epsilon set to 0.0280450728002803\n",
      "Starting trial 762\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 391us/sample - loss: 13229.1363\n",
      "Epsilon set to 0.0279048474362789\n",
      "Starting trial 763\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 405us/sample - loss: 10867.2109\n",
      "Epsilon set to 0.027765323199097504\n",
      "Starting trial 764\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 391us/sample - loss: 22296.7174\n",
      "Epsilon set to 0.027626496583102015\n",
      "Starting trial 765\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 397us/sample - loss: 10231.0140\n",
      "Epsilon set to 0.027488364100186506\n",
      "Starting trial 766\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 406us/sample - loss: 15953.8323\n",
      "Epsilon set to 0.027350922279685573\n",
      "Starting trial 767\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 407us/sample - loss: 8231.2843\n",
      "Epsilon set to 0.027214167668287145\n",
      "Starting trial 768\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 401us/sample - loss: 11576.1590\n",
      "Epsilon set to 0.02707809682994571\n",
      "Starting trial 769\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 393us/sample - loss: 5353.1991\n",
      "Epsilon set to 0.02694270634579598\n",
      "Starting trial 770\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 387us/sample - loss: 3570.9558\n",
      "Epsilon set to 0.026807992814067\n",
      "Starting trial 771\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 423us/sample - loss: 4544.7983\n",
      "Epsilon set to 0.026673952849996664\n",
      "Starting trial 772\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 407us/sample - loss: 10500.3051\n",
      "Epsilon set to 0.02654058308574668\n",
      "Starting trial 773\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 411us/sample - loss: 6102.0505\n",
      "Epsilon set to 0.026407880170317945\n",
      "Starting trial 774\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 405us/sample - loss: 9734.6474\n",
      "Epsilon set to 0.026275840769466357\n",
      "Starting trial 775\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 395us/sample - loss: 7637.8319\n",
      "Epsilon set to 0.026144461565619025\n",
      "Starting trial 776\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 399us/sample - loss: 5867.7166\n",
      "Epsilon set to 0.02601373925779093\n",
      "Starting trial 777\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 393us/sample - loss: 10850.7962\n",
      "Epsilon set to 0.025883670561501974\n",
      "Starting trial 778\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 411us/sample - loss: 4504.4522\n",
      "Epsilon set to 0.025754252208694463\n",
      "Starting trial 779\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 404us/sample - loss: 4089.5111\n",
      "Epsilon set to 0.02562548094765099\n",
      "Starting trial 780\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 409us/sample - loss: 5336.3789\n",
      "Epsilon set to 0.025497353542912736\n",
      "Starting trial 781\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 391us/sample - loss: 6787.0261\n",
      "Epsilon set to 0.02536986677519817\n",
      "Starting trial 782\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 417us/sample - loss: 5936.1884\n",
      "Epsilon set to 0.02524301744132218\n",
      "Starting trial 783\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 398us/sample - loss: 4968.5204\n",
      "Epsilon set to 0.025116802354115567\n",
      "Starting trial 784\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 417us/sample - loss: 6475.3168\n",
      "Epsilon set to 0.024991218342344988\n",
      "Starting trial 785\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 411us/sample - loss: 6823.7850\n",
      "Epsilon set to 0.024866262250633264\n",
      "Starting trial 786\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 397us/sample - loss: 6630.4847\n",
      "Epsilon set to 0.024741930939380097\n",
      "Starting trial 787\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 401us/sample - loss: 7320.1581\n",
      "Epsilon set to 0.024618221284683196\n",
      "Starting trial 788\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 411us/sample - loss: 7437.0845\n",
      "Epsilon set to 0.02449513017825978\n",
      "Starting trial 789\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 397us/sample - loss: 15241.6189\n",
      "Epsilon set to 0.02437265452736848\n",
      "Starting trial 790\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 410us/sample - loss: 6846.3353\n",
      "Epsilon set to 0.024250791254731636\n",
      "Starting trial 791\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 397us/sample - loss: 3786.5189\n",
      "Epsilon set to 0.024129537298457977\n",
      "Starting trial 792\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 405us/sample - loss: 5378.9043\n",
      "Epsilon set to 0.024008889611965685\n",
      "Starting trial 793\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 395us/sample - loss: 10090.8714\n",
      "Epsilon set to 0.023888845163905856\n",
      "Starting trial 794\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 416us/sample - loss: 6819.8967\n",
      "Epsilon set to 0.023769400938086327\n",
      "Starting trial 795\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 405us/sample - loss: 10768.6972\n",
      "Epsilon set to 0.023650553933395897\n",
      "Starting trial 796\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 399us/sample - loss: 9188.6902\n",
      "Epsilon set to 0.023532301163728918\n",
      "Starting trial 797\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 396us/sample - loss: 7778.0785\n",
      "Epsilon set to 0.023414639657910272\n",
      "Starting trial 798\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 395us/sample - loss: 2644.8813\n",
      "Epsilon set to 0.023297566459620722\n",
      "Starting trial 799\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 411us/sample - loss: 7666.7113\n",
      "Epsilon set to 0.023181078627322618\n",
      "Starting trial 800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 390us/sample - loss: 9538.4718\n",
      "Epsilon set to 0.023065173234186005\n",
      "Starting trial 801\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 400us/sample - loss: 5761.6221\n",
      "Epsilon set to 0.022949847368015076\n",
      "Starting trial 802\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 403us/sample - loss: 6629.8630\n",
      "Epsilon set to 0.022835098131175\n",
      "Starting trial 803\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 401us/sample - loss: 9265.3973\n",
      "Epsilon set to 0.022720922640519125\n",
      "Starting trial 804\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 409us/sample - loss: 10697.5764\n",
      "Epsilon set to 0.02260731802731653\n",
      "Starting trial 805\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 409us/sample - loss: 3286.3658\n",
      "Epsilon set to 0.022494281437179946\n",
      "Starting trial 806\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 399us/sample - loss: 3519.9969\n",
      "Epsilon set to 0.022381810029994047\n",
      "Starting trial 807\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 398us/sample - loss: 3180.0582\n",
      "Epsilon set to 0.022269900979844076\n",
      "Starting trial 808\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 415us/sample - loss: 3893.0004\n",
      "Epsilon set to 0.022158551474944856\n",
      "Starting trial 809\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 419us/sample - loss: 5779.3592\n",
      "Epsilon set to 0.022047758717570132\n",
      "Starting trial 810\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 418us/sample - loss: 2854.6152\n",
      "Epsilon set to 0.021937519923982282\n",
      "Starting trial 811\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 401us/sample - loss: 4358.8678\n",
      "Epsilon set to 0.021827832324362372\n",
      "Starting trial 812\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 395us/sample - loss: 5829.3918\n",
      "Epsilon set to 0.02171869316274056\n",
      "Starting trial 813\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 393us/sample - loss: 2917.3795\n",
      "Epsilon set to 0.021610099696926857\n",
      "Starting trial 814\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 394us/sample - loss: 3436.9993\n",
      "Epsilon set to 0.021502049198442223\n",
      "Starting trial 815\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 409us/sample - loss: 5671.0072\n",
      "Epsilon set to 0.021394538952450012\n",
      "Starting trial 816\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 399us/sample - loss: 1998.0161\n",
      "Epsilon set to 0.02128756625768776\n",
      "Starting trial 817\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 388us/sample - loss: 3648.7716\n",
      "Epsilon set to 0.021181128426399323\n",
      "Starting trial 818\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 411us/sample - loss: 1505.4402\n",
      "Epsilon set to 0.021075222784267326\n",
      "Starting trial 819\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 411us/sample - loss: 3182.0409\n",
      "Epsilon set to 0.020969846670345987\n",
      "Starting trial 820\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 406us/sample - loss: 5828.1944\n",
      "Epsilon set to 0.020864997436994256\n",
      "Starting trial 821\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 403us/sample - loss: 3387.2078\n",
      "Epsilon set to 0.020760672449809284\n",
      "Starting trial 822\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 397us/sample - loss: 4546.8692\n",
      "Epsilon set to 0.020656869087560238\n",
      "Starting trial 823\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 421us/sample - loss: 3426.0038\n",
      "Epsilon set to 0.020553584742122436\n",
      "Starting trial 824\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 395us/sample - loss: 5446.2291\n",
      "Epsilon set to 0.020450816818411825\n",
      "Starting trial 825\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 421us/sample - loss: 5681.1632\n",
      "Epsilon set to 0.020348562734319765\n",
      "Starting trial 826\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 401us/sample - loss: 5807.7890\n",
      "Epsilon set to 0.020246819920648168\n",
      "Starting trial 827\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 417us/sample - loss: 5042.1806\n",
      "Epsilon set to 0.020145585821044927\n",
      "Starting trial 828\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 401us/sample - loss: 4892.9792\n",
      "Epsilon set to 0.020044857891939702\n",
      "Starting trial 829\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 402us/sample - loss: 2468.2622\n",
      "Epsilon set to 0.019944633602480003\n",
      "Starting trial 830\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 432us/sample - loss: 4071.7518\n",
      "Epsilon set to 0.019844910434467605\n",
      "Starting trial 831\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 401us/sample - loss: 2419.5647\n",
      "Epsilon set to 0.019745685882295267\n",
      "Starting trial 832\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 405us/sample - loss: 4274.1009\n",
      "Epsilon set to 0.01964695745288379\n",
      "Starting trial 833\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 395us/sample - loss: 2122.7461\n",
      "Epsilon set to 0.01954872266561937\n",
      "Starting trial 834\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 410us/sample - loss: 1538.6978\n",
      "Epsilon set to 0.019450979052291272\n",
      "Starting trial 835\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 395us/sample - loss: 1317.5056\n",
      "Epsilon set to 0.019353724157029815\n",
      "Starting trial 836\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 398us/sample - loss: 1341.1880\n",
      "Epsilon set to 0.019256955536244666\n",
      "Starting trial 837\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 399us/sample - loss: 1714.5736\n",
      "Epsilon set to 0.019160670758563442\n",
      "Starting trial 838\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 409us/sample - loss: 626.8762\n",
      "Epsilon set to 0.019064867404770626\n",
      "Starting trial 839\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 387us/sample - loss: 1444.0017\n",
      "Epsilon set to 0.018969543067746772\n",
      "Starting trial 840\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 390us/sample - loss: 1777.3003\n",
      "Epsilon set to 0.018874695352408037\n",
      "Starting trial 841\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 394us/sample - loss: 2638.6775\n",
      "Epsilon set to 0.018780321875645996\n",
      "Starting trial 842\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 403us/sample - loss: 1401.8400\n",
      "Epsilon set to 0.018686420266267767\n",
      "Starting trial 843\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 417us/sample - loss: 2172.0844\n",
      "Epsilon set to 0.018592988164936427\n",
      "Starting trial 844\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 397us/sample - loss: 2279.6799\n",
      "Epsilon set to 0.018500023224111744\n",
      "Starting trial 845\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 403us/sample - loss: 4090.7975\n",
      "Epsilon set to 0.018407523107991184\n",
      "Starting trial 846\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 401us/sample - loss: 1808.1034\n",
      "Epsilon set to 0.01831548549245123\n",
      "Starting trial 847\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 411us/sample - loss: 2564.7227\n",
      "Epsilon set to 0.018223908064988973\n",
      "Starting trial 848\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 408us/sample - loss: 1464.7668\n",
      "Epsilon set to 0.018132788524664028\n",
      "Starting trial 849\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 419us/sample - loss: 2695.2521\n",
      "Epsilon set to 0.018042124582040707\n",
      "Starting trial 850\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 411us/sample - loss: 1378.8878\n",
      "Epsilon set to 0.017951913959130504\n",
      "Starting trial 851\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 409us/sample - loss: 1038.9200\n",
      "Epsilon set to 0.01786215438933485\n",
      "Starting trial 852\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 391us/sample - loss: 1475.9028\n",
      "Epsilon set to 0.017772843617388175\n",
      "Starting trial 853\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 391us/sample - loss: 2104.4303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon set to 0.017683979399301233\n",
      "Starting trial 854\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 383us/sample - loss: 2691.0797\n",
      "Epsilon set to 0.017595559502304726\n",
      "Starting trial 855\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 406us/sample - loss: 982.9399\n",
      "Epsilon set to 0.0175075817047932\n",
      "Starting trial 856\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 397us/sample - loss: 1863.6479\n",
      "Epsilon set to 0.017420043796269234\n",
      "Starting trial 857\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 402us/sample - loss: 1302.8317\n",
      "Epsilon set to 0.017332943577287888\n",
      "Starting trial 858\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 390us/sample - loss: 1417.6374\n",
      "Epsilon set to 0.01724627885940145\n",
      "Starting trial 859\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 418us/sample - loss: 1062.3650\n",
      "Epsilon set to 0.017160047465104442\n",
      "Starting trial 860\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 393us/sample - loss: 2168.9819\n",
      "Epsilon set to 0.01707424722777892\n",
      "Starting trial 861\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 403us/sample - loss: 725.5172\n",
      "Epsilon set to 0.016988875991640028\n",
      "Starting trial 862\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 417us/sample - loss: 701.5128\n",
      "Epsilon set to 0.016903931611681827\n",
      "Starting trial 863\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 423us/sample - loss: 1447.3641\n",
      "Epsilon set to 0.01681941195362342\n",
      "Starting trial 864\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 406us/sample - loss: 597.4953\n",
      "Epsilon set to 0.016735314893855303\n",
      "Starting trial 865\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 391us/sample - loss: 1270.8442\n",
      "Epsilon set to 0.016651638319386028\n",
      "Starting trial 866\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 395us/sample - loss: 1527.1909\n",
      "Epsilon set to 0.0165683801277891\n",
      "Starting trial 867\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 390us/sample - loss: 5882.0642\n",
      "Epsilon set to 0.016485538227150154\n",
      "Starting trial 868\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 409us/sample - loss: 4018.1057\n",
      "Epsilon set to 0.0164031105360144\n",
      "Starting trial 869\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 425us/sample - loss: 2053.3141\n",
      "Epsilon set to 0.01632109498333433\n",
      "Starting trial 870\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 397us/sample - loss: 4778.7930\n",
      "Epsilon set to 0.016239489508417658\n",
      "Starting trial 871\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 415us/sample - loss: 4094.7612\n",
      "Epsilon set to 0.01615829206087557\n",
      "Starting trial 872\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 393us/sample - loss: 1418.0882\n",
      "Epsilon set to 0.01607750060057119\n",
      "Starting trial 873\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 398us/sample - loss: 1470.9777\n",
      "Epsilon set to 0.015997113097568336\n",
      "Starting trial 874\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - ETA: 0s - loss: 4411.13 - 0s 405us/sample - loss: 3984.8684\n",
      "Epsilon set to 0.015917127532080494\n",
      "Starting trial 875\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 417us/sample - loss: 5006.3328\n",
      "Epsilon set to 0.01583754189442009\n",
      "Starting trial 876\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 433us/sample - loss: 3040.9275\n",
      "Epsilon set to 0.01575835418494799\n",
      "Starting trial 877\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 416us/sample - loss: 1203.1184\n",
      "Epsilon set to 0.01567956241402325\n",
      "Starting trial 878\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 398us/sample - loss: 4183.8770\n",
      "Epsilon set to 0.015601164601953134\n",
      "Starting trial 879\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 407us/sample - loss: 1961.6041\n",
      "Epsilon set to 0.015523158778943369\n",
      "Starting trial 880\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 418us/sample - loss: 3882.5142\n",
      "Epsilon set to 0.015445542985048652\n",
      "Starting trial 881\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 410us/sample - loss: 2324.4442\n",
      "Epsilon set to 0.015368315270123408\n",
      "Starting trial 882\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 423us/sample - loss: 1767.3201\n",
      "Epsilon set to 0.01529147369377279\n",
      "Starting trial 883\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 418us/sample - loss: 1285.9597\n",
      "Epsilon set to 0.015215016325303928\n",
      "Starting trial 884\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 390us/sample - loss: 1149.7039\n",
      "Epsilon set to 0.015138941243677408\n",
      "Starting trial 885\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 415us/sample - loss: 1643.2488\n",
      "Epsilon set to 0.01506324653745902\n",
      "Starting trial 886\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 407us/sample - loss: 3009.3268\n",
      "Epsilon set to 0.014987930304771725\n",
      "Starting trial 887\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 407us/sample - loss: 3800.0862\n",
      "Epsilon set to 0.014912990653247866\n",
      "Starting trial 888\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 397us/sample - loss: 672.0446\n",
      "Epsilon set to 0.014838425699981627\n",
      "Starting trial 889\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 401us/sample - loss: 5384.3520\n",
      "Epsilon set to 0.01476423357148172\n",
      "Starting trial 890\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 395us/sample - loss: 610.8206\n",
      "Epsilon set to 0.014690412403624311\n",
      "Starting trial 891\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 403us/sample - loss: 587.5942\n",
      "Epsilon set to 0.01461696034160619\n",
      "Starting trial 892\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 395us/sample - loss: 432.0428\n",
      "Epsilon set to 0.014543875539898159\n",
      "Starting trial 893\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 417us/sample - loss: 548.5209\n",
      "Epsilon set to 0.014471156162198668\n",
      "Starting trial 894\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 401us/sample - loss: 693.0988\n",
      "Epsilon set to 0.014398800381387675\n",
      "Starting trial 895\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 395us/sample - loss: 717.8745\n",
      "Epsilon set to 0.014326806379480736\n",
      "Starting trial 896\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 407us/sample - loss: 814.7989s - loss: 826.413\n",
      "Epsilon set to 0.014255172347583332\n",
      "Starting trial 897\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 404us/sample - loss: 1696.0992\n",
      "Epsilon set to 0.014183896485845416\n",
      "Starting trial 898\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 395us/sample - loss: 746.8512\n",
      "Epsilon set to 0.014112977003416188\n",
      "Starting trial 899\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 388us/sample - loss: 1961.8848\n",
      "Epsilon set to 0.014042412118399107\n",
      "Starting trial 900\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 428us/sample - loss: 523.8209\n",
      "Epsilon set to 0.013972200057807112\n",
      "Starting trial 901\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 385us/sample - loss: 2441.2876\n",
      "Epsilon set to 0.013902339057518077\n",
      "Starting trial 902\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 418us/sample - loss: 1483.3061\n",
      "Epsilon set to 0.013832827362230486\n",
      "Starting trial 903\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 406us/sample - loss: 1256.7509\n",
      "Epsilon set to 0.013763663225419333\n",
      "Starting trial 904\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 399us/sample - loss: 672.8156\n",
      "Epsilon set to 0.013694844909292236\n",
      "Starting trial 905\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 423us/sample - loss: 1394.8963\n",
      "Epsilon set to 0.013626370684745774\n",
      "Starting trial 906\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 407us/sample - loss: 1315.1160\n",
      "Epsilon set to 0.013558238831322046\n",
      "Starting trial 907\n",
      "Train on 500 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 0s 406us/sample - loss: 602.2105\n",
      "Epsilon set to 0.013490447637165436\n",
      "Starting trial 908\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 415us/sample - loss: 1047.1134\n",
      "Epsilon set to 0.013422995398979608\n",
      "Starting trial 909\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 421us/sample - loss: 856.0418\n",
      "Epsilon set to 0.01335588042198471\n",
      "Starting trial 910\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 391us/sample - loss: 871.2514\n",
      "Epsilon set to 0.013289101019874787\n",
      "Starting trial 911\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 411us/sample - loss: 811.2676\n",
      "Epsilon set to 0.013222655514775413\n",
      "Starting trial 912\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 405us/sample - loss: 1085.6293\n",
      "Epsilon set to 0.013156542237201536\n",
      "Starting trial 913\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 405us/sample - loss: 559.3748\n",
      "Epsilon set to 0.013090759526015528\n",
      "Starting trial 914\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 401us/sample - loss: 703.7478\n",
      "Epsilon set to 0.01302530572838545\n",
      "Starting trial 915\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 400us/sample - loss: 686.9771\n",
      "Epsilon set to 0.012960179199743523\n",
      "Starting trial 916\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 414us/sample - loss: 661.9993\n",
      "Epsilon set to 0.012895378303744804\n",
      "Starting trial 917\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 403us/sample - loss: 479.2134\n",
      "Epsilon set to 0.01283090141222608\n",
      "Starting trial 918\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 401us/sample - loss: 566.4276\n",
      "Epsilon set to 0.012766746905164949\n",
      "Starting trial 919\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 411us/sample - loss: 1019.8676\n",
      "Epsilon set to 0.012702913170639124\n",
      "Starting trial 920\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 409us/sample - loss: 525.5469\n",
      "Epsilon set to 0.012639398604785928\n",
      "Starting trial 921\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 415us/sample - loss: 315.0439\n",
      "Epsilon set to 0.012576201611761997\n",
      "Starting trial 922\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 419us/sample - loss: 656.2984\n",
      "Epsilon set to 0.012513320603703188\n",
      "Starting trial 923\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 410us/sample - loss: 1014.3568\n",
      "Epsilon set to 0.012450754000684672\n",
      "Starting trial 924\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 405us/sample - loss: 690.7796\n",
      "Epsilon set to 0.012388500230681249\n",
      "Starting trial 925\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 415us/sample - loss: 614.2339s - loss: 650.587\n",
      "Epsilon set to 0.012326557729527843\n",
      "Starting trial 926\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 396us/sample - loss: 506.3399\n",
      "Epsilon set to 0.012264924940880204\n",
      "Starting trial 927\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 411us/sample - loss: 798.1186\n",
      "Epsilon set to 0.012203600316175803\n",
      "Starting trial 928\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 406us/sample - loss: 713.1835\n",
      "Epsilon set to 0.012142582314594924\n",
      "Starting trial 929\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 404us/sample - loss: 678.2915\n",
      "Epsilon set to 0.01208186940302195\n",
      "Starting trial 930\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 401us/sample - loss: 1769.1425\n",
      "Epsilon set to 0.01202146005600684\n",
      "Starting trial 931\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 427us/sample - loss: 1847.6035\n",
      "Epsilon set to 0.011961352755726806\n",
      "Starting trial 932\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 400us/sample - loss: 826.6452\n",
      "Epsilon set to 0.01190154599194817\n",
      "Starting trial 933\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 415us/sample - loss: 606.8865\n",
      "Epsilon set to 0.01184203826198843\n",
      "Starting trial 934\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 417us/sample - loss: 665.4327\n",
      "Epsilon set to 0.011782828070678488\n",
      "Starting trial 935\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 405us/sample - loss: 1961.7376\n",
      "Epsilon set to 0.011723913930325095\n",
      "Starting trial 936\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 425us/sample - loss: 1505.8573\n",
      "Epsilon set to 0.01166529436067347\n",
      "Starting trial 937\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 407us/sample - loss: 1151.0701\n",
      "Epsilon set to 0.011606967888870102\n",
      "Starting trial 938\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 411us/sample - loss: 905.6679\n",
      "Epsilon set to 0.01154893304942575\n",
      "Starting trial 939\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 413us/sample - loss: 2117.5856\n",
      "Epsilon set to 0.011491188384178622\n",
      "Starting trial 940\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 409us/sample - loss: 1402.7951\n",
      "Epsilon set to 0.011433732442257729\n",
      "Starting trial 941\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 405us/sample - loss: 1124.8750\n",
      "Epsilon set to 0.01137656378004644\n",
      "Starting trial 942\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 408us/sample - loss: 717.8121\n",
      "Epsilon set to 0.011319680961146208\n",
      "Starting trial 943\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 445us/sample - loss: 688.4071\n",
      "Epsilon set to 0.011263082556340478\n",
      "Starting trial 944\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 403us/sample - loss: 1223.6535\n",
      "Epsilon set to 0.011206767143558775\n",
      "Starting trial 945\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 399us/sample - loss: 549.8167\n",
      "Epsilon set to 0.011150733307840981\n",
      "Starting trial 946\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 413us/sample - loss: 648.8149\n",
      "Epsilon set to 0.011094979641301777\n",
      "Starting trial 947\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 401us/sample - loss: 1115.0428\n",
      "Epsilon set to 0.011039504743095268\n",
      "Starting trial 948\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 411us/sample - loss: 727.9849\n",
      "Epsilon set to 0.01098430721937979\n",
      "Starting trial 949\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 403us/sample - loss: 682.6847\n",
      "Epsilon set to 0.010929385683282892\n",
      "Starting trial 950\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 397us/sample - loss: 787.0285\n",
      "Epsilon set to 0.010874738754866477\n",
      "Starting trial 951\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 384us/sample - loss: 560.1830\n",
      "Epsilon set to 0.010820365061092144\n",
      "Starting trial 952\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 392us/sample - loss: 545.0291\n",
      "Epsilon set to 0.010766263235786683\n",
      "Starting trial 953\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 407us/sample - loss: 470.3423\n",
      "Epsilon set to 0.01071243191960775\n",
      "Starting trial 954\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 407us/sample - loss: 755.2814\n",
      "Epsilon set to 0.010658869760009713\n",
      "Starting trial 955\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - ETA: 0s - loss: 1209.69 - 0s 412us/sample - loss: 1101.3541\n",
      "Epsilon set to 0.010605575411209664\n",
      "Starting trial 956\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 407us/sample - loss: 893.4315\n",
      "Epsilon set to 0.010552547534153616\n",
      "Starting trial 957\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 389us/sample - loss: 1175.8737\n",
      "Epsilon set to 0.010499784796482848\n",
      "Starting trial 958\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 401us/sample - loss: 2301.9724\n",
      "Epsilon set to 0.010447285872500434\n",
      "Starting trial 959\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 397us/sample - loss: 2524.5237\n",
      "Epsilon set to 0.01039504944313793\n",
      "Starting trial 960\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 399us/sample - loss: 579.2665\n",
      "Epsilon set to 0.010343074195922241\n",
      "Starting trial 961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 407us/sample - loss: 956.2521\n",
      "Epsilon set to 0.01029135882494263\n",
      "Starting trial 962\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 418us/sample - loss: 1197.2325\n",
      "Epsilon set to 0.010239902030817916\n",
      "Starting trial 963\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 403us/sample - loss: 1389.2807\n",
      "Epsilon set to 0.010188702520663827\n",
      "Starting trial 964\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 391us/sample - loss: 978.1088\n",
      "Epsilon set to 0.010137759008060509\n",
      "Starting trial 965\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 418us/sample - loss: 2035.8108\n",
      "Epsilon set to 0.010087070213020206\n",
      "Starting trial 966\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 425us/sample - loss: 846.9011\n",
      "Epsilon set to 0.010036634861955105\n",
      "Starting trial 967\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 409us/sample - loss: 841.7454\n",
      "Epsilon set to 0.01\n",
      "Starting trial 968\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 403us/sample - loss: 596.5303\n",
      "Epsilon set to 0.01\n",
      "Starting trial 969\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 413us/sample - loss: 588.9606\n",
      "Epsilon set to 0.01\n",
      "Starting trial 970\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 405us/sample - loss: 662.8584\n",
      "Epsilon set to 0.01\n",
      "Starting trial 971\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 409us/sample - loss: 621.8932\n",
      "Epsilon set to 0.01\n",
      "Starting trial 972\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 404us/sample - loss: 900.2367\n",
      "Epsilon set to 0.01\n",
      "Starting trial 973\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 401us/sample - loss: 2088.0904\n",
      "Epsilon set to 0.01\n",
      "Starting trial 974\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 399us/sample - loss: 1966.0824\n",
      "Epsilon set to 0.01\n",
      "Starting trial 975\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 393us/sample - loss: 1122.7833\n",
      "Epsilon set to 0.01\n",
      "Starting trial 976\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 411us/sample - loss: 1241.9108\n",
      "Epsilon set to 0.01\n",
      "Starting trial 977\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 409us/sample - loss: 1346.7519\n",
      "Epsilon set to 0.01\n",
      "Starting trial 978\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 407us/sample - loss: 1741.7191\n",
      "Epsilon set to 0.01\n",
      "Starting trial 979\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 406us/sample - loss: 1477.7765\n",
      "Epsilon set to 0.01\n",
      "Starting trial 980\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 397us/sample - loss: 1245.0649\n",
      "Epsilon set to 0.01\n",
      "Starting trial 981\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 403us/sample - loss: 1176.9057\n",
      "Epsilon set to 0.01\n",
      "Starting trial 982\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 413us/sample - loss: 1403.8975\n",
      "Epsilon set to 0.01\n",
      "Starting trial 983\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 387us/sample - loss: 1188.5184\n",
      "Epsilon set to 0.01\n",
      "Starting trial 984\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 405us/sample - loss: 746.8511\n",
      "Epsilon set to 0.01\n",
      "Starting trial 985\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 395us/sample - loss: 1357.5392\n",
      "Epsilon set to 0.01\n",
      "Starting trial 986\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 407us/sample - loss: 2502.8528\n",
      "Epsilon set to 0.01\n",
      "Starting trial 987\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 403us/sample - loss: 1610.0019\n",
      "Epsilon set to 0.01\n",
      "Starting trial 988\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 411us/sample - loss: 1671.6468\n",
      "Epsilon set to 0.01\n",
      "Starting trial 989\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 411us/sample - loss: 975.2928\n",
      "Epsilon set to 0.01\n",
      "Starting trial 990\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 395us/sample - loss: 903.7484\n",
      "Epsilon set to 0.01\n",
      "Starting trial 991\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 404us/sample - loss: 951.3840\n",
      "Epsilon set to 0.01\n",
      "Starting trial 992\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 421us/sample - loss: 2029.3915\n",
      "Epsilon set to 0.01\n",
      "Starting trial 993\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 413us/sample - loss: 1505.8159\n",
      "Epsilon set to 0.01\n",
      "Starting trial 994\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 399us/sample - loss: 2036.7685\n",
      "Epsilon set to 0.01\n",
      "Starting trial 995\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 414us/sample - loss: 1097.7228\n",
      "Epsilon set to 0.01\n",
      "Starting trial 996\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 439us/sample - loss: 797.5596\n",
      "Epsilon set to 0.01\n",
      "Starting trial 997\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 403us/sample - loss: 1407.3305\n",
      "Epsilon set to 0.01\n",
      "Starting trial 998\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 406us/sample - loss: 932.8274\n",
      "Epsilon set to 0.01\n",
      "Starting trial 999\n",
      "Train on 500 samples\n",
      "500/500 [==============================] - 0s 407us/sample - loss: 871.0975\n",
      "Epsilon set to 0.01\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x26e74ed42c8>]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "envlist = [Game_Environment(5, 5) for x in range(10)]\n",
    "trials  =  1000\n",
    "history = []\n",
    "dqn_agent = DQN(envlist=envlist)\n",
    "cur_states = [env.state.copy() for env in envlist]\n",
    "for trial in range(trials):\n",
    "    print('Starting trial {}'.format(trial))\n",
    "    actions = dqn_agent.explore(np.stack([env.state for env in envlist]))\n",
    "    rewards = [env.step(action) for env, action in zip(envlist, actions[:,0])]\n",
    "    new_states = [env.state.copy() for env in envlist]\n",
    "    done = [env.done for env in envlist]\n",
    "    [env.reset() for env in envlist if env.done]\n",
    "    [dqn_agent.remember(*item) for item in zip(cur_states, actions, rewards, new_states, done)]\n",
    "    cur_states = new_states.copy()\n",
    "    dqn_agent.replay()\n",
    "    if 'history' in dqn_agent.model.__dict__.keys():\n",
    "        history.extend(dqn_agent.model.history.history['loss'])\n",
    "    \n",
    "plt.plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 100 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(10, 10, figsize=(20, 20))\n",
    "\n",
    "for i in range(100):\n",
    "    test.reset()\n",
    "    axs[i // 10, i % 10].imshow(dqn_agent.exploit(dqn_agent.memory[i][0].reshape(1,5,5,3), True), cmap = 'gray')\n",
    "    axs[i // 10, i % 10].title.set_text(np.argmax(dqn_agent.exploit(dqn_agent.memory[i][0].reshape(1,5,5,3), True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x1440 with 20 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = Game_Environment(5, 5)\n",
    "fig, axs = plt.subplots(10, 2, figsize=(4, 20))\n",
    "\n",
    "for i in range(10):\n",
    "    test.reset()\n",
    "\n",
    "    axs[i, 0].imshow(test.state)\n",
    "    axs[i, 1].imshow(dqn_agent.exploit(test.state.reshape(1,5,5,3), True), cmap = 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['_self_setattr_tracking', '_nested_outputs', '_nested_inputs', 'inputs', 'outputs', '_thread_local', '_name', '_activity_regularizer', '_trainable', '_dynamic', '_is_compiled', '_layers', '_compute_output_and_mask_jointly', 'supports_masking', 'optimizer', '_trainable_weights', '_non_trainable_weights', '_updates', '_losses', '_callable_losses', '_metrics', '_scope', '_reuse', '_graph', '_dtype_policy', '_dtype_defaulted_to_floatx', '_outbound_nodes', '_inbound_nodes', '_trackable_saver', 'built', '_is_graph_network', '_expects_training_arg', '_expects_mask_arg', '_autocast', '_input_layers', '_output_layers', '_input_coordinates', '_output_coordinates', '_supports_ragged_inputs', '_output_mask_cache', '_output_tensor_cache', '_output_shape_cache', '_network_nodes', '_nodes_by_depth', '_layer_call_argspecs', '_self_unconditional_checkpoint_dependencies', '_self_unconditional_dependency_names', '_self_unconditional_deferred_dependencies', '_self_update_uid', '_self_name_based_restores', 'output_names', 'input_names', '_feed_input_names', '_feed_inputs', '_feed_input_shapes', '_obj_reference_counts_dict', '_distribution_strategy', '_compile_time_distribution_strategy', '_compile_distribution', '_run_eagerly', '_experimental_run_tf_function', '_function_kwargs', 'loss', 'loss_weights', 'sample_weight_mode', '_compile_metrics', '_compile_weighted_metrics', '_training_endpoints', '_compiled_trainable_state', '_distributed_model_cache', '_distributed_function_cache', '_compile_metric_functions', 'loss_functions', '_per_output_metrics', '_per_output_weighted_metrics', 'total_loss', 'train_function', 'test_function', 'predict_function', '_collected_trainable_weights', 'stop_training', '_successful_loop_finish'])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn_agent.model.__dict__.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:stonks]",
   "language": "python",
   "name": "conda-env-stonks-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
